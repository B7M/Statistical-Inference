<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Intervals, Testing, &amp; Pvalues | Statistical Inference</title>
  <meta name="description" content="Description about Course/Book." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Intervals, Testing, &amp; Pvalues | Statistical Inference" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Description about Course/Book." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Intervals, Testing, &amp; Pvalues | Statistical Inference" />
  
  <meta name="twitter:description" content="Description about Course/Book." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="assets/dasl_favicon.ico" type="image/x-icon" />
<link rel="prev" href="variability-distribution-asymptotics.html"/>
<link rel="next" href="power-bootstrapping-permutation-tests.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<a href="http://jhudatascience.org/"><img src="https://jhudatascience.org/images/dasl.png" style=" width: 80%; padding-left: 40px; padding-top: 8px; vertical-align: top "</a>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Course</a></li>
<li class="chapter" data-level="1" data-path="probability-expected-values.html"><a href="probability-expected-values.html"><i class="fa fa-check"></i><b>1</b> Probability &amp; Expected Values</a>
<ul>
<li class="chapter" data-level="1.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#course-book"><i class="fa fa-check"></i><b>1.1.1</b> Course Book:</a></li>
<li class="chapter" data-level="1.1.2" data-path="probability-expected-values.html"><a href="probability-expected-values.html#github-repository"><i class="fa fa-check"></i><b>1.1.2</b> Github repository</a></li>
<li class="chapter" data-level="1.1.3" data-path="probability-expected-values.html"><a href="probability-expected-values.html#homework-problems"><i class="fa fa-check"></i><b>1.1.3</b> Homework Problems</a></li>
<li class="chapter" data-level="1.1.4" data-path="probability-expected-values.html"><a href="probability-expected-values.html#differences-of-opinion"><i class="fa fa-check"></i><b>1.1.4</b> Differences of opinion</a></li>
<li class="chapter" data-level="1.1.5" data-path="probability-expected-values.html"><a href="probability-expected-values.html#data-science-specialization-community-site"><i class="fa fa-check"></i><b>1.1.5</b> Data Science Specialization Community Site</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="probability-expected-values.html"><a href="probability-expected-values.html#probability"><i class="fa fa-check"></i><b>1.2</b> Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#probability-mass-functions-and-probability-density-functions"><i class="fa fa-check"></i><b>1.2.1</b> Probability mass functions and probability density functions</a></li>
<li class="chapter" data-level="1.2.2" data-path="probability-expected-values.html"><a href="probability-expected-values.html#quantiles"><i class="fa fa-check"></i><b>1.2.2</b> Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="probability-expected-values.html"><a href="probability-expected-values.html#conditional-probability"><i class="fa fa-check"></i><b>1.3</b> Conditional probability</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#independence"><i class="fa fa-check"></i><b>1.3.1</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="probability-expected-values.html"><a href="probability-expected-values.html#expected-values"><i class="fa fa-check"></i><b>1.4</b> Expected values</a></li>
<li class="chapter" data-level="1.5" data-path="probability-expected-values.html"><a href="probability-expected-values.html#practical-r-exercises-in-swirl"><i class="fa fa-check"></i><b>1.5</b> Practical R Exercises in swirl</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html"><i class="fa fa-check"></i><b>2</b> Variability, Distribution, &amp; Asymptotics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#variability"><i class="fa fa-check"></i><b>2.1</b> Variability</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#variance-simulation-examples"><i class="fa fa-check"></i><b>2.1.1</b> Variance simulation examples</a></li>
<li class="chapter" data-level="2.1.2" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#variance-data-example"><i class="fa fa-check"></i><b>2.1.2</b> Variance data example</a></li>
<li class="chapter" data-level="2.1.3" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#poisson-distribution"><i class="fa fa-check"></i><b>2.1.3</b> Poisson distribution</a></li>
<li class="chapter" data-level="2.1.4" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#asymptotics-and-the-clt"><i class="fa fa-check"></i><b>2.1.4</b> Asymptotics and the CLT</a></li>
<li class="chapter" data-level="2.1.5" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#asymptotics-and-confidence-intervals"><i class="fa fa-check"></i><b>2.1.5</b> Asymptotics and confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#practical-r-exercises-in-swirl-1"><i class="fa fa-check"></i><b>2.2</b> Practical R Exercises in swirl</a></li>
<li class="chapter" data-level="2.3" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#quiz"><i class="fa fa-check"></i><b>2.3</b> Quiz</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html"><i class="fa fa-check"></i><b>3</b> Intervals, Testing, &amp; Pvalues</a>
<ul>
<li class="chapter" data-level="3.0.1" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#independent-group-t-intervals"><i class="fa fa-check"></i><b>3.0.1</b> Independent group T intervals</a></li>
<li class="chapter" data-level="3.1" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.1</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#t-tests"><i class="fa fa-check"></i><b>3.1.1</b> T tests</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#p-values"><i class="fa fa-check"></i><b>3.2</b> P values</a></li>
<li class="chapter" data-level="3.3" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#knitr"><i class="fa fa-check"></i><b>3.3</b> Knitr</a></li>
<li class="chapter" data-level="3.4" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#practical-r-exercises-in-swirl-2"><i class="fa fa-check"></i><b>3.4</b> Practical R Exercises in swirl</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html"><i class="fa fa-check"></i><b>4</b> Power, Bootstrapping, &amp; Permutation Tests</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#calculating-power"><i class="fa fa-check"></i><b>4.0.1</b> Calculating Power</a></li>
<li class="chapter" data-level="4.1" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#multiple-comparisons"><i class="fa fa-check"></i><b>4.1</b> Multiple Comparisons</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="5" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>5</b> References</a></li>
<li class="divider"></li>
<p style="text-align:center;"> <a href="https://github.com/jhudsl/OTTR_Template" target="blank" > This content was published with</a> <a href="https://bookdown.org/" target="blank"> bookdown by:</a> </p>
<p style="text-align:center;"> <a href="http://jhudatascience.org/"> The Johns Hopkins Data Science Lab </a></p>
<p style="text-align:center; font-size: 12px;"> <a href="https://github.com/rstudio4edu/rstudio4edu-book/"> Style adapted from: rstudio4edu-book </a> <a href ="https://creativecommons.org/licenses/by/2.0/"> (CC-BY 2.0) </a></p>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<head>
  <meta name="viewport" content="width=device-width,minimum-scale=1.0,maximum-scale=1.0,initial-scale=1.0">
  <!--script src="https://kit.fontawesome.com/6a26f47516.js"></script-->
  <!--<script src="assets/hideOutput.js"></script>-->
  <link href="assets/style.css" rel="stylesheet">
</head>



<div class="hero-image-container">
  <img class= "hero-image" src= "https://github.com/jhudsl/OTTR_Template/raw/main/assets/dasl_thin_main_image.png">
</div>
<div id="intervals-testing-pvalues" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Intervals, Testing, &amp; Pvalues</h1>
<p>When we estimate something using statistics, usually that estimate comes with uncertainty. Take, for example, election polling. When we get a polled percentage of voters that favor a candidate, we were only able to sample a small subset of voters. Therefore, our estimate has uncertainty associated with it.</p>
<p>Confidence intervals are a convenient way to communicate that uncertainty in estimates.
## Confidence intervals
Hello, I’m Brian Caffo, and I would like to welcome you to the lecture on T Confidence Intervals. This lecture is part of the Coursera Statistical Inference class, which is a component of the Coursera Data Science Specialization. I co-teach this class with Jeff Leek and Roger Peng, and we are all members of the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health.</p>
<p>In the previous lecture, we explored the creation of confidence intervals using the central limit theorem. The intervals we discussed followed the format of estimates plus or minus a quantile from the standard normal distribution multiplied by the estimated standard error. In this lecture, we will focus on methods suitable for small sample sizes. Specifically, we will discuss the Student’s or Gosset’s T distribution and T confidence intervals. These intervals have the format of estimate plus or minus a T quantile multiplied by the standard error of the estimate. The only difference is that we have replaced the z quantile with a t quantile.</p>
<p>The T distribution has fatter tails compared to the normal distribution, resulting in slightly wider intervals. These intervals are extremely useful in statistics, and when you have the option to choose between a t interval and a z interval for cases where both are available, it is advisable to select the t interval. As you gather more data, the t interval gradually becomes more similar to the z interval.</p>
<p>We will cover the single and two-group versions of the t interval in this lecture. Additional t intervals that are valuable will be discussed in our regression class. The t distribution was developed by William Gosset, who published his work under the pseudonym “Student” in 1908. Since Gosset worked for the Guinness Brewery, they did not allow him to publish under his real name.</p>
<p>Unlike the normal distribution, which is characterized by two parameters (mean and variance), the t distribution is primarily centered around zero with a standardized formula for the scale. It is indexed by a single parameter known as degrees of freedom. As the degrees of freedom increase, the t distribution becomes more similar to the standard normal distribution. The reason for the t distribution is that when we divide the difference between the sample mean (x-bar) and the population mean by the estimated standard error for independent and identically distributed (IID) Gaussian data, the resulting distribution is not Gaussian. If we replaced the estimated standard error (s) with the true standard deviation (sigma), the distribution would be exactly standard normal. However, when using the estimated standard error, the distribution follows a t distribution. This distinction becomes less significant as the sample size (n) increases, but for small sample sizes, the difference can be substantial. Using the standard normal distribution for small sample sizes can lead to narrow confidence intervals.</p>
<p>The formula for the t interval is x-bar plus or minus the t quantile with degrees of freedom (n-1) times the estimated standard error. We will provide examples to help you understand this concept. I will also demonstrate the t distribution overlaying the normal distribution for various degrees of freedom using R Studios manipulate function. As the degrees of freedom decrease, the t distribution exhibits heavier tails compared to the normal distribution. However, in the plot, it may be challenging to observe the impact clearly as the focus is on the peak region where the distributions are similar. To illustrate the quantiles, I have plotted the t distribution quantiles against the normal distribution quantiles, starting at the 50th percentile. The plot includes reference lines for the 97.5th quantile, which is typically around 1.96 for the standard normal distribution but can be considerably larger for the t distribution. For instance, with two degrees of freedom, the t quantile exceeds four. However, it’s worth noting that having only</p>
<p>three data points to estimate the variance (n-1 degrees of freedom) is not ideal. When we increase the degrees of freedom to 20, the t quantiles become much closer to the normal quantiles. The light blue reference line represents the identity line, and deviations from this line demonstrate the distinction between the two intervals. The t interval will yield a quantile slightly larger than two, while the z interval will yield a quantile slightly smaller than two. This discrepancy can have a notable impact on the intervals and even determine whether the interval includes zero or not. Hence, we opt for the t distribution in such cases.</p>
<p>In summary, the t interval is wider than the normal interval due to the additional parameter we estimate, the standard deviation. The t interval assumes the data are independent and identically distributed (IID) and approximately symmetric and mound-shaped. It is also applicable for paired observations, such as measurements taken at different times on the same units, by considering the differences or differences on the logarithmic scale. As the degrees of freedom increase, the t quantiles approach those of the standard normal distribution. Therefore, it is advisable to use the t interval rather than selecting between the t and normal intervals. For skewed distributions, the assumptions of the t interval are violated, and alternative procedures like working on the log scale or using bootstrap confidence intervals may be more appropriate. Lastly, for highly discrete data, such as binary or Poisson data, other intervals are available and preferable to the t interval.</p>
<p>I hope this lecture clarifies the concept of T Confidence Intervals. If you have any questions or need further clarification, please feel free to ask.</p>
<p>In R, if you type “data(sleep)”, it will load the sleep data set, which was originally analyzed in Gosset’s Biometrika paper. The data set shows the increase in hours slept for patients on sleep medications. R treats the data as two groups, but we will treat it as paired data. To load the data, you can use the command “head(sleep)” to view the first few rows of the data frame. The variable “extra” represents the extra hours slept, “group” is the group ID, and “ID” is the subject ID. The subjects are numbered from 1 to 10, and then the numbering repeats.</p>
<p>Next, I plot the data and connect each subject with a line. This visualization clearly demonstrates the benefit of acknowledging that these are repeat measurements on the same subjects. If you fail to acknowledge this, you would be comparing the variation within group 1 to the variation within group 2. However, if you acknowledge the pairing, you compare the subject-specific differences across groups, where the variation in these differences is lower due to the correlation within subjects.</p>
<p>To calculate the differences between group 2 and group 1, I extract the first ten measurements (subjects 1-10) and the latter ten measurements (subjects 1-10 on the second medication). The vector “y_subdra” represents the subtraction of group 2 minus group 1, and I calculate the mean and standard deviation of the difference.</p>
<p>To obtain a t confidence interval, I use the formula: mean ± t quantile (evaluated at n-1 degrees of freedom) × standard error of the interval. In this case, I define n as 10. Alternatively, you can use the function “t.test” by passing it the two vectors and setting the argument “paired” to true. Another option is to use a model statement, such as “outcome ~ group, paired = TRUE, data = sleep”.</p>
<p>I have formatted the results into a matrix for better readability. The output provides similar results from these commands, indicating that the difference in means between the groups is between 0.7 and 2.46. Since this is a confidence interval, we can interpret it as follows: if we were to repeatedly perform this procedure on independent samples, about 95% of the intervals obtained would contain the true mean difference we are estimating. This assumes that the subjects are a relevant sample from the population of interest.</p>
<div id="independent-group-t-intervals" class="section level3" number="3.0.1">
<h3><span class="header-section-number">3.0.1</span> Independent group T intervals</h3>
<p>Suppose we want to compare the mean blood pressure between two groups in a randomized trial: the treatment group and the placebo group. This scenario is similar to A/B testing, commonly used in data science. In both A/B testing and randomized trials, randomization is performed to balance unobserved covariates that may affect the results. Since randomization has been conducted, it is reasonable to compare the two groups using a t confidence interval or a t test.</p>
<p>However, we cannot use a paired t test in this case because there is no matching of subjects between the two groups. Therefore, we will discuss methods for comparing independent groups.</p>
<p>The standard confidence interval for comparing independent groups is calculated as follows: (Y bar - X bar) ± (t quantile * standard error of the difference). The degrees of freedom (df) are determined by nx + ny - 2, where nx is the number of observations in group X and ny is the number of observations in group Y. The standard error of the difference is given by S sub p * sqrt(1/nx + 1/ny), where S sub p is the pooled standard deviation.</p>
<p>The pooled standard deviation (S sub p) is the square root of the pooled variance. It is an estimate of the common variance if we assume that the variances in the two groups are equal due to randomization. The pooled variance is a weighted average of the variances from each group, with the weights determined by the sample sizes. If the sample sizes are equal, the pooled variance is the simple average of the variances.</p>
<p>It’s important to note that this interval assumes a constant variance across the two groups. If this assumption is violated, the interval may not provide accurate coverage. In such cases, alternative approaches accounting for different variances per group should be considered.</p>
<p>To illustrate an example from Rosner’s “Fundamentals of Biostatistics” book, we compare 8 oral contraceptive users to 21 controls regarding blood pressure. The average systolic blood pressure for contraceptive users is 133 mmHg with a standard deviation of 15, while the control group has an average blood pressure of 127 mmHg with a standard deviation of 18. To manually construct the independent group interval, we calculate the pooled standard deviation by taking the square root of the weighted average of the variances. The weights are determined by the sample sizes and adjusted for degrees of freedom. We then compute the interval as the difference in means ± (t quantile * pooled standard deviation * sqrt(1/n1 + 1/n2)). In this specific example, the interval ranges from negative 10 to 20. Since the interval contains zero, we cannot rule out the possibility of the population difference being zero.</p>
<p>Let’s consider another example. We’ll revisit the sleep patients example, but this time let’s assume that the subjects were not matched. In this case, we have n1 and n2 as the sample sizes for group 1 and group 2, respectively. Both of these sample sizes will be 10 in this example.</p>
<p>We begin by constructing the pooled standard deviation estimate, calculating the mean difference, and determining the standard error of the mean difference. Then, we manually construct the confidence interval by subtracting the t quantile times the standard error of the mean from the mean difference. Next, we use the t.test function to perform the t-test, specifying paired equals FALSE to indicate that the samples are not paired, and var.equal equals TRUE to assume equal variances in the two groups. We extract the confidence interval from the t.test results.</p>
<p>Comparing the results of the manual calculation and the t.test, we find that they agree perfectly. However, the interval obtained when considering the pairing of subjects is entirely above 0, whereas the interval obtained without considering pairing contains 0. The plot of the data clearly illustrates why this is the case. When comparing the variability between the two groups, there is significant variability. However, when accounting for pairing and considering the variability in the differences within each subject, a substantial portion of the variability is explained by inter-subject differences.</p>
<p>Let’s move on to another example using the ChickWeight dataset in R, which contains weight measurements of chicks from birth to a few weeks later. To access the dataset, you can load the “datasets” package and use the command “data(ChickWeight)”. To work with the data, the “reshape2” package is recommended.</p>
<p>The ChickWeight data is initially in a long format, where the chicks are arranged in a long vector. If you want to convert it to a wide format, where each time point has its own column, you can use the “dcast” function from the reshape2 package. By applying “dcast” to the ChickWeight data frame, with Diet and Chick as the variables that remain the same, and Time as the variable to be converted from long to wide format, you can reshape the data. If you prefer different column names, you can rename them accordingly.</p>
<p>Furthermore, you may want to create a specific variable that represents the total weight gain from time zero in the dataset.</p>
<p>I utilized the dplyr package for data manipulation. First, I used the “mutate” command to create a new variable in my data frame. This variable represents the change in weight, calculated as the final time point weight minus the baseline weight. From this point onward, I will analyze the change in weight variable. Before conducting the test, let’s examine the data visually. I created a spaghetti plot using the ggplot2 package, which displays the weight measurements for each of the four diets over time. Each line represents a different diet, starting from the baseline and ending at the final time point. It appears that there are some noticeable differences, particularly regarding the variability between the diets. However, due to varying sample sizes, it can be challenging to make definitive conclusions. I included a reference line representing the mean for each diet. Without conducting a formal statistical test, it seems that the average weight gain for the first diet is slightly slower than that of the fourth diet. To confirm this observation, let’s proceed with a formal confidence interval analysis.</p>
<p>Instead of plotting individual measurements, I created a violin plot to compare the end weight minus the baseline weight for diets one and four. We will be comparing these two violin plots. The assumption of equal variances seems questionable in this case. To perform a t-test, we need the explanatory variable to have only two levels. To address this, I used the “subset” command to filter the data, including only records where the diet variable is one or four, excluding diets two and three. Please note that if you conduct this analysis on your own, you may want to compare all possible combinations (e.g., one to two, one to three, one to four, etc.), and adjust for multiplicity if necessary.</p>
<p>Next, I used the t.test function to calculate the confidence interval. Since the vectors for diet one and diet four have different lengths, the paired equals TRUE option is not available. I compared the assumption of equal variances versus the assumption of unequal variances. The resulting intervals differ, but both indicate that weight gain on diet one is lower than on diet four. The first interval is -108 to -14, and the second is -104 to -18, with both intervals entirely below zero. However, whether the specific interval change is substantial or not depends on the dataset, which I don’t have enough information about. Nevertheless, because it may be important, let’s also explore the t interval assuming unequal variances.
### A note on unequal variance
I hope the formula for the case of unequal variances seems familiar to you. It involves calculating the difference in means and adding or subtracting a t quantile times the standard error. The standard error is calculated assuming different variances in each of the two groups. However, it’s important to note that if the x and y observations are independent and identically distributed (IID) normal, potentially with different means and variances, the relevant normalized statistic does not follow a t distribution exactly. Instead, it can be approximated by a t distribution with a specific formula for degrees of freedom. While the degrees of freedom calculation may seem unusual because it doesn’t involve sample sizes, it relies on estimated standard deviations and variances from the two groups. Despite this complexity, using this approach yields a t calculation that closely approximates the true distribution, even though it’s not strictly a t distribution. In practice, it’s often recommended to use the unequal variance interval when in doubt.</p>
<p>On the following page, I demonstrate the calculations for the oral contraceptive example mentioned earlier. Going through this calculation can help you understand how to plug in the values, particularly noting that the degrees of freedom in this case are 15.04. However, typically, when we want to perform unequal variance t tests, we can simply use the “t.test” function in R with “var.equal” set to FALSE. This will conduct the relevant t test with unequal variances and provide the corresponding t quantile.</p>
<p>To summarize today’s discussion, we explored creating intervals using the t distribution, which are highly useful in statistics. When dealing with single or paired observations, where the differences are taken into account, the t interval provides robust intervals that are not heavily dependent on assumptions about the data distribution. However, there are cases where alternatives to the t distribution and t intervals may be preferable. For example, if the data is highly skewed, it may be beneficial to consider taking a logarithmic transformation or explore different procedures. In addition, for binary data, odds ratios can be more suitable, which we will cover in the regression class’s generalized linear model component. Similar considerations apply to count data, where we will discuss Poisson models and generalized linear models for rates in the regression class. For other special cases involving two groups, you can find further coverage in the course “Mathematical Biostatistics Boot Camp 2” on Coursera.</p>
</div>
<div id="hypothesis-testing" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Hypothesis testing</h2>
<p>Deciding between two hypotheses is a core activity in scientific discovery. Statistical hypothesis testing is the formal inferential framework around choosing between hypotheses.</p>
<p>Hello and welcome to the lecture on hypothesis testing in the Statistical Inference Coursera class. I’m Brian Caffo, and I co-teach this class with Jeff Leek and Roger Peng. We are all part of the Biostatistics department at the Johns Hopkins Bloomberg School of Public Health. After covering confidence intervals, developing an understanding of hypothesis testing should be relatively straightforward.</p>
<p>Hypothesis testing involves making decisions based on data. It typically begins with a null hypothesis, denoted as H0, which represents the status quo or a default assumption. The null hypothesis is assumed to be true initially, and we need statistical evidence to reject it in favor of an alternative hypothesis, often referred to as the research hypothesis.</p>
<p>To illustrate the concept of hypothesis testing, let’s consider a simple example. Suppose we are interested in studying sleep disordered breathing, and a respiratory disturbance index (RDI) of more than 30 events per hour indicates severe sleep disordered breathing. In a sample of 100 overweight subjects with other risk factors for sleep disordered breathing in a sleep clinic, we find that the mean RDI is 32 events per hour, with a standard deviation of 10 events per hour. We want to test whether the population mean RDI for this population is equal to 30 (our benchmark for severe sleep disordered breathing) or if it is greater than 30. We can specify the null hypothesis as H0: μ = 30 and the alternative hypothesis as Ha: μ &gt; 30. In this example, we are interested in determining if the respiratory disturbance index for this population is greater than 30.</p>
<p>It’s important to note that the truth can only be one of the following: either H0 is true, or Ha is true. As a result, there are only four possible outcomes. If the truth is H0, and we decide to accept H0, we have correctly accepted the null hypothesis. If the truth is H0, but we decide to reject H0 and accept Ha, we have made a Type I error. In the hypothesis testing framework we will present, we aim to control the probability of Type I error to be small.</p>
<p>If the truth is Ha, and we correctly reject H0 and accept Ha, we have correctly rejected the null hypothesis. However, if the truth is Ha, but we mistakenly accept H0 and reject Ha, we have made a Type II error. The rates of Type I and Type II errors are related, meaning that as the Type I error rate decreases, the Type II error rate tends to increase, and vice versa.</p>
<p>An analogy that can help illustrate this is a court of law. In most courts, the null hypothesis is that the defendant is innocent until proven guilty. Rejecting the null hypothesis in this case would mean convicting the defendant. We require evidence and set a standard for that evidence to reject the null hypothesis and convict someone. If we set a very low standard, meaning we don’t require much evidence to convict, we may increase the percentage of innocent people wrongly convicted (Type I errors). However, we would also increase the percentage of guilty people correctly convicted. On the other hand, if we set a very high standard, such as requiring irrefutable evidence to convict, we may increase the percentage of innocent people correctly acquitted (a good thing), but we would also increase the percentage of guilty people wrongly acquitted (Type II errors). This example demonstrates the relationship between Type I and Type II error rates.</p>
<p>Ideally, we aim to gather better evidence for a given standard, such as increasing the sample size. However, before delving into such considerations, let’s focus on how we conduct hypothesis tests.</p>
<p>Let’s revisit the respiratory disturbance index example and consider a reasonable testing strategy. We can reject the null hypothesis if our sample mean respiratory disturbance index is larger than a certain constant, denoted as C. This constant takes into account the variability of the sample mean (X bar). Typically, C is chosen such that the probability of a Type I error (rejecting the null when it is true) is low. In hypothesis testing, a common benchmark for the Type I error rate is 5%.</p>
<p>To determine the appropriate constant C, we need to consider the standard error of the mean, which is 10 (assumed standard deviation of the population) divided by the square root of the sample size, which is 100 in this case, resulting in a value of 1. Under the null hypothesis (H0: μ = 30), the distribution of the sample mean (X bar) follows a normal distribution with a mean of 30 and a variance of 1 (calculated from the standard error squared).</p>
<p>We want to choose the constant C such that the probability of X bar being larger than C, under the null hypothesis, is 5%. The 95th percentile of the standard normal distribution corresponds to 1.645 standard deviations from the mean. Therefore, setting C as 1.645 standard deviations from the mean under the null hypothesis will achieve the desired probability of 5%. In this case, C is calculated as 30 (hypothesized mean) + 1 (standard error of the mean) × 1.645 = 31.645.</p>
<p>To summarize, the rule is to reject the null hypothesis if the observed sample mean is larger than 31.645. This rule ensures that we will reject the null hypothesis 5% of the time when the null hypothesis is true, assuming a sample size of 100 and a population standard deviation of 10. Instead of calculating C in the original units of the data, it is common to convert the sample mean into standard error units from the hypothesized mean. For example, if the observed sample mean is 32, the hypothesized mean is 30, and the standard error is 2, which is greater than 1.645, the chance of this occurring is less than 5%. Therefore, we would reject the null hypothesis in favor of the alternative hypothesis.</p>
<p>To summarize the rule, we reject the null hypothesis when (X bar - hypothesized mean) divided by the standard error of the mean is greater than the appropriate upper quantile that leaves Alpha percent in the upper tail.</p>
<div id="t-tests" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> T tests</h3>
<p>Let’s reconsider our example with a different sample size. Suppose the sample size (n) is now 16 instead of 100. The test statistic remains the same, which is calculated as the sample mean minus the hypothesized mean (H0: μ = 30) divided by the standard error of the mean. However, now we have a square root of 16 instead of the square root of 100, and the standard deviation (s) is still 10.</p>
<p>In this case, the test statistic follows a t-distribution with 15 degrees of freedom. Under the null hypothesis, the probability of the test statistic being larger than the 95th percentile of the t-distribution is 5%. To calculate this percentile, we can use the function <code>qt(0.95, 15)</code>, which gives a value of 1.7531.</p>
<p>If we plug in the values of s = 10 and x bar = 32 into the test statistic formula, we get a test statistic value of 0.8. Since 0.8 is smaller than 1.7531, we fail to reject the null hypothesis.</p>
<p>Next, let’s consider a two-sided test. In this scenario, we want to reject the null hypothesis if the mean is different from 30, regardless of whether it is too large or too small. To achieve a two-sided test, we need to split the probability equally in both tails of the distribution, resulting in a 2.5% probability in each tail.</p>
<p>We can calculate the critical values for the two-sided test by using the function <code>qt(0.975, 15)</code> and <code>qt(0.025, 15)</code>. These values represent the 2.5th percentile and 97.5th percentile of the t-distribution with 15 degrees of freedom. If the test statistic is larger than the positive critical value or smaller than the negative critical value, we reject the null hypothesis. Alternatively, we can take the absolute value of the test statistic and reject it if it is larger than the positive critical value.</p>
<p>In our example, since the test statistic is positive, we only need to consider the larger side. Since 0.8 is smaller than the positive critical value (2.5th percentile), we fail to reject the null hypothesis in the two-sided test.</p>
<p>In practice, instead of manually calculating the rejection region and performing hypothesis tests, it is common to use statistical functions like <code>t.test()</code> in R. These functions provide all the relevant statistics, including the test statistic, degrees of freedom, and p-values. They also offer the option to perform paired tests when appropriate.</p>
<p>As an illustration, we can use the <code>t.test()</code> function in R with the <code>father.son</code> dataset to test whether the population mean of son’s heights is equivalent to the population mean of father’s heights. Since the observations are paired, we can pass the difference between son’s and father’s heights directly to the function or specify <code>paired = TRUE</code> when passing the individual vectors. The <code>t.test()</code> function provides the t statistic, degrees of freedom, and automatically calculates the t confidence interval. By analyzing the output, we can determine the statistical significance and evaluate the practical significance of the results by examining the range of values in the confidence interval.
### Two group testing</p>
<p>Let’s revisit our example once again, this time considering a sample size of 16 instead of 100. The test statistic remains unchanged: it is calculated as the sample mean minus the hypothesized mean (H0: μ = 30), divided by the standard error of the mean. However, we now have a square root of 16 instead of the square root of 100, and the standard deviation (s) is still 10.</p>
<p>This test statistic follows a t-distribution with 15 degrees of freedom in this specific case. Under the null hypothesis, the probability of the test statistic being larger than the 95th percentile of the t-distribution is 5%. To calculate this percentile, we can use the function <code>qt(0.95, 15)</code>, which yields a value of 1.7531.</p>
<p>When we substitute the values s = 10 and x bar = 32 into the test statistic formula, we obtain a test statistic value of 0.8. Since 0.8 is smaller than 1.7531, we fail to reject the null hypothesis.</p>
<p>Now, let’s consider a two-sided test. Although it may not be meaningful in our specific scientific context, there are instances where a two-sided test is required in scientific settings, regardless of its practical significance. In this case, we want to reject the null hypothesis if the mean is different from 30, regardless of whether it is too large or too small.</p>
<p>To perform a two-sided test, we need to split the probability evenly in both tails of the distribution, resulting in a 2.5% probability in each tail. We can calculate the critical values for the two-sided test using the function <code>qt(0.975, 15)</code> and <code>qt(0.025, 15)</code>. These values represent the 2.5th percentile and 97.5th percentile of the t-distribution with 15 degrees of freedom.</p>
<p>In our example, since the test statistic is positive, we only need to consider the larger side. If the test statistic is larger than the positive critical value, we reject the null hypothesis. Alternatively, we can take the absolute value of the test statistic and reject it if it exceeds the positive critical value.</p>
<p>By comparing the test statistic value of 0.8 to the positive critical value, we fail to reject the null hypothesis in the two-sided test.</p>
<p>In practice, instead of manually calculating the rejection region and performing hypothesis tests, it is common to use statistical functions like <code>t.test()</code> in R. These functions provide all the relevant statistics, including the test statistic, degrees of freedom, and p-values. They also offer the option to perform paired tests when appropriate.</p>
<p>For example, let’s use the <code>t.test()</code> function in R with the <code>father.son</code> dataset to test whether the population mean of son’s heights is equivalent to the population mean of father’s heights. Since the observations are paired, we can directly pass the difference between son’s and father’s heights to the function or specify <code>paired = TRUE</code> when passing the individual vectors. The <code>t.test()</code> function will provide the t statistic, degrees of freedom, and automatically calculate the t confidence interval. It is useful to analyze both the statistical significance and the practical significance by examining the range of values in the confidence interval, which is expressed in the units of the data of interest.</p>
<p>Now, let’s consider the scenario where the observations are paired. In this case, we have measurements of one son paired with measurements of one father, and so on. We want to test whether the difference in heights between sons and fathers is zero or non-zero. To perform this test, we can use the <code>t.test()</code> function in R. We have two options: either pass the difference directly to the function or pass the two vectors and set the argument <code>paired = TRUE</code>.</p>
<p>By applying the <code>t.test()</code> function to our data, we obtain the t statistic of 11.79 and the degrees of freedom of 1,077. Since the t statistic is quite large, we reject the null hypothesis. It is worth noting that the degrees of freedom are also large in this case, making the distinction between a t-test and a z-test irrelevant.</p>
<p>The <code>t.test()</code> function conveniently provides the t confidence interval automatically. It is valuable to examine the confidence interval alongside the test output as it helps bridge the gap between statistical significance and practical significance. By assessing the range of values in the confidence interval, which is expressed in the units of the data of interest, we can determine whether they are practically meaningful or not.</p>
<p>In the previous lectures on confidence intervals, we explored whether a hypothesized mean was supported by checking if it fell within the confidence interval. Similarly, we performed a hypothesis test to determine if the mean was equal to a specific value or not. It turns out that these two procedures do not disagree. Checking whether the hypothesized mean (μ0) falls within the interval is equivalent to conducting a two-sided hypothesis test, with the caveat that the significance level (α) used for the interval should be equal to 1 minus the significance level (1-α) used for the hypothesis test.</p>
<p>In other words, if we construct a 95% confidence interval and check whether μ0 is within that interval, we fail to reject the null hypothesis if it is inside the interval and reject it if it is outside. This procedure aligns with performing the hypothesis test at a significance level of α. This relationship is stated in the slide, where the confidence interval can be seen as the set of all possible values for which we fail to reject the null hypothesis.</p>
<p>With the understanding of confidence intervals and hypothesis tests for one group in place, extending these concepts to two groups is a straightforward extension. The rejection rules remain the same, and now we want to test whether the means of two groups are equal or not. We have the same set of alternatives: μ1 &gt; μ2, μ1 &lt; μ2, or μ1 ≠ μ2. The test statistic remains the same, calculated as the difference between the sample means (X-bar1 - X-bar2) minus the hypothesized mean difference (μ1 - μ2), divided by the standard error of the mean.</p>
<p>To illustrate this, let’s consider the example of the <code>ChickWeight</code> dataset we examined in the previous lecture. We can load the dataset using <code>library(datasets)</code> and <code>data(ChickWeight)</code>. To reshape the data into the desired format, we may need to use the <code>reshape2</code> package. The <code>ChickWeight</code> dataset contains measurements for different chicks at various time points, represented in a long format.</p>
<p>We desired a wide format for our data, so we used the <code>dcast()</code> function to achieve that. Additionally, I renamed the resulting dataset and defined a new variable called “weight_gain,” which represents the difference between “time21” and “time0.” In this particular dataset, most chicks had nearly identical weights at “time0” relative to “time21.” Although this doesn’t significantly affect the results, I wanted to demonstrate the use of the <code>mutate()</code> function, which simplifies adding variables to a data frame.</p>
<p>To conduct the t-test, I selected a subset of the data where the diet was either 1 or 4. This was necessary because the tilde operator in the <code>t.test()</code> function requires the predictor variable (diet) to have exactly two levels when comparing groups. By setting <code>paired = FALSE</code>, we indicate that the chicks receiving diet 1 and diet 4 are completely separate groups, and there is no pairing between them. Chick 1 from diet 1 has no connection to chick 1 from diet 4.</p>
<p>In the previous lecture, we discussed how assuming equal variances may not be the best approach for this dataset. Therefore, I suggest trying the example with <code>var.equal = FALSE</code> to observe how the results change.</p>
<p>The resulting t statistic represents the estimate of the difference in average weight gain between the two diets, minus the hypothesized value of 0. When comparing two groups, unless a specific hypothesized difference in means is specified, the default assumption is that we are testing whether the means are equal under the null hypothesis or different under the alternative. The degrees of freedom are calculated as n1 + n2 - 2, which we covered in the confidence interval lecture. If unequal variances are used, fractional degrees of freedom may be obtained.</p>
<p>The output also provides the confidence interval, which is always useful to examine when performing a hypothesis test.</p>
<p>In the next lecture, we will discuss the concept of p-values and how they facilitate hypothesis testing. The calculated T statistic of -2.7 indicates how many estimated standard errors the difference in means is from the hypothesized mean. Since it is far into the tail of the t-distribution or normal distribution, it falls well below our cutoff value. Although we don’t explicitly determine a cutoff value in this case, we can immediately conclude, after learning about p-values, that this result would be rejected in a 5% level test without calculating the specific t quantiles.</p>
<p>Let’s now consider a simple example of hypothesis testing that does not involve a normal or t distribution. Suppose you have a friend who has eight children, with seven of them being girls. You want to evaluate whether this supports the belief that the genders are independent and equally likely (like a fair coin flip). You want to test the null hypothesis that the probability of having a girl is 0.5 against the alternative hypothesis that it is greater than 0.5, as you are slightly skeptical.</p>
<p>To determine the number of girls the couple could have for the probability of having that many or more to be less than 5% under the null hypothesis of a fair coin, we can set up a rejection region. However, if we consider a rejection region from zero to eight girls, we would always reject the null hypothesis.</p>
<p>If we set up a rejection region where we would reject the null hypothesis if the couple had one to eight girls, we still wouldn’t achieve a 5% significance level. It would be nearly one, indicating a very high rejection rate. However, if we consider having seven or eight girls as the rejection region, the probability of rejecting under the null hypothesis is just under 5%. It is worth noting that we cannot achieve an exact 5% level test in this case due to the discrete nature of the binomial distribution. For larger sample sizes, a normal approximation could have been used by treating the coin flip outcomes as averages and assuming a Gaussian distribution. However, you already know how to handle that.</p>
<p>In this specific test, we observe that the closest rejection region consists of having seven or eight girls. Since your friend had seven girls, we would reject the null hypothesis based on this observation. However, it’s important to acknowledge that an exact 5% level test is not feasible in this case due to the discrete nature of the binomial distribution. For two-sided tests, the approach is not obvious. We will discuss a method for conducting two-sided tests in the next lecture, and I believe it will become clearer when we introduce the concept of p-values. If this example seems confusing, the lecture on p-values will help clarify it. The exact binomial or Poisson tests will become easier to comprehend.</p>
<p>It is worth mentioning that if you can perform a two-sided test for a binomial or Poisson distribution, you can also invert those tests. By considering the values for which you would fail to reject the null hypothesis, you can generate exact confidence intervals for the binomial and Poisson parameters. This is precisely how R calculates exact binomial intervals, without relying on asymptotic or central limit theorem approximations. They invert a two-sided hypothesis test of this nature.</p>
<p>I look forward to the next lecture where we will delve into p-values. It will solidify these concepts and make the execution of hypothesis tests a bit more straightforward.</p>
</div>
</div>
<div id="p-values" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> P values</h2>
<p>P-values are a convenient way to communicate the results of a hypothesis test. When communicating a P-value, the reader can perform the test at whatever Type I error rate that they would like. Just compare the P-value to the desired Type I error rate and if the P-value is smaller, reject the null hypothesis.</p>
<p>Formally, the P-value is the probability of getting data as or more extreme than the observed data in favor of the alternative. The probability calculation is done assuming that the null is true. In other words if we get a very large T statistic the P-value answers the question “How likely would it be to get a statistic this large or larger if the null was actually true?”. If the answer to that question is “very unlikely”, in other words the P-value is very small, then it sheds doubt on the null being true, since you actually observed a statistic that extreme.</p>
<p>Hello, I’m Brian Caffo, and this lecture is about p-values in the statistical inference Coursera class as part of our data science specialization. I co-teach this class with my colleagues Jeff Leek and Roger Peng, and we are all from the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health. P-values are widely used as a measure of statistical significance. Almost every statistical software that performs hypothesis tests provides a p-value as an output. However, due to their popularity and frequent misinterpretation, p-values have become a subject of controversy among statisticians. In this class, we will not focus extensively on these controversies. Instead, our main goal is to understand how to generate p-values correctly and interpret them appropriately.</p>
<p>The fundamental concept of a p-value is to start by assuming the null hypothesis, which assumes no effect or relationship, and then calculate the probability of obtaining evidence as extreme or more extreme than the evidence observed under this null hypothesis. In other words, we assess how unusual our observed result is if the null hypothesis were true. Let’s follow a simple three-step approach, and then we will delve into the calculations in the subsequent slides.</p>
<p>Firstly, we establish the hypothetical distribution of a summary statistic, often referred to as a test statistic, such as the t-statistic from the t-test lecture, assuming no effect or relationship (null distribution). Secondly, we calculate the test statistic using the actual data we have. For example, in the case of a t-test, we substitute the empirical mean, subtract the hypothesized mean, and divide by the standard error. Finally, we determine the probability of obtaining a test statistic as extreme or more extreme than the one calculated. In other words, we compare our calculated test statistic to the hypothetical distribution and assess its level of extremity towards the alternative hypothesis. If the p-value is small, it indicates that the probability of observing a test statistic as extreme as the one we observed is low under the assumption that the null hypothesis is true.</p>
<p>Let’s discuss p-values with a bit more formality. The p-value is the probability, under the null hypothesis, of obtaining evidence as extreme or more extreme than what was actually observed. Typically, this evidence refers to the test statistic. Therefore, the p-value represents the probability of obtaining a test statistic as extreme or more extreme in favor of the alternative hypothesis than the observed test statistic. If the p-value is small, it suggests that either the null hypothesis is true, and we have observed something highly supportive of the alternative that is unlikely to occur, or the null hypothesis itself is false.</p>
<p>To illustrate this concept, let’s consider a numerical example using the t-statistic and a simple t-test. Suppose we want to test the null hypothesis mu = mu_0 versus the alternative hypothesis mu &gt; mu_0. If our calculated t-statistic is 2.5 with 15 degrees of freedom, we can determine the probability of obtaining a t-statistic as large as 2.5 in this scenario. By calculating pt(2.5, 15, lower.tail = false), we find that the probability is approximately 1%.</p>
<p>Therefore, the probability of observing evidence as extreme or more extreme than what was actually obtained under the null hypothesis is 1%. This suggests that either the null hypothesis is true and we have observed an unusually large test statistic, or the null hypothesis is false. Another way to interpret the p-value is as the attained significance level. Let’s explore this concept briefly.</p>
<p>Consider an example where our test statistic is 2 for the null hypothesis mu = 30 versus the alternative hypothesis mu &gt; 30. Test statistics larger than 2 provide stronger evidence in favor of the alternative hypothesis, where 2 represents two standard errors above the hypothesized mean of 30. Assuming our test statistic follows a standard normal distribution instead of a t-distribution for simplicity, if we set alpha to 0.05, we would reject the null hypothesis because the test statistic lies above the critical value of 1.645 corresponding to an alpha of 0.05.</p>
<p>Now, imagine if we set alpha to 0.04, resulting in a slightly closer critical value than 1.645. What if we found the exact error rate where the critical value aligns exactly with 2? That would be equivalent to calculating the probability of obtaining a test statistic as large or larger than 2 under the null hypothesis, which is nothing other than the p-value we calculated. In essence, the p-value represents the smallest alpha level for which we would still reject the null hypothesis. Hence, it is referred to as the attained significance level.</p>
<p>The advantage of the p-value is that it provides a convenient test statistic that can be interpreted by others. When you report a p-value, the reader or recipient can perform the hypothesis test at any alpha level they choose. The simple rule is that if the p-value is less than the chosen alpha level, the null hypothesis is rejected. If the p-value is greater than the alpha level, the null hypothesis is not rejected. It’s worth noting that for one-sided hypothesis tests using t-tests or z-tests, the calculated p-value already accounts for evidence as extreme or more extreme in one direction. However, for two-sided hypothesis tests, where evidence in both tails is considered equally probable, the p-value needs to be doubled.</p>
<p>I would like to caution that most statistical software automatically interprets the p-value as a two-sided test for most cases. If it’s not explicitly mentioned, the calculated p-value is for the two-sided test. Additionally, in more advanced statistics classes covering tests like the chi-squared test, the calculated p-values are inherently two-sided, and there’s no need to double them.</p>
<p>In a previous class, we discussed an example that illustrates the concept of p-values, and I’d like to revisit it now that we have a better understanding. Let’s consider the scenario of gender assignment for children, treating it as a coin flip for a specific couple. Suppose you have a friend who has had seven girls out of eight kids, and you want to determine the probability that the coin lands on a girl, denoted by p. We are interested in testing whether p is equal to 0.5 or greater than 0.5, with the null hypothesis H0: p = 0.5 and the alternative hypothesis Ha: p &gt; 0.5.</p>
<p>Under the null hypothesis, we calculate the probability of obtaining evidence as extreme or more extreme. In this case, the most logical test statistic is the count of girls out of eight. The p-value calculation involves considering the binomial probability of observing seven or eight girls, assuming p is 0.5. This calculation yields a p-value of approximately 3.5%. Alternatively, you can use the pbinom function to directly calculate the p-value, resulting in the same value. If we were conducting a hypothesis test, we would reject the null hypothesis at a 5% level and also at a 4% level. However, we would not reject it at a type 1 error rate of 3%.</p>
<p>It’s important to note that in this specific problem, the calculation of the two-sided p-value is not obvious. To address this, a simple trick is to calculate the two one-sided p-values. For instance, the probability of having seven or more girls represents one-sided p-value, and the probability of having seven or fewer girls represents the other one-sided p-value. Taking the smaller of these two p-values and doubling it gives us the two-sided p-value for binomial exact calculations.</p>
<p>Now let’s move on to a Poisson example. Imagine a hospital that has an infection rate of 10 infections per 100 person-days at risk, equivalent to a rate of 0.1 infections per person-day at risk during the last monitoring period. The hospital considers a rate of 0.05 infections per person-day at risk as an important benchmark, and they would implement quality control procedures if the rate exceeds this threshold. However, they don’t want to trigger these procedures based on random fluctuations alone. To formally test this hypothesis, accounting for the data’s uncertainty, we assume that the count of infections follows a Poisson distribution.</p>
<p>The null hypothesis states that lambda (the rate) is 0.05, while the alternative hypothesis suggests that lambda is greater than 0.05. In this case, we want to determine the probability of observing 10 or more infections if the true infection rate for 100 person-days at risk is 5. Using the ppois function in R, we calculate the upper tail probability. Due to a quirk in R’s syntax, we need to input 9 instead of 10 as the value and set lower.tail = FALSE to obtain the probability of strictly greater than 9. The resulting p-value indicates the probability of obtaining 10 or more infections when the true rate is 5 for 100 person-days at risk. In this example, the probability is approximately 3%, suggesting a relatively low likelihood of observing as many as 10 infections for 100 person-days at risk. Thus, the hospital may consider implementing quality control procedures.</p>
<p>To summarize, the calculation of a p-value involves determining the probability of obtaining data as extreme or more extreme than what was observed in favor of the alternative hypothesis, with the probability calculation performed under the null hypothesis. This approach applies to all p-values, and we have explored the formal rules for executing z-tests and t-tests, as well as examples involving the binomial distribution and the Poisson distribution.</p>
</div>
<div id="knitr" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Knitr</h2>
<p>For the course project, you’ll need to use knitr. In the following video, I give you just enough knitr to do the project. If you haven’t already, Roger Peng’s course on Reproducible Research covers knitr quite well.</p>
<p>Hello everyone! In this brief tutorial, I’ll walk you through the basics of using Knitr for your art project, which you’ll be submitting as part of your assignments. I’ll be demonstrating the process using RStudio. So let’s get started.</p>
<p>First, open RStudio and navigate to “File” (Alt-F) and select “New File.” You’ll see various options, and you should choose “R Markdown.” This will generate a simple Knitr document for you. Feel free to edit the title; for example, you can rename it to “Test Knitr Document.”</p>
<p>In the document, you’ll notice R commands and some formatting. To execute R code within the document, you need to use a set of three backticks or quotation marks (usually found below the Escape key on the keyboard). After the initial backticks, add an “r” to indicate that you’re using R code, followed by a comma to open up additional options.</p>
<p>Knitr offers numerous options, but I’ll highlight a few essential ones. The “cache” option determines whether R should store the code’s results. Another useful option is “eval,” which specifies whether the code should be evaluated or simply displayed in the document. You can choose to display code with results or hide code using the “results” option. Additionally, the “echo” option controls whether the code is displayed or not.</p>
<p>For example, the document includes a code snippet demonstrating a plot using the command <code>plot(cars)</code>. Feel free to modify and add your own code snippets.</p>
<p>Once you’ve made your changes, save the document. Give it a name like “test.Rmd.” To knit the document into an HTML format, you can either use the Knit HTML button in the toolbar or go to “Code” and select “Knit Document.” This will create an HTML document displaying the code, results, and any additional content.</p>
<p>If you want to view the HTML document, you can find it in the working directory. In R, you can use the <code>dir()</code> function to see the files, and then use the <code>browseURL()</code> function to open the HTML document in a web browser. For example, <code>browseURL("test.html")</code> will open the HTML document named “test.html.”</p>
<p>That’s the basic process of using Knitr in a nutshell. Feel free to explore the additional options and customization features to create dynamic and interactive documents for your art project.</p>
</div>
<div id="practical-r-exercises-in-swirl-2" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Practical R Exercises in swirl</h2>

</div>
</div>
<hr>
<center> 
  <div class="footer">
      All illustrations <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY. </a>
      <br>
      All other materials <a href= "https://creativecommons.org/licenses/by/4.0/"> CC-BY </a> unless noted otherwise.
  </div>
</center>
            </section>

          </div>
        </div>
      </div>
<a href="variability-distribution-asymptotics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="power-bootstrapping-permutation-tests.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
