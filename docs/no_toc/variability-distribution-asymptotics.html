<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Variability, Distribution, &amp; Asymptotics | Course Name</title>
  <meta name="description" content="Description about Course/Book." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Variability, Distribution, &amp; Asymptotics | Course Name" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Description about Course/Book." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Variability, Distribution, &amp; Asymptotics | Course Name" />
  
  <meta name="twitter:description" content="Description about Course/Book." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="assets/dasl_favicon.ico" type="image/x-icon" />
<link rel="prev" href="probability-expected-values.html"/>
<link rel="next" href="intervals-testing-pvalues.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<a href="http://jhudatascience.org/"><img src="https://jhudatascience.org/images/dasl.png" style=" width: 80%; padding-left: 40px; padding-top: 8px; vertical-align: top "</a>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Course</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#available-course-formats"><i class="fa fa-check"></i><b>0.1</b> Available course formats</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="probability-expected-values.html"><a href="probability-expected-values.html"><i class="fa fa-check"></i><b>1</b> Probability &amp; Expected Values</a>
<ul>
<li class="chapter" data-level="1.0.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#syllabus"><i class="fa fa-check"></i><b>1.0.1</b> Syllabus</a></li>
<li class="chapter" data-level="1.0.2" data-path="probability-expected-values.html"><a href="probability-expected-values.html#course-book-statistical-inference-for-data-science"><i class="fa fa-check"></i><b>1.0.2</b> Course Book: Statistical Inference for Data Science</a></li>
<li class="chapter" data-level="1.0.3" data-path="probability-expected-values.html"><a href="probability-expected-values.html#homework-problems"><i class="fa fa-check"></i><b>1.0.3</b> Homework Problems</a></li>
<li class="chapter" data-level="1.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="probability-expected-values.html"><a href="probability-expected-values.html#probability"><i class="fa fa-check"></i><b>1.2</b> Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#probability-mass-functions-and-probability-density-functions"><i class="fa fa-check"></i><b>1.2.1</b> Probability mass functions and probability density functions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="probability-expected-values.html"><a href="probability-expected-values.html#conditional-probability"><i class="fa fa-check"></i><b>1.3</b> Conditional probability</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#independence"><i class="fa fa-check"></i><b>1.3.1</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="probability-expected-values.html"><a href="probability-expected-values.html#expected-values"><i class="fa fa-check"></i><b>1.4</b> Expected values</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html"><i class="fa fa-check"></i><b>2</b> Variability, Distribution, &amp; Asymptotics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#variability"><i class="fa fa-check"></i><b>2.1</b> Variability</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#variance-simulation-examples"><i class="fa fa-check"></i><b>2.1.1</b> Variance simulation examples</a></li>
<li class="chapter" data-level="2.1.2" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#variance-data-example"><i class="fa fa-check"></i><b>2.1.2</b> Variance data example</a></li>
<li class="chapter" data-level="2.1.3" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#poisson-distribution"><i class="fa fa-check"></i><b>2.1.3</b> Poisson distribution</a></li>
<li class="chapter" data-level="2.1.4" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#asymptotics-and-the-clt"><i class="fa fa-check"></i><b>2.1.4</b> Asymptotics and the CLT</a></li>
<li class="chapter" data-level="2.1.5" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#asymptotics-and-confidence-intervals"><i class="fa fa-check"></i><b>2.1.5</b> Asymptotics and confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#practical-r-exercises-in-swirl"><i class="fa fa-check"></i><b>2.2</b> Practical R Exercises in swirl</a></li>
<li class="chapter" data-level="2.3" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#quiz"><i class="fa fa-check"></i><b>2.3</b> Quiz</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html"><i class="fa fa-check"></i><b>3</b> Intervals, Testing, &amp; Pvalues</a>
<ul>
<li class="chapter" data-level="3.0.1" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#independent-group-t-intervals"><i class="fa fa-check"></i><b>3.0.1</b> Independent group T intervals</a></li>
<li class="chapter" data-level="3.1" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.1</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#t-tests"><i class="fa fa-check"></i><b>3.1.1</b> T tests</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#p-values"><i class="fa fa-check"></i><b>3.2</b> P values</a></li>
<li class="chapter" data-level="3.3" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#knitr"><i class="fa fa-check"></i><b>3.3</b> Knitr</a></li>
<li class="chapter" data-level="3.4" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#practical-r-exercises-in-swirl-1"><i class="fa fa-check"></i><b>3.4</b> Practical R Exercises in swirl</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html"><i class="fa fa-check"></i><b>4</b> Power, Bootstrapping, &amp; Permutation Tests</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#calculating-power"><i class="fa fa-check"></i><b>4.0.1</b> Calculating Power</a></li>
<li class="chapter" data-level="4.1" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#multiple-comparisons"><i class="fa fa-check"></i><b>4.1</b> Multiple Comparisons</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="5" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>5</b> References</a></li>
<li class="divider"></li>
<p style="text-align:center;"> <a href="https://github.com/jhudsl/OTTR_Template" target="blank" > This content was published with</a> <a href="https://bookdown.org/" target="blank"> bookdown by:</a> </p>
<p style="text-align:center;"> <a href="http://jhudatascience.org/"> The Johns Hopkins Data Science Lab </a></p>
<p style="text-align:center; font-size: 12px;"> <a href="https://github.com/rstudio4edu/rstudio4edu-book/"> Style adapted from: rstudio4edu-book </a> <a href ="https://creativecommons.org/licenses/by/2.0/"> (CC-BY 2.0) </a></p>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Course Name</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<head>
  <meta name="viewport" content="width=device-width,minimum-scale=1.0,maximum-scale=1.0,initial-scale=1.0">
  <!--script src="https://kit.fontawesome.com/6a26f47516.js"></script-->
  <!--<script src="assets/hideOutput.js"></script>-->
  <link href="assets/style.css" rel="stylesheet">
</head>



<div class="hero-image-container">
  <img class= "hero-image" src= "https://github.com/jhudsl/OTTR_Template/raw/main/assets/dasl_thin_main_image.png">
</div>
<div id="variability-distribution-asymptotics" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Variability, Distribution, &amp; Asymptotics</h1>
<div id="variability" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Variability</h2>
<p>An important characterization of a population is how spread out it is. One of the key measures of spread is variability. We measure population variability with the sample variance, or more often we consider the square root of both, called the standard deviation. The reason for taking the standard deviation is because that measure has the same units as the population. So if our population is a length measurement in meters, the standard deviation is in meters (whereas the variance is in meters squared).</p>
<p>Variability has many important uses in statistics. First, the population variance is itself an intrinsically interesting quantity that we want to estimate. Secondly, variability in our estimates is what makes them not imprecise. An important aspect of statistics is quantifying the variability in our estimates.</p>
<p>In the previous lecture, we discussed the population mean as a measure of the center of a distribution. Now, let’s explore another important property called variance, which describes the spread or concentration of the density around the mean. If we imagine a bell curve, the probability density function will shift to the left or right as the mean changes. Variance quantifies how widely or narrowly the density is distributed around the mean.</p>
<p>For a random variable X with a mean μ, the variance is precisely the expected squared distance between the random variable and the mean. I provide the formula here, but there’s also a useful shortcut: the expected value of X squared minus the square of the expected value of X. Densities with higher variance are more spread out compared to those with lower variances. The square root of variance is known as the standard deviation, which is expressed in the same units as X.</p>
<p>In this class, we won’t spend much time manually calculating expected values or variances for populations. However, let’s go through one such calculation. In the previous lecture, we found that the expected value of X, when rolling a die, is 3.5. To calculate the expected value of X squared, we square each number (1, 2, 3, 4, 5) and multiply them by their associated probabilities. Summing these values gives us 15.17. By subtracting (15.17 - 3.5) squared, we find the variance of a die roll to be 2.92.</p>
<p>Now, let’s move on to another example. Consider tossing a coin with a probability of heads, p. From the previous lecture, we know that the expected value of a coin toss is p. When calculating the expected value of X squared, 0 squared is 0, and 1 squared is 1. Thus, the expected value of X squared is p. Plugging these values into our formula, we get p - p squared, which simplifies to p(1 - p). This formula is widely recognized and it’s advisable to memorize it.</p>
<p>I’m providing examples of population densities with varying variances. The salmon-colored density represents a standard normal distribution with a variance of 1. As the variance increases, the density becomes flatter and spreads more into the tails. Consequently, if someone is from a normal distribution with a variance of 4, they are more likely to have a value beyond 5 compared to someone from a normal distribution with a variance of 3.</p>
<p>Similar to the relationship between population mean and sample mean, the population variance and sample variance are directly analogous. The population mean represents the center of mass of the population, while the sample mean represents the center of mass of the observed data. Similarly, the population variance quantifies the expected squared distance of a random variable from the population mean, while the sample variance measures the average squared distance of the observed data points from the sample mean. Note that in the denominator of the sample variance formula, we divide by (n - 1) instead of n, and I will explain why in a moment.</p>
<p>I want to address a conceptually challenging point, which is the variance of the sample variance. Recall that the sample variance is a function of the data, making it a random variable with its own population distribution. The expected value of this distribution corresponds to the population variance being estimated by the sample variance. As we gather more data, the distribution of the sample variance becomes increasingly concentrated around the population variance it seeks to estimate.</p>
<p>Lastly, I’d like to remind you that the square root of the sample variance is the sample standard deviation, which provides a measure of dispersion that is more interpretable since it is in the same units as X.</p>
<div id="variance-simulation-examples" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Variance simulation examples</h3>
<p>Let’s consider the following scenario. Suppose I simulate ten standard normal random variables and calculate their sample variance. If I repeat this process many times, I will obtain a distribution of sample variances. This distribution, represented by the salmon-colored density, emerges from repeating the process thousands of times. What I mentioned earlier, and what Rissotto discussed in the previous slide, is that if I sample enough data points, the center of mass of this distribution will precisely match the variance of the original population I was sampling from—the standard normal distribution with a variance of one.</p>
<p>The same holds true when I consider sample variances based on 20 observations from the standard normal distribution. I repeat the process of sampling 20 standard normals, calculating the sample variance, and obtaining a distribution of sample variances. This distribution, depicted in a more aqua color, is also centered at one.</p>
<p>The pattern continues when I examine sample variances based on 30 observations. However, what’s interesting to note is that as the sample size increases, the variance of the population distribution of the sample variances becomes more concentrated. In simpler terms, collecting more data leads to a better and more tightly focused estimate of what the sample variance is trying to estimate. In this case, all the sample variances are estimating a population variance of one because they are sampled from a population with a variance of one.</p>
<p>In an earlier lecture, we found that the variance of a die roll was 2.92. Now, imagine if I were to roll ten dice and calculate the sample variance of the numbers on the sides facing up. By repeating this process numerous times, I can obtain a reliable understanding of the population distribution of the variance of ten die rolls. Although it requires a large number of repetitions, with the help of a computer, I can simulate this process thousands of times, as demonstrated here. Notice that the distribution of the variance of ten die rolls is precisely centered around 2.92, which is the variance of a single die roll. As I increase the number of dice to 20 and 30, the center of the distribution remains the same, but it becomes more concentrated around the true population variance. This indicates that the sample variance provides a good estimate of the population variance. As we collect more data, the distribution of the sample variance becomes increasingly concentrated around the true value it aims to estimate, demonstrating its unbiasedness.</p>
<p>The reason we divide by (n - 1) instead of n in the denominator of the sample variance formula is to ensure its unbiasedness.
### Standard error of the mean
Now that we have extensively discussed variances and briefly touched upon the distribution of sample variances, let’s revisit the distribution of sample means. It is important to remember that the average of numbers sampled from a population is a random variable with its own population mean and population variance. The population mean remains the same as the original population, while the variance of the sample mean can be related to the variance of the original population. Specifically, the variance of the sample mean decreases to zero as more data is accumulated. This means that the sample mean becomes more concentrated around the population mean it is trying to estimate, which is a valuable characteristic since we usually only have one sample mean in a given dataset.</p>
<p>Although we do not have multiple repeated sample means to investigate their variability like we do in simulation experiments, we can still estimate the population variance, denoted as sigma squared, using the available data. With knowledge of sigma squared and the sample size (denoted as n), we can gather valuable information about the distribution of the sample mean. The square root of the statistic, sigma over square root n, is referred to as the standard error of the mean, denoted as the standard deviation of the distribution of a statistic. The term “standard error” is used to represent the variability of means, while the standard error of a regression coefficient describes the variability in regression coefficients.</p>
<p>In summary, considering a population with a mean (mew) and variance (sigma squared), when we draw a random sample from that population and calculate the variance, it serves as an estimate of sigma squared. Similarly, when we calculate the mean, it estimates mu (population mean). However, s squared (sample variance) is also a random variable with its own distribution centered around sigma squared, becoming more concentrated around it as more observations contribute to the squared value. Additionally, the distribution of sample means from that population is centered at mu and becomes more concentrated around mu as more observations are included. Moreover, we precisely know the variance of the distribution of sample means, which is sigma squared divided by n.</p>
<p>Since we lack repeated sample means in a given dataset, we estimate the sample variance of the mean as s squared divided by n and the logical estimate of the standard error as s over square root n. The standard error of the mean (or the sample standard error of the mean) is defined as s over square root n. The standard deviation (s) is an estimate of the variability of the population, while the standard error (s over square root n) represents the variability of averages of random samples of size n from the population.</p>
<p>To illustrate these concepts, let’s consider some simulation examples. If we take standard normals (with a variance of one), the standard deviation of means of n standard normals is expected to be one over square root n. By simulating multiple draws of ten standard normals and calculating their mean, followed by taking the standard deviation of these averages, we should obtain an approximate value of one over square root n. Similar simulations can be performed for standard uniforms (variance of 1/12), Poisson distributions (variance of 4), and coin flips (variance of p times 1 minus p, assuming p is a half). The results of these simulations should align with the theoretical values predicted by our rule.</p>
<p>In conclusion, understanding the standard error of the mean is crucial in determining the variability of sample means. Simulation experiments can help illustrate these concepts, especially when investigating the distribution of sample means and estimating their standard error.</p>
</div>
<div id="variance-data-example" class="section level3" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Variance data example</h3>
<p>Now, let’s dive into a practical example using the father-son data from our dataset. We will focus on the height of the sons, with “n” representing the number of observations as usual. If we plot a histogram of the son’s height and overlay it with a continuous density estimate, we observe a distribution that closely resembles a Gaussian curve. This density estimate provides an approximation of the population density, given the finite amount of data we have collected. The histogram’s variability, which the sample variance calculates, serves as an estimate of the variability in son’s height from the population this data was drawn from, assuming it was a random sample.</p>
<p>Now, let’s go through some calculated values. By calculating the variance of x, variance of x divided by n, standard deviation of x, and standard deviation of x divided by square root n, and rounding them to two decimal places, we obtain 7.92 and 2.81 as the variance of x and the standard deviation of x, respectively. These numbers represent the variability in son’s heights from the dataset and act as estimates of the population variability of son’s heights if we assume these sons are a random sample from a meaningful population. In this case, I prefer the value 2.81 over 7.92 since 7.92 is expressed in inches squared, while 2.81 is expressed in inches. Working with the actual units is more intuitive.</p>
<p>Moving on to 0.01 and 0.09, these values no longer reflect the variability in children’s heights. Instead, they represent the variability in averages of ten children’s heights. The value 0.09 is particularly meaningful as it represents the standard error or the standard deviation in the distribution of averages of n children’s heights. While it’s an estimate based on the available data, it’s the best estimate we can derive from the dataset.</p>
<p>To summarize what we have learned, this lecture covered several complex topics, but at its core, understanding variability is the key to understanding statistics. In fact, grasping the concept of variability might be the most crucial aspect of statistics. Here’s a summary of our findings: the sample variance provides an estimate of the population variance, and the distribution of the sample variance is centered around the value it is estimating, indicating an unbiased estimation. Moreover, as more data is collected, the distribution becomes more concentrated around the estimated value, leading to a better estimate. We have also gained insights into the distribution of sample means. In addition to knowing its center, as discussed in the previous lecture, we now understand that the variance of the sample mean is the population variance divided by n, and its square root, sigma divided by the square root of n, is known as the standard error. These quantities capture the variability of averages drawn from the population, and surprisingly, even though we only have access to one sample mean in a given dataset, we can make substantial inferences about the distribution of averages from random samples. This knowledge provides us with a solid foundation for various statistical analyses and methodologies.
## Distributions
Some probability distributions are so important that we need to internalize their characteristics. In these lectures we cover the most important probability distributions.
### Binomial distribution
Let’s begin with the simplest distribution, known as the Bernoulli distribution, named after Jacob Bernoulli, a renowned mathematician from a distinguished family of mathematicians. If you’re interested, you can explore the Bernoulli family further through their Wikipedia pages. The Bernoulli distribution originates from a coin flip, where a “0” represents tails and a “1” represents heads. We can consider a potentially biased coin with a probability “p” for heads and “1 - p” for tails. The Bernoulli probability mass function is typically denoted as “p^x * (1 - p)^(1 - x).” As we have seen before, the mean of a Bernoulli random variable is “p,” and the variance is “p * (1 - p).”</p>
<p>In the context of a Bernoulli random variable, we often refer to “x = 1” as a success, irrespective of the specific definition of success in a given scenario, and “x = 0” as a failure. Now, let’s move on to discussing the binomial distribution. A binomial random variable is obtained by summing up a series of independent and identically distributed (iid) Bernoulli random variables. Essentially, a binomial random variable represents the total number of heads obtained in a series of coin flips with a potentially biased coin. Mathematically, if we let “x1” to “xn” be iid Bernoulli variables with parameter “p,” then the sum of these variables, denoted as “x,” is a binomial random variable.</p>
<p>The binomial probability mass function closely resembles the Bernoulli mass function, but with the inclusion of “n choose x” in front. The notation “n choose x” represents the binomial coefficient, calculated as “n factorial / (x factorial * (n - x) factorial).” It is worth noting that “n choose 0” and “n choose n” both equal 1. This coefficient helps solve a common combinatorial problem, counting the number of ways to select “x” items out of “n” without replacement while disregarding the ordering of the items.</p>
<p>To illustrate a binomial calculation, let’s consider an example. Suppose your friend has eight children, with seven of them being girls (and no twins). Assuming each gender has an independent 50% probability for each birth, what is the probability of having seven or more girls out of eight births? We can apply the binomial formula to calculate this probability: “P(seven or more girls) = (8 choose 7) * 0.5^7 * (1 - 0.5)^1 + (8 choose 8) * 0.5^8 * (1 - 0.5)^0.” The result turns out to be a 4% chance.</p>
<p>In the provided R code, you can find the implementation of this calculation. Furthermore, for most common distributions, including the binomial distribution, there are built-in functions in R. For example, the “pbinom” function can be used to obtain these probabilities conveniently.
### Normal distribution
Probabilities play a crucial role in statistics, and among all the distributions, the normal distribution stands out as the most important one. In the upcoming lecture, we will explore why it holds such significance. In fact, if all distributions were to gather and elect a leader, the normal distribution would undoubtedly take the crown.</p>
<p>Let’s start by understanding a random variable that follows a normal (Gaussian) distribution with a mean of μ and a variance of σ squared. This distribution is characterized by a density function that resembles a bell curve, as we will illustrate shortly. If we have a random variable X with this density, its expected value is μ, and its variance is σ squared. We can express this concisely as X ~ N(μ, σ^2), denoting a normal distribution with mean μ and variance σ squared. When μ equals 0 and σ equals 1, the resulting distribution is known as the standard normal distribution. Standard normal random variables are often denoted by the letter z. Here, we depict the standard normal density function, which represents the famous bell curve you have likely encountered before. It is important to note that for the standard normal distribution, the mean is 0, and the standard deviation (and variance) is 1. In the diagram, we illustrate one standard deviation above and below the mean, two standard deviations above and below the mean, and three standard deviations above and below the mean. The units on the standard normal distribution can be interpreted as standard deviation units. For example, moving one unit in either direction corresponds to one standard deviation. Additionally, it is worth mentioning that statisticians often find it convenient to revert to the standard normal distribution when discussing normal probabilities, even when dealing with non-standard normal distributions. Therefore, if you want to calculate the probability that a non-standard normal lies between μ + 1σ and μ - 1σ (where μ and σ are specific to its distribution), the probability area is equivalent to that between -1 and +1 on the standard normal distribution. In essence, all normal distributions have the same underlying shape, with the only difference being the units along the axis. By reverting to standard deviations from the mean, all probabilities and calculations can be transformed back to those associated with the standard normal distribution. We will explore some examples to solidify this concept.</p>
<p>Now, let’s discuss some fundamental reference probabilities related to the standard normal distribution and use visual aids to help us remember them. First, consider one standard deviation from the mean in the standard normal distribution (or any normal distribution). Approximately 34% of the distribution lies on each side, resulting in a total area of 68% within one standard deviation. Moving on to two standard deviations, denoted by the magenta area in the diagram, around 95% of the distribution falls within this range for any normal distribution. This leaves 2.5% in each tail, and we often utilize this information when calculating confidence intervals. Lastly, when considering three standard deviations from the mean, the area encompasses approximately 99% of the distribution’s mass, although it may be difficult to discern from the diagram. These reference probabilities are essential to commit to memory.</p>
<p>In summary, probabilities are a fundamental concept, and the normal distribution holds a special place in statistics. Understanding its properties and the relationship to the standard normal distribution allows us to solve problems effectively. All normal distributions share the same essential shape, differing only in their units along the axis. By leveraging the standard normal distribution and converting to standard deviations from the mean, we can simplify calculations and derive consistent results.</p>
<p>The primary difference between different normal distributions lies in the units along the axis. When discussing normal probabilities and converting to standard deviations from the mean, all probabilities and calculations revert back to those associated with the standard normal distribution. To illustrate this concept, let’s consider some basic reference probabilities on the standard normal distribution, which can aid our understanding.</p>
<p>First, let’s focus on one standard deviation from the mean in the standard normal distribution (or any normal distribution). Roughly 34% of the distribution lies on each side, resulting in a total area of 68% within one standard deviation. Moving on to two standard deviations, represented by the magenta area, approximately 95% of the distribution falls within this range for any normal distribution. This leaves 2.5% in each tail, and we often utilize this information when calculating confidence intervals. Lastly, considering three standard deviations from the mean, the area encompasses approximately 99% of the distribution’s mass. Although it may be challenging to read from the diagram, this region represents about 99% of the total probability. These reference probabilities should be committed to memory for future calculations.</p>
<p>Let’s now discuss some simple rules for converting between standard and non-standard normal distributions. If we have a random variable X that follows a normal distribution with a mean of μ and variance of σ squared, we can convert the units of X to standard deviations from the mean by subtracting the mean μ and dividing by the standard deviation σ. The resulting random variable Z will follow a standard normal distribution. Conversely, if we start with a standard normal random variable Z and want to convert back to the units of the original data, we multiply Z by σ and add μ. The resulting random variable X will then follow a non-standard normal distribution with a mean of μ and variance of σ squared. We have already covered the first bullet point, which indicates that 68%, 95%, and 99% of a normal distribution lie within 1, 2, and 3 standard deviations from the mean, respectively.</p>
<p>Let’s also discuss some standard normal quantiles that are important to remember. In the diagram, we have plotted a normal distribution, and we can identify specific points. For instance, -1.28 is a quantile such that 10% of the density lies below it, and 90% lies above it. For a potentially non-standard normal distribution, this point would be μ - 1.28σ. By symmetry, 1.28 on the standard normal distribution represents the quantile at which 10% lies above it. For a potentially non-standard normal distribution, this point would be μ + 1.28σ. Another crucial quantile is 1.96 (often approximated as 2), where -1.96 represents the point below which 2.5% of the mass of the normal distribution lies, and +1.96 represents the point above which 2.5% of the mass lies. This implies that 95% of the distribution lies between these two points. For a potentially non-standard normal distribution, these points would be μ - 1.96σ and μ + 1.96σ, respectively. It is worth noting that when μ equals 0 and σ equals 1 for the standard normal distribution, the calculation of 1.96 directly yields the correct value.</p>
<p>Now, let’s move on to some example calculations of increasing difficulty. First, let’s determine the 95th percentile of a normal distribution with mean μ and variance σ squared. In other words, we seek the value X.95 such that 95% of the distribution lies below it. This value represents the threshold if we were to draw samples from this population.</p>
<p>We can find the point X.95, which represents the 95th percentile of a normal distribution, by utilizing the q qualifier for the density in R. In this case, we can use the function qnorm with the desired quantile 0.95. It’s crucial to input the mean μ and the standard deviation σ (not the variance) into the function. By using qnorm with the specified parameters, we can directly obtain the desired value. Another approach to solving this is by leveraging our memorized standard normal quartiles. Since we know that 1.645 standard deviations from the mean corresponds to a quantile with 95% lying below it and 5% lying above it for the standard normal distribution (centered at 0 with standard deviation units from the mean), we can apply this concept to a non-standard normal distribution as well. To calculate the desired point, we can simply compute μ + σ * 1.645. This will give us the answer we are looking for.</p>
<p>Now, let’s address a more general question, which will help set the context for subsequent questions. The question is: What is the probability that a non-standard normal distribution with mean μ and variance σ squared is larger than x? To answer this question in R, we can use the pnorm function with the specified values of x, mean (mu), and standard deviation (sigma). It’s important to remember to input the sigma value rather than the squared sigma value to avoid incorrect results. Additionally, we set the argument lower.tail = FALSE to indicate that we are interested in the upper tail of the distribution. Alternatively, we can omit this argument and calculate 1 minus the probability obtained from pnorm to achieve the same result.</p>
<p>A conceptually easy way to estimate this probability, which allows us to quickly assess probabilities mentally, is to convert the value x into the number of standard deviations it is from the mean. To achieve this, we subtract the mean μ from x and divide the result by the standard deviation σ. The resulting number represents x expressed in terms of how many standard deviations it is from the mean. For example, if the calculated value is approximately two standard deviations from the mean, we can estimate that the probability associated with it is around 2.5%.</p>
<p>To provide a concrete example and illustrate the application of these concepts, let’s consider the scenario where the number of daily ad clicks for companies follows an approximately normal distribution with a mean of 1,020 clicks per day and a standard deviation of 50 clicks per day.</p>
<p>Let’s consider a scenario where the number of daily ad clicks for a company follows an approximately normal distribution with a mean of 1,020 and a standard deviation of 50. We want to determine the probability of getting more than 1,060 clicks on a given day.</p>
<p>Since 1,060 clicks is 2.8 standard deviations from the mean, we can infer that this probability will be relatively low. This is because it is nearly 3 standard deviations away from the mean, and we know that such values are located in the tail of the normal distribution. To calculate this probability, we can use the pnorm function with the input values of 1,160 for the clicks, a mean of 1,020, and a standard deviation of 50. By setting the argument lower.tail = FALSE, we ensure that we obtain the probability of the value being larger than 1,060. The result we obtain is approximately 0.003.</p>
<p>Alternatively, we can directly calculate this probability using the standard normal distribution. By expressing 1,160 as the number of standard deviations it is away from the mean, which is 2.8, we can plug this value into the pnorm function with lower.tail = FALSE and obtain the same result.</p>
<p>Now let’s move on to a similar quantile calculation. Assuming the number of daily ad clicks for the company follows an approximately normal distribution with a mean of 1,020 and a standard deviation of 50, we want to find the number of daily ad clicks that represents the point where 75% of the days have fewer clicks.</p>
<p>Before performing the calculation in R, let’s analyze the situation intuitively. Since 1,020 is both the mean and the median of the specific normal distribution, we know that about 50% of the days lie below this point. Therefore, the desired number of clicks should be greater than 1,020. Additionally, let’s consider one standard deviation above the mean, which corresponds to 1,070. Within this range, we know that 68% of the days lie, leaving 32% outside of it, and 16% in each tail due to the symmetry of the normal distribution. Hence, the desired number of clicks should be around 84% of the distribution, lying between 1,020 and 1,070.</p>
<p>To calculate this quantile, we can use the qnorm function with the input value of 0.75, representing the 75th percentile. The mean is set to 1,020, and the standard deviation is 50. When we execute this command, qnorm(0.75, mean = 1,020, sd = 50), we obtain a number between the previously mentioned range, approximately 1,054.</p>
</div>
<div id="poisson-distribution" class="section level3" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Poisson distribution</h3>
<p>If there were a competition to determine the most useful distribution, the normal distribution would unquestionably win by a wide margin. However, selecting the second most useful distribution would spark a lively debate, with the Poisson distribution being a strong contender. The Poisson distribution is commonly employed to model counts, and its probability mass function is given by λ^x * e^(-λ) / x!, where x represents non-negative integers (0, 1, 2, and so on). The mean of a Poisson random variable is equal to λ, and the variance also equals λ. It is worth noting that when modeling with Poisson data, the mean and variance must be equal—a condition that can be verified if one has repeated Poisson data.</p>
<p>The Poisson distribution finds utility in various instances. Whenever count data needs to be modeled, especially when the counts are unbounded, the Poisson distribution is a suitable choice. Another prevalent application arises in the field of biostatistics, where event time or survival data is common. For example, in cancer trials, the time until the recurrence of symptoms is modeled using statistical techniques that account for censoring, and these techniques have a strong association with the Poisson distribution. Additionally, when classifying a sample of people based on certain characteristics, creating a contingency table—such as tabulating hair color by race—the Poisson distribution is the default choice for modeling such data. It is worth mentioning that the Poisson distribution is deeply connected to other models, including multinomials and binomials, which might be considered as alternatives.</p>
<p>Another prominent application of the Poisson distribution, though often overlooked due to its commonplace usage, is in cases where a binomial distribution is approximated by the Poisson distribution. This occurs when the sample size (n) is large, and the probability of success (p) is small. Epidemiology, for instance, frequently employs this approximation when dealing with situations where n is large (representing a population) and p is small (indicating the occurrence of rare events). By assuming a Poisson distribution, researchers can effectively model the occurrence rates of events, such as the number of new cases of respiratory diseases in a city as air pollution levels fluctuate. This practice is so prevalent that it is commonly understood within the field without explicit mention.</p>
<p>To illustrate the usage of the Poisson distribution for modeling rates, let’s consider an example. Suppose the number of people showing up at a bus stop follows a Poisson distribution with a mean of 2.5 people per hour. If we observe the bus stop for four hours, we can calculate the probability of three or fewer people showing up during that entire duration. To do this, we apply the Poisson probability formula to the values of three, two, one, and zero, using a rate of 2.5 events per hour multiplied by four hours. The resulting probability is approximately 1%.</p>
<p>Furthermore, we can discuss the Poisson approximation to the binomial distribution, specifically when the sample size (n) is large, and the probability of success (p) is small. In this scenario, the Poisson distribution can serve as a reasonably accurate approximation for the binomial distribution. To establish notation, let x represent a binomial distribution with parameters n and p, and define λ as the product of n and p. When n is large and p is small, it is proposed that the probability distribution governing x can be well approximated using Poisson probabilities, where the rate parameter λ is determined as n times p. As an example, let’s consider flipping a coin with a success probability of 0.01 for a total of 500 times. We want to calculate the probability of obtaining two or fewer successes. Using the binomial distribution with size 500 and probability</p>
<p>0.01, we obtain approximately 12%. By employing the Poisson approximation with a rate of λ = 500 * 0.01, the result is around 12.5%, which is reasonably close to the binomial calculation.
## Asymptotics
Asymptotics are an important topics in statistics. Asymptotics refers to the behavior of estimators as the sample size goes to infinity. Our very notion of probability depends on the idea of asymptotics. For example, many people define probability as the proportion of times an event would occur in infinite repetitions. That is, the probability of a head on a coin is 50% because we believe that if we were to flip it infinitely many times, we would get exactly 50% heads.</p>
<p>We can use asymptotics to help is figure out things about distributions without knowing much about them to begin with. A profound idea along these lines is the Central Limit Theorem. It states that the distribution of averages is often normal, even if the distribution that the data is being sampled from is very non-normal. This helps us create robust strategies for creating statistical inferences when we’re not willing to assume much about the generating mechanism of our data.
### Asymptotics and LLN
Hello, I’m Brian Caffo, and welcome to the lecture on asymptotics as part of the statistical inference class in the Coursera data science specialization. This course is co-taught by my colleagues Jeff Leek and Roger Peng from the Johns Hopkins Bloomberg School of Public Health. Today, we’ll be focusing on the behavior of statistics as the sample size or some other relevant quantity approaches infinity, which is known as asymptotics. Specifically, we will discuss the case where the sample size tends to infinity.</p>
<p>In the land of asymptopia, everything works out well because there is an infinite amount of data available. Asymptotics play a crucial role in simple statistical inference and approximations. They serve as a versatile tool, akin to a Swiss army knife, allowing us to investigate the statistical properties of various statistics without requiring extensive computations.</p>
<p>Furthermore, asymptotics form the foundation for the frequency interpretation of probabilities. For instance, intuitively, we know that if we flip a coin and calculate the proportion of heads, it should approach 0.5 for a fair coin. This property is known as the law of large numbers, which we will explore shortly.</p>
<p>Fortunately, we don’t have to delve into the mathematical intricacies of the limits of random variables. Instead, we can rely on a set of powerful tools that enable us to discuss the behavior of sample means from a collection of independently and identically distributed (iid) observations in large samples. One of these tools is the law of large numbers, which states that the average of the observations converges to the population mean it is estimating. For example, if we repeatedly flip a fair coin, the sample proportion of heads will eventually converge to the true probability of a head.</p>
<p>To illustrate the law of large numbers in action, let’s consider an example. We’ll generate a large number of random normal variables and calculate their cumulative means. Initially, there is considerable variability in the means, but as the number of simulations increases, the cumulative means converge towards the true population mean of zero.</p>
<p>Similarly, we can apply the law of large numbers to the case of coin flipping. By repeatedly flipping a coin and calculating the cumulative means, we observe that the sample proportion of heads converges to the true value of 0.5 as the number of coin flips increases.</p>
<p>It’s worth mentioning that an estimator is considered consistent if it converges to the parameter it aims to estimate. For instance, the sample proportion from iid coin flips is consistent for estimating the true success probability of a coin. As we collect more and more coin flip data, the sample proportion of heads approaches the actual probability of obtaining a head.</p>
<p>Moreover, not only are sample means consistent estimators, but the sample variance and sample standard deviation of iid random variables are also consistent estimators.</p>
</div>
<div id="asymptotics-and-the-clt" class="section level3" number="2.1.4">
<h3><span class="header-section-number">2.1.4</span> Asymptotics and the CLT</h3>
<p>Welcome to the lecture on asymptotics as part of the statistical inference class in the Coursera data science specialization. I am Brian Caffo, one of the instructors, along with Jeff Leek and Roger Peng, from Johns Hopkins Bloomberg School of Public Health. Today, we will explore the concept of asymptotics, which refers to the behavior of statistics as the sample size or some other relevant quantity approaches infinity or zero. In this lecture, we will focus on the case where the sample size tends to infinity.</p>
<p>Asymptopia, as I like to call it, is a land where everything works out well because there is an infinite amount of data. It is a place where asymptotics come into play, proving to be incredibly useful for simple statistical inference and approximations. Asymptotics can be likened to a versatile Swiss army knife that allows us to investigate the statistical properties of various statistics without requiring extensive computation.</p>
<p>Moreover, asymptotics form the foundation for the frequency interpretation of probabilities. For instance, intuitively, we understand that flipping a fair coin repeatedly should converge to a proportion of heads close to 0.5. This property is known as the law of large numbers, which we will explore shortly. Fortunately, instead of delving into the mathematical intricacies of random variable limits, we have powerful tools at our disposal. These tools enable us to discuss the behavior of sample means for collections of independent and identically distributed (iid) observations.</p>
<p>The first tool we will explore is the law of large numbers. It states that the average of iid samples converges to the population mean it estimates. To illustrate this concept, let’s observe the law of large numbers in action. I will perform a simulation with 1000 iterations. First, I will generate 1000 random numbers from a standard normal distribution. Then, by taking the cumulative sum of these numbers and dividing by 1 to n, I obtain the cumulative means. When we plot these cumulative means against the index, we observe that initially, there is considerable variability. However, as the number of simulations increases, the cumulative means converge to the true population value of zero.</p>
<p>Next, let’s apply the law of large numbers to coin flips. Using the <code>sample</code> function in R, I will simulate 1000 coin flips. Each flip results in either 0 (tail) or 1 (head), with equal probability. Again, by taking the cumulative sum and dividing by 1 to n, we calculate the cumulative means. Plotting these cumulative means reveals a similar pattern as before. Initially, there is variability in the sample proportion of heads, but as the number of coin flips approaches infinity, it converges to the true probability of 0.5.</p>
<p>Now, let’s discuss the concept of consistency. An estimator is considered consistent if it converges to the quantity it aims to estimate. In the case of coin flips, the sample proportion of heads is consistent for the true success probability of a coin. As we flip a coin repeatedly, the sample proportion of heads converges to the actual probability of getting a head.</p>
<p>The law of large numbers guarantees the consistency of sample means, but it also applies to sample variances and standard deviations of iid random variables. In other words, these estimators also converge to their respective population counterparts as the sample size increases.</p>
<p>Moving on, we encounter the Central Limit Theorem, which is perhaps the most important theorem in statistics. It states that the distribution of averages of iid random variables becomes approximately standard normal as the sample size grows. The Central Limit Theorem is remarkably versatile, applying to a wide range of populations. Its loose requirements make it applicable in numerous settings.</p>
<p>To understand the Central Limit Theorem, let’s consider an estimate like the sample average (x-bar). If we subtract its population</p>
<p>mean (mu) and divide by its standard error (sigma/sqrt(n)), the resulting random variable approaches a standard normal distribution as the sample size increases. Importantly, replacing the unknown population standard deviation with the known sample standard deviation does not affect the Central Limit Theorem.</p>
<p>The most useful interpretation of the Central Limit Theorem is that the sample average is approximately normally distributed, with a mean equal to the population mean and a variance given by the standard error of the mean. To illustrate this, let’s simulate several examples. First, we will use a standard die. We know that the mean of the die rolls is 3.5, and its variance is 2.92. By rolling the die n times, calculating the sample mean, subtracting the population mean, and dividing by the standard error, we obtain a distribution that approximates a bell curve. As we increase the number of rolls, the approximation improves.</p>
<p>Next, let’s consider coin flips. Taking the result of each flip (0 or 1) as an iid random variable, we calculate the sample proportion of heads (p-hat). Subtracting the population mean (0.5) and dividing by the standard error (sqrt(p(1-p)/n)), we again obtain a distribution that approximates a bell curve. Similar to the previous example, the approximation improves as the number of coin flips increases.</p>
<p>It’s important to note that the speed at which the normalized coin flips converge to normality depends on the bias of the coin. If the coin is heavily biased, the approximation may not be perfect even with a large sample size. However, as the number of coin flips approaches infinity, the Central Limit Theorem guarantees an excellent approximation.</p>
<p>As a fun aside, let’s discuss Galton’s quincunx. This machine, often found in science museums, visually demonstrates the Central Limit Theorem using a game resembling Pachinko. In Galton’s quincunx, a ball falls through a series of pegs, bouncing left or right at each peg. Each bounce can be thought of as a coin flip or binomial experiment. The total number of successes (heads) follows an approximately normal distribution, as predicted by the Central Limit Theorem. At the museum, the balls collect in bins, forming a histogram that aligns with the expected normal distribution.</p>
<p>In summary, the Central Limit Theorem is a powerful tool that allows us to approximate the distribution of averages of iid random variables. It applies to various settings and provides valuable insights into statistical inference. The examples we explored, from dice rolls to coin flips to Galton’s quincunx, illustrate the practical applications of the Central Limit Theorem and the convergence to a standard normal distribution as the sample size increases.</p>
</div>
<div id="asymptotics-and-confidence-intervals" class="section level3" number="2.1.5">
<h3><span class="header-section-number">2.1.5</span> Asymptotics and confidence intervals</h3>
<p>Let’s discuss a practical application of the central limit theorem. The central limit theorem tells us that the sample mean follows an approximately normal distribution with a population mean of μ and a standard deviation of σ/√n. This distribution allows us to make inferences about the population mean based on sample data.</p>
<p>When considering the distribution, we observe that μ plus 2 standard errors is quite far out in the tail, with only a 2.5% chance of a normal value being larger than two standard deviations in the tail. Similarly, μ minus 2 standard errors is far in the left tail, with only a 2.5% chance of a normal value being smaller than two standard deviations in the left tail. Therefore, the probability that the sample mean (x-bar) is greater than μ plus 2 standard errors or smaller than μ minus 2 standard errors is 5%. Equivalently, the probability that μ is between these limits is 95%. By reversing the roles of x-bar and μ, we can conclude that the interval [x-bar - 2 standard errors, x-bar + 2 standard errors] contains μ with a probability of 95%.</p>
<p>It’s important to note that in this interpretation, we treat the interval [x-bar - 2 standard errors, x-bar + 2 standard errors] as random, while μ is fixed. This allows us to discuss the probability that the interval contains μ. In practice, if we repeatedly obtain samples of size n from the population and construct a confidence interval in each case, about 95% of the intervals will contain μ, the parameter we are trying to estimate. If we want a 90% confidence interval, we need 5% in each tail, so we would use a different multiplier instead of 2 (e.g., 1.645).</p>
<p>To illustrate this, let’s consider an example using the father-son data from the “Using R” package. We want to estimate the average height of sons (x-bar). We can calculate the mean of the sample (x) plus or minus the 0.975th normal quantile times the standard error of the mean (standard deviation of x divided by the square root of n). Dividing by 12 ensures that our confidence interval is in feet rather than inches. For instance, if we obtain a confidence interval of 5.710 to 5.738, we can say that if the sons in this data are a random sample from the population of interest, the confidence interval for the average height of the sons would be 5.71 to 5.74.</p>
<p>Another application is when dealing with coin flips and estimating the success probability (p) of the coin. Each observation (xi) in this case is either 0 or 1, with a common success probability (p). The variance of a coin flip is p times (1 - p), where p is the true success probability of the coin. The standard error of the mean is then √(p(1 - p)/n). The confidence interval takes the form of p-hat plus or minus the normal quantile times √(p(1 - p)/n). Since we don’t know the true value of p, we replace it with the estimated value p-hat. This type of confidence interval is known as the Wald confidence interval, named after the statistician Wald. When p equals 0.5, the variance p times (1 - p) is maximized, resulting in a standard error of 0.5. Multiplying it by 2 in the 95% interval cancels out, so for a 95% confidence interval, p-hat plus or minus 1/√n is a quick estimate for p.</p>
<p>For example, suppose</p>
<p>you are running for political office, and in a random sample of 100 likely voters, 56 intend to vote for you. To determine if you can relax or if you need to campaign more, you can use a quick calculation. Taking 1/√100 yields 0.1. This means the approximate 95% interval is 0.46 to 0.66. The confidence interval suggests that we cannot rule out possibilities below 0.5 with 95% confidence. Therefore, you shouldn’t relax and should continue campaigning.</p>
<p>As a general guideline, you typically need at least 100 observations for one decimal place in a binomial experiment, 10,000 for two decimal places, and a million for three decimal places. These numbers reflect the approximate sample sizes needed for accurate estimation.</p>
<p>In summary, the central limit theorem provides us with a practical tool for constructing confidence intervals and making inferences about population parameters. It allows us to estimate the population mean using the sample mean and provides a measure of uncertainty through confidence intervals. The Wald confidence interval is a useful approximation for estimating the success probability in binomial experiments. Additionally, considering the sample size helps determine the level of precision and confidence in our estimates.</p>
<p>Consider a simulation where I repeatedly flip a coin with a known success probability. The goal is to calculate the percentage of times that the confidence interval covers the true probability. In each simulation, I flip the coin 20 times and vary the true success probability between 0.1 and 0.9 in steps of 0.05. I conduct 1,000 simulations for each true success probability.</p>
<p>For each true success probability, I generate 1,000 sets of 20 coin flips and calculate the sample proportion. Then, I compute the lower and upper limits of the confidence interval for each set of coin flips. Finally, I determine the proportion of times that the confidence interval covers the true value of the success probability. I store these proportions in a variable called “coverage.”</p>
<p>To visualize the results, I plot the coverage as a function of the true success probability used in the simulation. For example, if the true value of p is 0.5, I perform 1,000 simulations and calculate the coverage based on whether the confidence interval covers 0.5 or not. In this case, the coverage is over 95%, indicating that the confidence interval provides better than 95% coverage for a true success probability of 0.5. Although there is some Monte Carlo error due to the finite number of simulations, 1,000 simulations generally yield good accuracy.</p>
<p>However, for a true success probability around 12%, the coverage falls well below the expected 95%. The reason behind this discrepancy is that the central limit theorem is not accurate enough for this specific value of n (the number of coin flips) and the true probability.</p>
<p>To address this issue for smaller values of n, a quick fix is to add 2 to the number of successes and 2 to the number of failures. This adjustment modifies the sample proportion, making it x+2/(n+4). After applying this adjustment, the confidence interval procedure can be performed as usual. This modified interval is known as the Agresti/Coull interval and tends to perform better than the standard Wald interval.</p>
<p>Before demonstrating the results for the adjusted intervals, it is important to note that larger values of n yield better performance. In a simulation where n is increased to 100, the coverage probability improves and remains close to the expected 95% across different values of p.</p>
<p>Returning to the simulation with n=20, when using the add 2 successes and 2 failures interval, the coverage probability is higher than 95%, indicating an improvement compared to the poor coverage of the Wald interval for certain true probability values. However, it’s important to balance coverage and interval width, as being too conservative can lead to overly wide intervals.</p>
<p>Based on these observations, I strongly recommend using the add 2 successes and 2 failures interval instead of the Wald interval in this specific scenario.</p>
<p>Let’s create a Poisson interval using the formula that involves the estimate plus or minus the normal quantile standard error. Although the application of the central limit theorem in this case may be less clear, we will discuss it shortly.</p>
<p>Consider a nuclear pump that failed 5 times over a monitoring period of 94.32 days. We want to calculate a 95% confidence interval for the failure rate per day. Assuming the number of failures follows a Poisson distribution with a failure rate of lambda and the monitoring period is denoted as t, the estimate of the failure rate is the number of failures divided by the total monitoring time. The variance of this estimate is lambda/t.</p>
<p>In the calculations performed in R, the number of events (x) is set to 5, and the monitoring time (t) is 94.32. The rate estimate (lambda hat) is computed as x/t, and the confidence interval estimate is obtained by adding or subtracting the relevant standard normal quantile multiplied by the standard error. The resulting interval is rounded to three decimal places.</p>
<p>In addition to the large sample interval, we can also calculate an exact Poisson interval using the <code>poisson.test</code> function in R. This exact interval guarantees the specified coverage (e.g., 95%), but it may be conservative and result in wider intervals than necessary.</p>
<p>To examine how confidence intervals perform in repeated samplings, let’s conduct a simulation similar to the one for the coin example, but for the Poisson coverage rate. We select a range of lambda values around those from our previous example and perform 1,000 simulations. The monitoring time is set to 100 for simplicity. We define coverage as the percentage of times the simulated interval contains the true lambda value used in the simulation. The simulation is repeated for various lambda values, and the resulting plot shows the lambda values on the x-axis and the estimated coverage on the y-axis.</p>
<p>The plot reveals that as lambda values increase, the coverage approaches 95%. However, there is some Monte Carlo error due to the finite number of simulations. On the other hand, as the true lambda value becomes smaller, the coverage deteriorates significantly. For very small lambda values, the purported 95% interval may only provide 50% actual coverage.</p>
<p>To address this issue, it is recommended not to rely on the asymptotic interval for small lambda values, especially when there are relatively few events during a large monitoring time. In such cases, the asymptotic interval does not align well with the Poisson distribution. Instead, an exact Poisson interval can be used as an alternative.</p>
<p>Although the central limit theorem’s application in the Poisson case may not be immediately clear, a simulation with a larger monitoring time (e.g., changing t from 100 to 1,000) demonstrates that as the monitoring time increases, the coverage improves and converges to 95% for most lambda values. However, some poor coverage may still occur for small lambda values, which we know the interval has trouble handling. In such cases, the exact Poisson interval remains a viable option.</p>
<p>Congratulations on making it through this extensive lecture. To summarize briefly, we covered the Law of Large Numbers, which states that averages of independent and identically distributed (IID) random variables converge to the quantities they are estimating. This applies to Poisson rates as well, although the convergence process may be less clear. As the monitoring time tends to infinity, for example, Poisson rates converge to their estimated values.</p>
<p>Next, we discussed the Central Limit Theorem, which states that averages are approximately normally distributed. These distributions are centered at the population mean, a concept we already knew without the theorem, with standard deviations equal to the standard error of the mean. However, the Central Limit Theorem does not guarantee that the sample size is large enough for this approximation to be accurate. We have observed instances where confidence intervals are very accurate and others where they are less accurate.</p>
<p>Speaking of confidence intervals, our default approach for constructing them is to take the mean estimate and add or subtract the relevant normal quantile times the standard error. This method, known as “walled intervals,” is used not only in this context but also in regression analysis, general linear models, and other complex subjects. For a 95% confidence interval, the quantile value can be taken as 2 or, for more accuracy, 1.96.</p>
<p>Confidence intervals become wider as the desired coverage increases within a specific technique. This is because wider intervals provide more certainty that the parameter lies within them. To illustrate, imagine an extreme scenario where your life depends on the confidence interval containing the true parameter. In this case, you would want to make the interval as wide as possible to ensure your safety. The mathematics behind confidence intervals follows the same principle.</p>
<p>In the cases of Poisson and binomial distributions, which are discrete, the Central Limit Theorem may not accurately approximate their distributions. However, exact procedures exist for these cases. We also learned a simple fix for constructing confidence intervals in the binomial case by adding two successes and two failures, which provides a better interval without requiring complex computations. This method can be easily done by hand or mentally, even without access to a computer.</p>
</div>
</div>
<div id="practical-r-exercises-in-swirl" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Practical R Exercises in swirl</h2>
</div>
<div id="quiz" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Quiz</h2>

</div>
</div>
<hr>
<center> 
  <div class="footer">
      All illustrations <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY. </a>
      <br>
      All other materials <a href= "https://creativecommons.org/licenses/by/4.0/"> CC-BY </a> unless noted otherwise.
  </div>
</center>
            </section>

          </div>
        </div>
      </div>
<a href="probability-expected-values.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="intervals-testing-pvalues.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

</body>

</html>
