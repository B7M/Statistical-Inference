<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Probability &amp; Expected Values | Statistical Inference</title>
  <meta name="description" content="Description about Course/Book." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Probability &amp; Expected Values | Statistical Inference" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Description about Course/Book." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Probability &amp; Expected Values | Statistical Inference" />
  
  <meta name="twitter:description" content="Description about Course/Book." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="assets/dasl_favicon.ico" type="image/x-icon" />
<link rel="prev" href="index.html"/>
<link rel="next" href="variability-distribution-asymptotics.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<a href="http://jhudatascience.org/"><img src="https://jhudatascience.org/images/dasl.png" style=" width: 80%; padding-left: 40px; padding-top: 8px; vertical-align: top "</a>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Course</a></li>
<li class="chapter" data-level="1" data-path="probability-expected-values.html"><a href="probability-expected-values.html"><i class="fa fa-check"></i><b>1</b> Probability &amp; Expected Values</a>
<ul>
<li class="chapter" data-level="1.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#course-book"><i class="fa fa-check"></i><b>1.1.1</b> Course Book:</a></li>
<li class="chapter" data-level="1.1.2" data-path="probability-expected-values.html"><a href="probability-expected-values.html#github-repository"><i class="fa fa-check"></i><b>1.1.2</b> Github repository</a></li>
<li class="chapter" data-level="1.1.3" data-path="probability-expected-values.html"><a href="probability-expected-values.html#homework-problems"><i class="fa fa-check"></i><b>1.1.3</b> Homework Problems</a></li>
<li class="chapter" data-level="1.1.4" data-path="probability-expected-values.html"><a href="probability-expected-values.html#differences-of-opinion"><i class="fa fa-check"></i><b>1.1.4</b> Differences of opinion</a></li>
<li class="chapter" data-level="1.1.5" data-path="probability-expected-values.html"><a href="probability-expected-values.html#data-science-specialization-community-site"><i class="fa fa-check"></i><b>1.1.5</b> Data Science Specialization Community Site</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="probability-expected-values.html"><a href="probability-expected-values.html#probability"><i class="fa fa-check"></i><b>1.2</b> Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#probability-mass-functions-and-probability-density-functions"><i class="fa fa-check"></i><b>1.2.1</b> Probability mass functions and probability density functions</a></li>
<li class="chapter" data-level="1.2.2" data-path="probability-expected-values.html"><a href="probability-expected-values.html#quantiles"><i class="fa fa-check"></i><b>1.2.2</b> Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="probability-expected-values.html"><a href="probability-expected-values.html#conditional-probability"><i class="fa fa-check"></i><b>1.3</b> Conditional probability</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#independence"><i class="fa fa-check"></i><b>1.3.1</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="probability-expected-values.html"><a href="probability-expected-values.html#expected-values"><i class="fa fa-check"></i><b>1.4</b> Expected values</a></li>
<li class="chapter" data-level="1.5" data-path="probability-expected-values.html"><a href="probability-expected-values.html#practical-r-exercises-in-swirl"><i class="fa fa-check"></i><b>1.5</b> Practical R Exercises in swirl</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html"><i class="fa fa-check"></i><b>2</b> Variability, Distribution, &amp; Asymptotics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#variability"><i class="fa fa-check"></i><b>2.1</b> Variability</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#variance-simulation-examples"><i class="fa fa-check"></i><b>2.1.1</b> Variance simulation examples</a></li>
<li class="chapter" data-level="2.1.2" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#normal-distribution"><i class="fa fa-check"></i><b>2.1.2</b> Normal distribution</a></li>
<li class="chapter" data-level="2.1.3" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#poisson-distribution"><i class="fa fa-check"></i><b>2.1.3</b> Poisson distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#asymptotics"><i class="fa fa-check"></i><b>2.2</b> Asymptotics</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#asymptotics-and-lln"><i class="fa fa-check"></i><b>2.2.1</b> Asymptotics and LLN</a></li>
<li class="chapter" data-level="2.2.2" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#asymptotics-and-the-clt"><i class="fa fa-check"></i><b>2.2.2</b> Asymptotics and the CLT</a></li>
<li class="chapter" data-level="2.2.3" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#asymptotics-and-confidence-intervals"><i class="fa fa-check"></i><b>2.2.3</b> Asymptotics and confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#practical-r-exercises-in-swirl-1"><i class="fa fa-check"></i><b>2.3</b> Practical R Exercises in swirl</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html"><i class="fa fa-check"></i><b>3</b> Intervals, Testing, &amp; Pvalues</a>
<ul>
<li class="chapter" data-level="3.1" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#confidence-intervals"><i class="fa fa-check"></i><b>3.1</b> Confidence intervals</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#independent-group-t-intervals"><i class="fa fa-check"></i><b>3.1.1</b> Independent group T intervals</a></li>
<li class="chapter" data-level="3.1.2" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#a-note-on-unequal-variance"><i class="fa fa-check"></i><b>3.1.2</b> A note on unequal variance</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.2</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#t-tests"><i class="fa fa-check"></i><b>3.2.1</b> T tests</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#p-values"><i class="fa fa-check"></i><b>3.3</b> P values</a></li>
<li class="chapter" data-level="3.4" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#knitr"><i class="fa fa-check"></i><b>3.4</b> Knitr</a></li>
<li class="chapter" data-level="3.5" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#practical-r-exercises-in-swirl-2"><i class="fa fa-check"></i><b>3.5</b> Practical R Exercises in swirl</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html"><i class="fa fa-check"></i><b>4</b> Power, Bootstrapping, &amp; Permutation Tests</a>
<ul>
<li class="chapter" data-level="4.1" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#power"><i class="fa fa-check"></i><b>4.1</b> Power</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#calculating-power"><i class="fa fa-check"></i><b>4.1.1</b> Calculating Power</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#multiple-comparisons"><i class="fa fa-check"></i><b>4.2</b> Multiple Comparisons</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#notes-on-the-bootstrap"><i class="fa fa-check"></i><b>4.2.1</b> Notes on the bootstrap</a></li>
<li class="chapter" data-level="4.2.2" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#permutation-tests"><i class="fa fa-check"></i><b>4.2.2</b> Permutation tests</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#course-project"><i class="fa fa-check"></i><b>4.3</b> Course project</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#part-1-simulation-exercise-instructions"><i class="fa fa-check"></i><b>4.3.1</b> Part 1: Simulation Exercise Instructions</a></li>
<li class="chapter" data-level="4.3.2" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#part-2-basic-inferential-data-analysis-instructions"><i class="fa fa-check"></i><b>4.3.2</b> Part 2: Basic Inferential Data Analysis Instructions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#practical-r-exercises-in-swirl-3"><i class="fa fa-check"></i><b>4.4</b> Practical R Exercises in swirl</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="5" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>5</b> References</a></li>
<li class="divider"></li>
<p style="text-align:center;"> <a href="https://github.com/jhudsl/OTTR_Template" target="blank" > This content was published with</a> <a href="https://bookdown.org/" target="blank"> bookdown by:</a> </p>
<p style="text-align:center;"> <a href="http://jhudatascience.org/"> The Johns Hopkins Data Science Lab </a></p>
<p style="text-align:center; font-size: 12px;"> <a href="https://github.com/rstudio4edu/rstudio4edu-book/"> Style adapted from: rstudio4edu-book </a> <a href ="https://creativecommons.org/licenses/by/2.0/"> (CC-BY 2.0) </a></p>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<head>
  <meta name="viewport" content="width=device-width,minimum-scale=1.0,maximum-scale=1.0,initial-scale=1.0">
  <!--script src="https://kit.fontawesome.com/6a26f47516.js"></script-->
  <!--<script src="assets/hideOutput.js"></script>-->
  <link href="assets/style.css" rel="stylesheet">
</head>



<div class="hero-image-container">
  <img class= "hero-image" src= "https://github.com/jhudsl/OTTR_Template/raw/main/assets/dasl_thin_main_image.png">
</div>
<div id="probability-expected-values" class="section level1" number="1">
<h1><span class="header-section-number">Chapter 1</span> Probability &amp; Expected Values</h1>
<p>In this module, we’ll go over some information and resources to help you get started and succeed in the course. During this week, we’ll focus on the fundamentals including probability, random variables, expectations.</p>
<div id="introduction" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Introduction</h2>
<p>Greetings and a warm welcome to the Probability class, which is a part of the Statistical Inference course within the Coursera Data Science series. I’m Brian Caffo, and I will be one of your instructors for this class. Alongside me, we have Jeff Leek and Roger Peng, who will also be co-teaching the course. We all belong to the Department of Biostatistics at the Bloomberg School of Public Health.
### Syllabus
The primary instructor of this class is Brian Caffo.
Brian is a professor at Johns Hopkins Biostatistics and co-directs the SMART working group.</p>
<p>This class is co-taught by Roger Peng and Jeff Leek. In addition, Sean Kross and Nick Carchedi have been helping greatly.</p>
<p><strong>Course Content</strong></p>
<p>In this course we will cover the following topics:
1. Probability
2. Conditional Probability
3. Expectations
4. Variance
5. Common Distributions
6. Asymptotics
7. T confidence intervals
8. Hypothesis testing
9. P-values
10. Power
11. Multiple Testing
12. Resampling</p>
<p>If you’d prefer to watch the videos on YouTube, you can do so through this <a href="https://www.youtube.com/playlist?list=PLpl-gQkQivXiBmGyzLrUjzsblmQsLtkzJ">link</a>.</p>
<div id="course-book" class="section level3" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Course Book:</h3>
<p>Statistical Inference for Data Science book is now available. It’s a different sort of book published on LeanPub. If you purchase the book on LeanPub, you’ll get all editions in the future for free. You pick the price on the site. You can get it <a href="https://leanpub.com/LittleInferenceBook">here</a>.</p>
<p>Following our style for the specialization, the book is creative commons licensed to offer you maximum flexibility in how you use the materials.</p>
<p>You can also just read a web page rendering of the book <a href="https://leanpub.com/LittleInferenceBook/read">here</a></p>
<p>In addition, the book is available on <a href="https://github.com/bcaffo/LittleInferenceBook">GitHub</a> if you wish to render it yourself using pandoc.</p>
</div>
<div id="github-repository" class="section level3" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Github repository</h3>
<p>The most up to date information on the course lecture notes will always be on the <a href="https://github.com/DataScienceSpecialization/courses/tree/master/06_StatisticalInference">Github repository</a>.</p>
<p>Please issue pull requests so that we may improve the materials. Notice that Brian’s forked github repo is sometimes out of sync with the Data Science Specialization repo managed by the other instructors. Make sure to check in Brian’s master repo for the most up to date material.</p>
<p>If you would just like the full set of lecture pdfs, grab them <a href="https://github.com/bcaffo/courses/blob/master/06_StatisticalInference/lectures.zip?raw=true">here</a>.</p>
<p>If you would just like the full set of Rmd files for the lecture code, get those <a href="https://github.com/bcaffo/courses/blob/master/06_StatisticalInference/rmd.zip?raw=true">here</a>.</p>
</div>
<div id="homework-problems" class="section level3" number="1.1.3">
<h3><span class="header-section-number">1.1.3</span> Homework Problems</h3>
<p>In the book, there are homework problems fairly similar to the quiz questions.</p>
<p>If you can do them, you should be in very good shape for the quizzes. The homework assignments in this course are optional. They won’t count toward your final grade, but they are a good opportunity to practice the skills covered in the course. There are worked out solutions on youtube linked to the book. These are ordered in an odd way, as the class has been restructured. So, it’s probably best to just do them through the book.</p>
<p><a href="http://bcaffo.github.io/courses/06_StatisticalInference/homework/hw1.html#1">Homework 1</a>
<a href="http://bcaffo.github.io/courses/06_StatisticalInference/homework/hw2.html#1">Homework 2</a>
<a href="http://bcaffo.github.io/courses/06_StatisticalInference/homework/hw3.html#1">Homework 3</a>
<a href="http://bcaffo.github.io/courses/06_StatisticalInference/homework/hw4.html#1">Homework 4</a></p>
</div>
<div id="differences-of-opinion" class="section level3" number="1.1.4">
<h3><span class="header-section-number">1.1.4</span> Differences of opinion</h3>
<p>Keep in mind that currently data analysis is as much art as it is science - so we may have a difference of opinion - and that is ok! Please refrain from angry, sarcastic, or abusive comments on the message boards. Our goal is to create a supportive community that helps the learning of all students, from the most advanced to those who are just seeing this material for the first time.</p>
</div>
<div id="data-science-specialization-community-site" class="section level3" number="1.1.5">
<h3><span class="header-section-number">1.1.5</span> Data Science Specialization Community Site</h3>
<p>Since the beginning of the Data Science Specialization, we’ve noticed the unbelievable passion students have about our courses and the generosity they show toward each other on the course forums. A couple students have created quality content around the subjects we discuss, and many of these materials are so good we feel that they should be shared with all of our students.</p>
<p>We’re excited to announce that we’ve created a site using <a href="http://datasciencespecialization.github.io/">GitHub Pages</a> to serve as a directory for content that the community has created. If you’ve created materials relating to any of the courses in the Data Science Specialization, please send us a pull request so we can add a link to your content on our site. You can find out more about contributing <a href="https://github.com/DataScienceSpecialization/DataScienceSpecialization.github.io#contributing">here</a></p>
<p>We can’t wait to see what you’ve created and where the community can take this site!</p>
</div>
</div>
<div id="probability" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Probability</h2>
<p>In today’s lecture, we will cover the fundamentals of probability at a beginner’s level, providing you with the necessary knowledge for your journey in the data science specialization. If you’re interested in delving deeper into this topic, I highly recommend checking out my comprehensive mathematical biostatistics boot camp series. In addition,the course notes are on GitHub.</p>
<p>In this module we discuss probability, the foundation of statistical analysis. Probability assigns a number between 0 and 1 to events to give a sense of the “chance” of the event. Probability has become our default model for apparently random phenomena. Our eventual goal is to use probability models, our formal mechanism for connecting our data to a population. However, before we get to probability models, we need to understand the basics of probability calculus. The next few lectures cover these basics.</p>
<p><strong>Probability</strong> = the study of quantifying the likelihood of particular events occurring
- given a random experiment, probability = population quantity that summarizes the randomness
This summary is not just about the data at hand, but a conceptual quantity that exist in the population that we want to estimate.</p>
<p>Let’s delve into the concept of probability. In the context of a random experiment, such as rolling a die, probability quantifies the inherent randomness of the outcomes. It’s important to highlight the term “population” here. When considering a die roll, probability is seen as an intrinsic characteristic of the die itself, rather than being dependent on a specific sequence of fixed rolls. Therefore, when we discuss probability, we’re referring to a conceptual property that exists within the population we aim to estimate, rather than being directly observable in the data we have. Now, let’s define the principles that govern probability, known as probability calculus. Firstly, probability operates on the potential outcomes of an experiment. For instance, when rolling a die, the possible outcomes could be 1, the set {1, 2}, the set of even numbers {2, 4, 6}, or the set of odd numbers {1, 3, 5}, and so on. Probability is a function that assigns a number between 0 and 1 to each of these sets of possible outcomes.
We must adhere to the rule that the probability of an event occurring, such as rolling the die and obtaining a particular number, must be equal to one. Additionally, the probability of the union of two mutually exclusive sets of outcomes must be equal to the sum of their individual probabilities. For example, consider the scenario where one possible outcome is obtaining either a one or a two, while another possible outcome is obtaining either a three or a four. These two sets, {1, 2} and {3, 4}, cannot occur simultaneously. The probability of the union, i.e., obtaining a one, two, three, or four, is the sum of the probabilities of obtaining a one or two, plus the sum of the probabilities of obtaining a three or four.</p>
<p>Interestingly, these simple rules encompass all the necessary principles to establish the general rules that govern probability. This significant discovery was made by the Russian mathematician Kolmogorov. Let’s explore some of the essential rules that probability must abide by. While I have already mentioned a few, others naturally follow from the previously stated rules.</p>
<ol style="list-style-type: decimal">
<li>The probability of an event not occurring, or “nothing” happening, is zero. In the case of rolling a die, something is bound to occur, and you will obtain a number.</li>
<li>Conversely, the probability of an event occurring, such as rolling a specific number on the die, is equal to one.</li>
<li>It is intuitive to understand that the probability of an event happening is equal to one minus the probability of the opposite event occurring. For example, the probability of rolling an even number on a die is equal to one minus the probability of rolling an odd number. This is because the set of odd numbers is considered the opposite of obtaining an even number in the context of rolling a die.</li>
<li>The probability of at least one of two or more mutually exclusive events, which cannot occur simultaneously, is the sum of their individual probabilities. This aligns with the definition we discussed earlier.</li>
<li>Another consequence of probability calculus is that if event A implies the occurrence of event B, then the probability of event A is less than or equal to the probability of event B. Although this may sound complex when explained verbally, it becomes clearer when visualized using a Venn diagram.</li>
</ol>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="resources/images/week_01_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g256d730c400_0_1.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.1: Event A being sub section of B event
</p>
</div>
<p>In the diagram, event A is represented by a circle contained within event B. When we consider the probability of A, we assign a number to the area within circle A. Similarly, when discussing event B, we refer to the probability assigned to the entire circle, which includes the area of A. Therefore, it logically follows that the probability of B is larger than or equal to the probability of A. This concept is often intuitive and easily understood once visualized. For instance, the probability of rolling a 1 (set A) is less than the probability of rolling a 1 or a 2 (set B).</p>
<ol style="list-style-type: decimal">
<li>For any two events, the probability of at least one occurring is equal to the sum of their probabilities minus the probability of their intersection.</li>
</ol>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="resources/images/week_01_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g25780f6af6f_0_0.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.2: Events A and B with intersection
</p>
</div>
<p>Again, visualizing this with a Venn diagram helps in understanding it better. Consider set A and set B. When we add their individual probabilities, we are effectively adding the intersection region twice, once when considering A and once when considering B. Since we have counted the intersection twice, to obtain the probability of their union, we need to subtract the intersection once. This rule highlights that we cannot simply add probabilities if there exists a non-trivial intersection between the events.</p>
<p>Now, let’s illustrate an example to demonstrate why we cannot simply add probabilities when the events are not mutually exclusive. According to the National Sleep Foundation, approximately 3% of the American population has sleep apnea, while around 10% of the North American and European population has restless leg syndrome. Let’s assume, for the sake of argument, that these probabilities are derived from the same population. The question is, can we add these probabilities together to conclude that about 13% of people in this population have at least one of these sleep problems? The answer is no. The reason is that these events, sleep apnea and restless leg syndrome, can occur simultaneously and are not mutually exclusive. There is a non-trivial portion of the population that experiences both conditions concurrently.</p>
<p>To elaborate further, let’s define event A as the occurrence of sleep apnea in a person drawn from this population, and event B as the occurrence of restless leg syndrome. In this case, we believe that the intersection of these two events (the occurrence of both conditions) is non-trivial. If we were to naively add the probabilities of A and B, we would essentially count the intersection twice, which would result in an overestimate. To determine the probability of the union (at least one of the conditions), we need to subtract the intersection once, recognizing that it was mistakenly included twice in the initial addition.</p>
<div id="probability-mass-functions-and-probability-density-functions" class="section level3" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Probability mass functions and probability density functions</h3>
<p>Probability calculus provides a valuable framework for understanding the fundamental rules that govern probability and serves as the basis for all probabilistic thinking. However, when it comes to numeric outcomes of experiments, we require a more practical approach. This is where densities and mass functions for random variables come into play, serving as a convenient starting point. These concepts will be sufficient for our purposes, which is collecting data that will be utilized to estimate properties of the population.</p>
<p>One of the most well-known examples of a density function is the bell curve, also known as the normal distribution. In this class, you will gain a deeper understanding of what it truly means for data to follow a bell curve. You will learn about the significance and interpretation of the bell curve. Importantly, you will also realize that when discussing probabilities associated with the bell curve or the normal distribution, we are referring to population quantities, not statements solely based on the observed data.</p>
<p>Before delving into data analysis, it is crucial to develop our intuition for understanding population quantities. A random variable represents the numerical outcome of an experiment. In our study, we will encounter two types of random variables: discrete and continuous.
Discrete random variables are those that can be counted, such as the number of web hits or the possible outcomes of rolling a die. They can even include non-numeric attributes like hair color, which can be assigned numeric values (e.g., 1 for blonde, 2 for brown, 3 for black, etc.). For discrete random variables, we assign probabilities to each possible value they can take. On the other hand, continuous random variables can assume any value within a range or continuum. When working with continuous random variables, we assign probabilities to ranges of values they can take.</p>
<p>Let’s consider some simple examples that can be viewed as random variables, as these examples will aid in building our intuition throughout the course. One prominent example is the flip of a coin, where we can assign values of “heads” or “tails” (or 0 and 1) to represent the outcomes. This is a discrete random variable since it can only take two distinct levels. Another example of a discrete random variable is the outcome of rolling a die. It can only take one of six possible values, making it a discrete random variable with simple probability mechanics. A more complex random variable would be, the amount of website traffic or the number of web hits on a given day. While we’ll likely treat it as discrete, it’s interesting because it doesn’t have an upper bound. In such cases, we might employ the Poisson distribution to model it.
The hypertension status of a randomly selected subject from a population can also be a random variable. We may assign a value of 1 to indicate the presence of hypertension or a diagnosis, and 0 otherwise. This random variable would typically be modeled as discrete.</p>
<p>An example of continuous random variable would be measuring a subject’s body mass index (BMI). In this case, BMI would be considered a continuous random variable, as it can assume any value within a range.
Intelligence quotients (IQ) are often modeled as continuous random variables.</p>
<p>When working with discrete random variables, we assign a probability to each possible value they can take. We represent this assignment using a function called the probability mass function (PMF). The PMF takes any value of the discrete random variable and assigns the probability of it taking that specific value.</p>
<p>For example, in the case of a die roll, the PMF would assign a probability of one-sixth to the value one, one-sixth to the value two, one-sixth to the value three, and so on.</p>
<p>To ensure that the PMF satisfies the basic rules of probability, we have two requirements. First, the PMF must always be greater than or equal to zero since probabilities range from zero to one, inclusive. Second, the sum of the probabilities assigned to all possible values of the random variable must add up to one. In the case of a die roll, if we add the probabilities of getting one, two, three, four, five, and six, the sum should equal one. This ensures that the probability of any possible outcome occurring is accounted for.</p>
<p>Therefore, the PMF of a discrete random variable must adhere to these two rules to accurately represent probabilities.</p>
<p>We will primarily focus on using probability mass functions (PMFs) that are particularly useful in our context. Two examples of such PMFs are the binomial distribution, commonly used for coin flips, and the Poisson distribution, commonly used for counting events. However, let’s discuss one of the most well-known PMFs, the Bernoulli distribution, which is often used to model the outcome of a coin flip.</p>
<p>Let’s denote the random variable representing the coin flip outcome as capital X, where X = 0 represents tails and X = 1 represents heads. In this notation, an uppercase letter represents a potential value of the random variable that may or may not occur. On the other hand, a lowercase x serves as a placeholder for a specific value that we will substitute.</p>
<p>The PMF for the Bernoulli distribution is represented as <span class="math inline">\(P(X) = (0.5)^{x} * (0.5)^{(1-x)}\)</span>. When we substitute x = 0 into this PMF, we obtain a probability of one-half. Similarly, when we substitute x = 1, we also get a probability of one-half. This means that the probability of the random variable X taking the value 0 is one-half, and the probability of it taking the value 1 is also one-half.</p>
<p>When we introduce an unfair coin, we can adjust our approach by considering a parameter, theta, representing the probability of getting a head. The probability of getting a tail would then be 1 minus theta, where theta is a number between 0 and 1. In this case, the probability mass function can be written as follows: <span class="math display">\[P(X) = \theta^x * (1 - \theta)^{(1 - x)}\]</span>.
By substituting x = 1 into this PMF, we obtain the probability <span class="math inline">\(\theta\)</span>. Similarly, when we substitute x = 0, we get the probability <span class="math inline">\(1-\theta\)</span>. This implies that for this population distribution, the probability of the random variable X taking the value 0 is <span class="math inline">\(1-\theta\)</span>, and the probability of it taking the value 1 is <span class="math inline">\(\theta\)</span>.</p>
<p>This approach is particularly useful for modeling the prevalence of a certain condition or event. For instance, if we want to model the prevalence of hypertension, we can assume that the population or sample we are studying can be likened to the outcomes of biased coin flips with a success probability represented by <span class="math inline">\(\theta\)</span>. However, the challenge lies in not knowing the exact value of <span class="math inline">\(\theta\)</span>. Therefore, we will utilize our data to estimate this proportion within the population.</p>
<p>In contrast to the probability mass function, which assigns probabilities to specific values for discrete random variables, the probability density function (PDF) is associated with continuous random variables. Similar to the rules that the probability mass function follows, a valid probability density function must satisfy two specific rules: it must be greater than or equal to zero everywhere, and the total area under the function must be equal to one. The key concept of a probability density function is that areas under the curve correspond to probabilities for the random variable. For instance, if we state that intelligence quotients (IQ) are normally distributed with a mean of 100 and a standard deviation of 15, we are implying that the population follows a bell-shaped curve. In this case, the probability that a randomly selected individual from that population has an IQ between 100 and 115 is represented by the area under the curve within that range. It is important to note that the probability density function represents a statement about the population of IQs and not the data itself. The data will be used to assess and evaluate the assumptions made about the population’s probability distribution. It is worth emphasizing that whenever the term “probability” is used, it refers to a population quantity.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-3"></span>
<img src="resources/images/week_01_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g25780f6af6f_0_3.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.3: Area between 100-115 IQ under normal distribution
</p>
</div>
<p>It is interesting to note that when we model continuous probabilities using probability density functions (PDFs) for continuous random variables, the probability of the variable taking any specific value is actually <em>zero</em>. This is due to the fact that the area under a line, which represents a single point, is zero. However, this does not pose a problem and is simply a quirk arising from modeling random variables with infinite precision. It does not affect the functioning of probability calculations.</p>
<p>The bell-shaped curve, which represents a normal distribution, can be quite challenging to work with until you learn the appropriate techniques, which will be covered in a separate lecture. For now, let’s consider a simpler density function that resembles a right triangle. We’ll use the function <span class="math inline">\(f(x) = 2x\)</span> for x between 0 and 1, and 0 otherwise, as an example. Let’s provide some context for this function: imagine it represents the proportion of help calls that are addressed in a random day by a helpline.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-4"></span>
<img src="resources/images/week_01_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_1.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.4: Shape of the density function for f(x)=2x
</p>
</div>
<p>What does this density function imply? It means that the probability of the number of calls being addressed falling between 20% and 60% of the total calls for that day is given by the area under the curve in that range. Now, let’s evaluate whether this function is a mathematically valid probability density function.</p>
<p>Looking at the plot of the PDF, which resembles a right triangle, we can see that it is always greater than or equal to zero. Next, let’s calculate the area under the curve. Since it is a right triangle, the area is equal to half the base (which is 1) multiplied by the height (which is 2). Thus, the area is 1. Therefore, this function satisfies the requirements of a valid probability density function, as it is always non-negative and the total area under the curve is equal to 1.</p>
<p>Example: we want to find the probability that 75% or fewer calls get addressed in a randomly sampled day from this population. At the point (0.75, 1.5) on the density function, the height is 1.5 because the function is defined as 2 times x. The base value is 0.75. To calculate the probability, we divide the area, which is half the base times the height, by 2. So the probability turns out to be 56%, as shown in the example.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-5"></span>
<img src="resources/images/week_01_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_5.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.5: Shape of the density function for f(x)=2x
</p>
</div>
<p>Interestingly, this density function is a special case of a well-known distribution called the <span class="math inline">\(\beta\)</span> distribution. I have provided the R code here for obtaining the probability directly from the <span class="math inline">\(\beta\)</span> distribution. Although in this simple case we don’t need it because we are working with triangles, in more complex scenarios, we will require these functions. It’s worth mentioning that in R language we can right this as <code>pbeta(0.75,2,1)</code> the <code>p</code> prefix before a function denotes the calculation of probabilities, 1 define the specific triangle we are using in this example, and you can test and see that it yields the same result of 56%. Certain areas of the density are so commonly used that they are given specific names. For instance, the cumulative distribution function (CDF) of a random variable X gives the probability that X is less than or equal to a given value x.
<span class="math display">\[F(x) = P(X \leq x)\]</span>
This definition holds for both discrete and continuous random variables. In the case of the beta distribution we just examined, the <code>pbeta</code> function in R always returns the probability of being less than or equal to the first argument provided.</p>
<p>Alternatively, the survival function is another useful concept. It is defined as 1 minus the cumulative distribution function and represents the probability of a random variable being greater than a given value.
<span class="math display">\[S(x) = P(X &gt; x) = 1 - F(x)\]</span>
Suppose we wanted to determine the cumulative distribution function for the previously mentioned density. For instance, we might want to find the probability that 40% or fewer, 50% or fewer, or 60% or fewer of the calls get answered in a given day based on this specific right triangle population density function. In each case, the calculation will resemble what we did earlier for 0.75. Since the density function is a right triangle, the probability is half the area of the base times the height. This simplifies to one-half times x times 2x, which equals <span class="math inline">\(x^2\)</span>. Therefore, the function <span class="math inline">\(x^2\)</span> provides the probability of that percentage or fewer calls being answered on a randomly sampled day.</p>
<p>To examine the results when we use the <code>pbeta</code> function, which corresponds to the cumulative distribution function in R, for the three values mentioned earlier, we can write the followings.</p>
<p><code>pbeta(c(0.4,0.5,0.6),2,1)</code> where parameters 2 and 1 are utilized to evaluate the specific <span class="math inline">\(\beta\)</span> density, yielding probabilities of 16%, 25%, and 36%. Therefore, the probability that 40% or fewer of the calls get answered on a given day is 16%, the probability that 50% or fewer get answered is 25%, and the probability that 60% or fewer get answered is 36%. In terms of the survival function, it is simply 1 minus the cumulative distribution function, which can be expressed as 1 minus <span class="math inline">\(x^2\)</span>.</p>
<p>As we progress, we will encounter more complex density functions. However, the process will be simpler since we can rely on existing functions such as <code>pnorm</code> and <code>pbeta</code> instead of calculating them directly.</p>
</div>
<div id="quantiles" class="section level3" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Quantiles</h3>
<p>You’re already familiar with sample quantiles, such as the 95th percentile, which represents the 0.95 quantile of a dataset. If you score at the 95th percentile on an exam, it means that 95% of the students scored worse than you while 5% scored better. Now, let’s introduce the concept of population analogs for quantiles. In the case of the 95th percentile or the 0.95 quantile, you would order the observations from least to greatest and locate the point or exam score below which 95% of the observations lie. This point is denoted as <span class="math inline">\(x_\alpha\)</span>, where alpha corresponds to the quantile. In other words, it satisfies the condition <span class="math inline">\(F(x_\alpha) = \alpha\)</span>, where F is the distribution function. To better understand this concept, let’s try to visualize it.</p>
<p>Let’s consider the distribution function <span class="math inline">\(F(x)\)</span>, which represents the area below point x on a density plot. This area corresponds to the probability that a random variable from the population is less than or equal to x. As an example let’s imagine a population of test scores, an infinite population of students. The distribution function gives us the probability of obtaining a score equal to or lower than x for a randomly selected student from this population.</p>
<p>Now, let’s introduce the concept of the <span class="math inline">\(\alpha^{th}\)</span> quantile. We move a line along the distribution until we find the point <span class="math inline">\(x_\alpha\)</span>, where exactly <span class="math inline">\(\alpha\)</span> proportion of the probability lies below it. This is similar to what we do with our data when finding an empirical quantile, where we locate the data point such that, for example, 95% of the test scores lie below it, which corresponds to the sample <span class="math inline">\(95^{th}\)</span> percentile. In the population distribution, we move the x point until we find the point where the probability of being below it is 95%. Percentiles are essentially quantiles with alpha expressed as a percentage rather than a proportion. The median, often the most well-known quantile, represents the 50th percentile.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-6"></span>
<img src="resources/images/week_01_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_9.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.6: 95 percentile of a distribution
</p>
</div>
<p>Quantiles are frequently used, particularly with the normal distribution. However, we rarely need to directly work with densities to calculate quantiles, as the distributions we commonly encounter have well-defined quantiles. In R, we can easily find quantiles using the <code>q</code> prefix before the density function name. For example, for the <span class="math inline">\(\beta\)</span> density we discussed earlier, the function <code>qbeta</code> gives us the relevant quantile. We can input 0.5 (<code>qbeta(0.5,2,1)</code>)to find the median, considering that R expects the quantile argument as a proportion rather than a percentage, i.e. 0.5 is acceptable and 50 for 50% is not acceptable. The parameters 2 and 1 are specific to the density we’re working with, which you’ll have to trust me on for now. When we calculate the quantile using <code>qbeta</code> with 0.5 as the argument, we obtain the same result as before, 0.7 or 0.71.</p>
<p>At this point, you might be wondering why the concept of the median seemed simpler before, when you would order observations and select the middle value or averaging the two middle values for an even number of observations.</p>
<p>The answer is there you had an estimator. However, in this class, we aim to go beyond just estimators and focus on the targets of estimation, known as estimands. In the case of the sample median, it estimates the population median.</p>
<p>To understand this, let’s consider an example where we sample a few days and calculate the percentage of calls answered on those days. If we line up these percentages in ascending order, the middle value represents the <em>sample median</em>. We can think of this sample median as an estimator for the true median percentage of calls answered in the population. However, to establish a connection between the sample and the population, we need to make certain assumptions, which we will thoroughly explore and formalize in this class.</p>
<p>In essence, for every estimator, there exists an estimand in this class. The sample mean estimates the population mean, the sample median estimates the population median, and the sample standard deviation estimates the population standard deviation, and so on. This process is known as <em>statistical inference</em>, where we link our sample data to the underlying population.</p>
</div>
</div>
<div id="conditional-probability" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Conditional probability</h2>
<p>Conditional probability is a very intuitive idea, “What is the probability given partial information about what has occurred?”. To illustrate the concept of conditioning, let’s consider <a href="https://xkcd.com/795/">XKCD comic</a>. The comic portrays two individuals standing in a field during a lightning storm near a tree. One person suggests going inside, but the other dismisses the idea, citing the low chance of getting struck by lightning, approximately one in seven million. However, the comic humorously points out that the death rate among people who know this statistic is one in six. The underlying message is that the second person has failed to consider the additional information available to them, leading to an incorrect assessment of risk.</p>
<p>Consider another example to better understand conditional probabilities. Suppose we have a standard die, and the probability of rolling a one is assumed to be <span class="math inline">\(\frac{1}{6}\)</span>. However, if we are given the extra information that the roll resulted in an odd number (one, three, or five), our perspective changes. Now, conditioned on this new information, we would no longer say that the probability of rolling a one is <span class="math inline">\(\frac{1}{6}\)</span>. Instead, we would consider the one, three, and five to be equally likely outcomes, so the probability of rolling a one becomes <span class="math inline">\(\frac{1}{3}\)</span>. This demonstrates how conditional probabilities adjust our understanding based on additional information.</p>
<p>To define conditional probability, suppose we have an event B with a nonzero probability. Then, the conditional probability of event A given that B has occurred is denoted as <span class="math inline">\(P(A|B)\)</span> and is defined as:
<span class="math display">\[P(A|B) = \frac{P(A \cap B)}{P(B)}\]</span>
In the case where A and B are statistically independent events (<em>we will define this later</em>), the conditional probability simplifies to the probability of A. This means that if the occurrence of event B provides no new information about event A, the probability of A remains unchanged.</p>
<p>Let’s verify that the concept of conditional probability aligns with our intuition in the example of rolling a die. Event B represents the occurrence of an odd number (one, three, or five), and event A represents rolling a one. We want to find the <span class="math inline">\(P(A|B)\)</span>. In other words, we are interested in the probability of rolling a one when we know that the outcome is an odd number. Using the definition of conditional probability, <span class="math display">\[P(A|B) = \frac{P(A \cap B)}{P(B)}\]</span>. Since A is entirely contained within B, the probability of A ∩ B is simply the probability of A, which is <span class="math inline">\(\frac{1}{6}\)</span>. The probability of B, in this case, is <span class="math inline">\(\frac{3}{6}\)</span> that is <span class="math inline">\(\frac{1}{6}\)</span> for each of the three mutually exclusive possibilities. Thus, the conditional probability <span class="math inline">\(P(A|B)\)</span> equals <span class="math inline">\({\frac{1}{6}}\)</span> divided by <span class="math inline">\({\frac{3}{6}}\)</span>, which simplifies to <span class="math inline">\(\frac{1}{3}\)</span>, confirming our previous understanding.</p>
<p>Conditional probability allows us to update our probabilities based on new information, and it plays a <strong>crucial</strong> role in statistical inference.
### Bayes’ rule
One of the well-known applications of conditional probability is Bayes’ rule, named after Thomas Bayes, a Presbyterian minister whose work was published posthumously. Bayes’ rule allows us to reverse the conditioning set and the set we are interested in finding the probability of. Suppose we want to calculate the probability of event B given event A, and we already know or can easily calculate the probability of event A given event B. Bayes’ rule enables us to evaluate the probability of B given A in terms of the probability of A given B.
<span class="math display">\[P(B|A) = \frac{P(A|B) * P(B)}{P(A|B) * P(B)+P(A|B^c)*P(B^c)}\]</span>
However, to apply Bayes’ rule, we also need the marginal probability of event B, which is valuable in various contexts such as diagnostic tests.</p>
<p>Conditional probability in the context of a diagnostic test, exemplifies one of the significant applications of conditional probability and Bayes’ rule. Consider a test for a disease, where we define plus(+) and minus(-) as events representing a positive or negative test result, respectively. D and <span class="math inline">\(D^c\)</span> represent the events of having or not having the disease, respectively. The sensitivity of the test is the probability that the test is positive given that the subject actually has the disease. A high sensitivity indicates a good test. The specificity, on the other hand, is the probability that the test is negative given that the subject does not have the disease. A high specificity is desirable for a good test. While obtaining accurate estimates of sensitivity and specificity can be challenging, in certain cases, like an HIV blood test, it is possible to test individuals known to have or not have the disease to estimate these probabilities.</p>
<p>When a diagnostic test is positive, the probability of having the disease given the positive test result (positive predictive value) is of particular interest. Similarly, when the test is negative, the probability of not having the disease given the negative test result (negative predictive value) becomes relevant. In the absence of a test, the probability of having the disease is known as the prevalence of the disease.</p>
<p>Here is an example to illustrate the calculation of positive predictive value using Bayes’ rule. Suppose a study comparing the efficacy of an HIV test reports sensitivity as 99.7% and specificity as 98.5%. These numbers are for illustrative purposes and do not reflect actual HIV test statistics. Now, consider a subject from a population with a 0.1% prevalence of HIV who receives a positive test result. We want to calculate the associated positive predictive value.</p>
<p>Applying Bayes’ rule, we have the probability of disease given a positive test result <span class="math inline">\(P(D|+)\)</span> equal to the probability of a positive test result given disease <span class="math inline">\(P(+|D)\)</span> multiplied by the probability of disease <span class="math inline">\(P(D)\)</span>, divided by the denominator. To simplify, we express the probability of a positive test result given no disease as <span class="math inline">\(1-P(-|D^c)\)</span> and the probability of no disease as <span class="math inline">\(1-P(D)\)</span>. Substituting known values, we find the positive predictive value to be 6% for this test in the given population. The low positive predictive value is primarily due to the low prevalence of the disease. However, in a counseling scenario, if the counselor discovers that the subject is an intravenous drug user who regularly has intercourse with an HIV-infected partner, the counselor would consider a much higher prevalence for this particular individual, leading to a higher positive predictive value.</p>
<p>Bayes’ rule provides a powerful framework for incorporating new information and adjusting probabilities based on conditional events, making it valuable in various fields, including diagnostics and decision-making.
We want to distinguish between two components: the component that is dependent on the prevalence and the component that is objective evidence of the positive test result. This is where diagnostic likelihood ratios(DLR) come into play, and we’ll explore them further. First, let’s revisit the formula for positive predictive value in Bayes’ rule, which depends on sensitivity, specificity, and disease prevalence.</p>
<p><span class="math display">\[P(D|+) = \frac{P(+|D)P(D)}{P(+|D) P(D) + P(+|D^c) P(D^c)}\]</span></p>
<p>We can apply a similar approach to calculate the probability of not having the disease given a positive test result.
<span class="math display">\[P(D^c|+) = \frac{P(+|D^c)P(D^c)}{P(+|D) P(D) + P(+|D^c) P(D^c)}\]</span>
By dividing these two equations, we arrive at the odds of disease given a positive test result divided by the odds of not having the disease given a positive test result.
<span class="math display">\[\frac{P(D|+)}{P(D^c|+)} = \frac{P(+|D)P(D)}{P(+|D^c)P(D^c)}\]</span>
post test odds of <span class="math inline">\(D_{+}\)</span>= <span class="math inline">\(DLR_{+}\)</span> * pre test odds of D
Dividing a probability by 1 minus that probability gives us the <strong>odds</strong>. Therefore, on the left side, we have the odds of disease given a positive test result, while on the right side, we have the odds of disease without the test result. The factor in the middle represents the diagnostic likelihood ratio for a positive test result.
The equation can be expressed as follows: the pretest odds of disease multiplied by the diagnostic likelihood ratio equals the post-test odds of disease. In other words, the diagnostic likelihood ratio of a positive test result indicates how much the odds change when multiplied by it, transitioning from pretest to post-test odds.</p>
<p>Returning to our example, assume a subject has a positive HIV test. Using the sensitivity and specificity values mentioned earlier, the diagnostic likelihood ratio is calculated as <span class="math inline">\(0.997\)</span> divided by <span class="math inline">\(1-0.985\)</span>, resulting in <span class="math inline">\(66\)</span>. Regardless of the pretest odds, multiplying them by 66 gives the post-test odds. Thus, the hypothesis of disease is 66 times more supported by the data compared to the hypothesis of no disease. Even if the pretest odds are initially small, multiplying them by 66 will still yield a larger but still small number.</p>
<p>Now, consider the scenario when a subject receives a negative test result using the <span class="math inline">\(DLR_{-}\)</span>. In this case, the <span class="math inline">\(DLR_{-}\)</span>, derived from the sensitivity and specificity values mentioned earlier, is 0.003.
<span class="math display">\[DLR_{-}=\frac{1-0.997}{0.985}≈0.003\]</span>
Consequently, the post-test odds of disease in light of a negative test result become 0.3% of the pretest odds of disease. Stated differently, the hypothesis of disease is supported 0.003 times the hypothesis of no disease given the negative test result.
By incorporating diagnostic likelihood ratios, we can assess the impact of a test result on the odds of disease and gain insights into the strength of evidence provided by the test.</p>
<div id="independence" class="section level3" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Independence</h3>
<p>As mentioned earlier, event A is considered independent of event B if the probability of A given B is equal to the probability of A, given that event B has a positive probability. Another definition of independence states that events A and B are independent if the probability of their intersection <span class="math inline">\(P(A \cap B)\)</span> equals the product of their individual probabilities. This leads us to an important lesson: we cannot simply multiply probabilities without considering the independence of the events involved. Multiplication of probabilities is valid only for independent events.</p>
<p>Example: What is the probability of getting two consecutive heads when flipping a fair coin? We define event A as the probability of getting a head on the first flip and event B as the probability of getting a head on the second flip. Both probabilities are 0.5 since we assume a fair coin. In this case, because the events are independent, the probability of <span class="math inline">\(P(A \cap B)\)</span> (getting heads on both flips) is the product of their probabilities, which is 0.25. This calculation is straightforward and correct.
However, problems arise when people multiply probabilities in situations where they shouldn’t. A notable example of incorrectly multiplying probabilities was reported in volume 309 of Science. It involved a physician who gave expert testimony in a criminal trial. The trial concerned a mother whose two children had died from sudden infant death syndrome (SIDS). The expert testimony multiplied the prevalence of SIDS (1 out of 8,500) by itself to calculate the probability of two children from the same mother having SIDS. Based on this evidence, among other factors, the mother was convicted of murder. The fundamental mistake in this case was multiplying probabilities for events that were not necessarily independent. It is reasonable to assume that events within families, such as the occurrence of SIDS, are dependent due to genetic or familial environmental factors.</p>
<p>In our class, we will primarily use the concept of independence by assuming that a collection of random variables are independent and identically distributed (IID). This means that the random variables are independent from each other and follow the same probability distribution. For example, several coin flips can be considered IID because each flip is independent of the others, and they all follow the same distribution with a 0.5 probability for heads and 0.5 for tails. IID sampling serves as our default model for a random sample. Even if we do not have an actual random sample, we often use the conceptual model of random sampling or IID to analyze our data. It will be the principal mode of analysis in this class.</p>
</div>
</div>
<div id="expected-values" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Expected values</h2>
<p>The empirical average is a very intuitive idea; it’s the middle of our data in a sense. But, what is it estimating? We can formally define the middle of a population distribution. This is the expected value. Expected values are very useful for characterizing populations and usually represent the first thing that we’re interested in estimating.</p>
<p>Now, we will discuss the process of drawing conclusions about populations based on noisy data obtained from them. We will assume that the populations and the randomness governing our samples are described by probability density functions and probability mass functions. Instead of focusing on the entire function, we will examine characteristics of these distributions that are reflected in the random variables drawn from them. The most valuable such characteristics are expected values, particularly the mean. The mean represents the center of a distribution. As the mean shifts, the distribution moves either to the left or right.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-7"></span>
<img src="resources/images/week_01_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_16.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.7: Mean of a distribution
</p>
</div>
Another important characteristic is variance, which measures the spread of a distribution.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-8"></span>
<img src="resources/images/week_01_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_19.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.8: Variance of a distribution
</p>
</div>
<p>Similar to how sample quantiles estimate population quantiles, sample expected values estimate population expected values. Therefore, the sample mean serves as an estimate of the population mean, the sample variance estimates the population variance, and the sample standard deviation approximates the population standard deviation.</p>
<p>The expected value, or mean, of a random variable represents the center of its distribution. For a discrete random variable x with a probability mass function <span class="math inline">\(p(x)\)</span>, the expected value is calculated by summing the possible values that x can take multiplied by their respective probabilities.
<span class="math display">\[E[X]=\sum_{x} xp(x)\]</span>
Conceptually, the expected value draws inspiration from the idea of the physical center of mass, where the probabilities act as weights and x represents the location along an axis. To illustrate this notion of center of mass, consider the sample mean. Even though we are focusing on the population mean in this discussion, it is interesting to note that the sample mean can be seen as the center of mass if we treat each data point as equally likely. In other words, each data point <span class="math inline">\(x_i\)</span> is assigned a probability of <span class="math inline">\(\frac{1}{N}\)</span>, where N is the sample size. Intuitively, we employ this center of mass idea when using the sample mean.</p>
<p>To demonstrate this concept, I have provided some code that calculates the sample mean of a dataset and depicts it as the center of mass by generating a histogram. The example employs a dataset from R called “Galton,” which consists of paired data representing the heights of parents and their children.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="probability-expected-values.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(reshape2)</span>
<span id="cb1-2"><a href="probability-expected-values.html#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(UsingR)</span>
<span id="cb1-3"><a href="probability-expected-values.html#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-4"><a href="probability-expected-values.html#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(galton)</span>
<span id="cb1-5"><a href="probability-expected-values.html#cb1-5" aria-hidden="true" tabindex="-1"></a>longGalton <span class="ot">&lt;-</span> <span class="fu">melt</span>(galton, <span class="at">measure.vars =</span> <span class="fu">c</span>(<span class="st">&quot;child&quot;</span>, <span class="st">&quot;parent&quot;</span>))</span>
<span id="cb1-6"><a href="probability-expected-values.html#cb1-6" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(longGalton, <span class="fu">aes</span>(<span class="at">x =</span> value)) <span class="sc">+</span> <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..density..,  <span class="at">fill =</span> variable), <span class="at">binwidth=</span><span class="dv">1</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span> <span class="fu">geom_density</span>(<span class="at">size =</span> <span class="dv">2</span>)</span>
<span id="cb1-7"><a href="probability-expected-values.html#cb1-7" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> g <span class="sc">+</span> <span class="fu">facet_grid</span>(. <span class="sc">~</span> variable)</span>
<span id="cb1-8"><a href="probability-expected-values.html#cb1-8" aria-hidden="true" tabindex="-1"></a>g</span></code></pre></div>
<p>The histogram displays the child’s height distribution, and a continuous density estimate is superimposed.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-10"></span>
<img src="resources/images/week_01_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_22.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.9: Height distribution for Childran and Parents
</p>
</div>
<p>To further explore this concept, we can use the “manipulate” function available in RStudio. By manipulating the mean value, we can observe how it balances out the histogram.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="probability-expected-values.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(manipulate)</span>
<span id="cb2-2"><a href="probability-expected-values.html#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(UsingR)</span>
<span id="cb2-3"><a href="probability-expected-values.html#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb2-4"><a href="probability-expected-values.html#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(galton)</span>
<span id="cb2-5"><a href="probability-expected-values.html#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="probability-expected-values.html#cb2-6" aria-hidden="true" tabindex="-1"></a>myHist <span class="ot">&lt;-</span> <span class="cf">function</span>(mu){</span>
<span id="cb2-7"><a href="probability-expected-values.html#cb2-7" aria-hidden="true" tabindex="-1"></a>    g <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(galton, <span class="fu">aes</span>(<span class="at">x =</span> child))</span>
<span id="cb2-8"><a href="probability-expected-values.html#cb2-8" aria-hidden="true" tabindex="-1"></a>    g <span class="ot">&lt;-</span> g <span class="sc">+</span> <span class="fu">geom_histogram</span>(<span class="at">fill =</span> <span class="st">&quot;salmon&quot;</span>, </span>
<span id="cb2-9"><a href="probability-expected-values.html#cb2-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">binwidth=</span><span class="dv">1</span>, <span class="fu">aes</span>(<span class="at">y =</span> ..density..), <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb2-10"><a href="probability-expected-values.html#cb2-10" aria-hidden="true" tabindex="-1"></a>    g <span class="ot">&lt;-</span> g <span class="sc">+</span> <span class="fu">geom_density</span>(<span class="at">size =</span> <span class="dv">2</span>)</span>
<span id="cb2-11"><a href="probability-expected-values.html#cb2-11" aria-hidden="true" tabindex="-1"></a>    g <span class="ot">&lt;-</span> g <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> mu, <span class="at">size =</span> <span class="dv">2</span>)</span>
<span id="cb2-12"><a href="probability-expected-values.html#cb2-12" aria-hidden="true" tabindex="-1"></a>    mse <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">mean</span>((galton<span class="sc">$</span>child <span class="sc">-</span> mu)<span class="sc">^</span><span class="dv">2</span>), <span class="dv">3</span>)  </span>
<span id="cb2-13"><a href="probability-expected-values.html#cb2-13" aria-hidden="true" tabindex="-1"></a>    g <span class="ot">&lt;-</span> g <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">title =</span> <span class="fu">paste</span>(<span class="st">&#39;mu = &#39;</span>, mu, <span class="st">&#39; MSE = &#39;</span>, mse))</span>
<span id="cb2-14"><a href="probability-expected-values.html#cb2-14" aria-hidden="true" tabindex="-1"></a>    g</span>
<span id="cb2-15"><a href="probability-expected-values.html#cb2-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-16"><a href="probability-expected-values.html#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="fu">manipulate</span>(<span class="fu">myHist</span>(mu), <span class="at">mu =</span> <span class="fu">slider</span>(<span class="dv">62</span>, <span class="dv">74</span>, <span class="at">step =</span> <span class="fl">0.5</span>))</span></code></pre></div>
You can use the slider to move the mean value and observe how it affects the mean squared error.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-12"></span>
<img src="resources/images/week_01_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_25.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.10: Height distribution for Childran
</p>
</div>
<p>The mean squared error is a measure of imbalance, indicating how stable or unsteady the histogram appears. As we move the mean closer to the center of the distribution, the mean value increases, while the mean squared error decreases, signifying a better balance. However, if we move the mean too far from the center, the mean squared error increases again, indicating increased imbalance. This demonstration illustrates that the empirical mean serves as the balancing point for the empirical distribution, and we will utilize this concept when discussing the population mean, which serves as the balancing point for the population distribution.</p>
<p>Example: Suppose we flip a fair coin, and we assign the value 0 to tails and the value 1 to heads. What is the expected value of X?
Again, the expected value represents a property of the population. By plugging the values into our formula, we calculate the expected value of X as follows:
<span class="math display">\[E[X]=\sum_{x} xp(x)=0*0.5+1*0.5=0.5\]</span></p>
<p>When we compute this expression, we find that the expected value of X is 0.5. It’s interesting to note that the expected value is a value that the coin itself cannot actually take. However, from a geometric perspective, the answer becomes quite obvious. If we visualize the coin’s values as two bars of equal height, one at 0 and the other at 1, we can easily determine the balancing point by placing our finger exactly at 0.5.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-13"></span>
<img src="resources/images/week_01_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_28.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.11: Expected value of a coin flip
</p>
</div>
<p>Example: A random variable X represents the outcome of a biased coin flip. The probability of obtaining heads is denoted as <span class="math inline">\(p\)</span>, while the probability of obtaining tails is <span class="math inline">\(1-p\)</span>. What is the expected value of X in this case?
By directly applying the formula, we multiply the value 0 by the probability <span class="math inline">\(1-p\)</span> and add it to the value 1 multiplied by the probability <span class="math inline">\(p\)</span>. The result simplifies to <span class="math inline">\(p\)</span>. Therefore, the expected value of a coin flip, even when the coin is biased, corresponds to the true long-run proportion of obtaining heads in an infinite number of coin flips.</p>
<p>Example: Suppose we roll a fair six-sided die, and X represents the number that appears face up. What is the expected value of X?
Here, we take the values 1, 2, 3, 4, 5, and 6 and multiply each by the corresponding probability of the random variable X taking those values (each value has a probability of <span class="math inline">\(\frac{1}{6}\)</span>). When we perform this calculation, we find that the expected value of X is 3.5. Once again, this is a value that the die itself cannot actually show.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-14"></span>
<img src="resources/images/week_01_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_40.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.12: Expected value of a die roll
</p>
</div>
<p>Similar to the coin example, the geometric argument makes it evident. We have six bars, each with a height of <span class="math inline">\(\frac{1}{6}\)</span>, representing the possible outcomes of the die. If we were to balance them, it becomes clear that the balancing point would be at 3.5.
### Expected values for PDFs
When dealing with continuous random variables, it can be helpful to imagine cutting out the shape of probability density on a piece of wood and determining where you would place your finger to balance it out. This concept aligns with the notion of the center of mass of a continuous body. In the case of probability mass functions, as the bars representing the probabilities become narrower and smaller, we can visualize their balancing point.</p>
<p>Example: Suppose we have a density that ranges from zero to one, and the question arises: Is this a valid density?
The answer is yes; it corresponds to a well-known density called the <strong>Uniform density</strong>. Now, what is its expected value?
If we were to cut this density out of a piece of wood and balance it, the position where we would place our finger to achieve balance is precisely at 0.5. This aligns perfectly with the expected value of the uniform density.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-15"></span>
<img src="resources/images/week_01_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_43.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.13: Uniform density
</p>
</div>
<p>It’s crucial to understand that expected values represent properties of the distribution. They serve as the center of mass of a distribution. Additionally, it’s important to note that the average of random variables is, in itself, a random variable. For example, if we roll six dice and calculate their average, the resulting value is a random variable. By repeatedly sampling from this average through multiple dice rolls, we generate a distribution that also possesses an expected value. The center of mass of this distribution coincides with the center of mass of the original distribution.</p>
<p>This topic becomes highly relevant to the field of inference, so let’s explore some simulation examples to gain a better understanding.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-16"></span>
<img src="resources/images/week_01_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_46.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.14: Simulation example
</p>
</div>
<p>In the first example, the blue density represents the outcome of numerous simulations based on a standard normal distribution. Due to the large number of simulations, this density provides a reliable approximation of the true distribution. It shows that collecting ample data from a population allows us to approximate its originating distribution effectively. The center of mass of this distribution, which would achieve balance, is located at zero.</p>
<p>Now, let’s shift our focus to simulating the average of ten standard normals. By repeatedly performing this process and plotting the resulting histogram or density estimate, we obtain a different distribution. It no longer represents the distribution of standard normals; rather, it illustrates the distribution of averages of ten standard normals. This new distribution, represented by the salmon-colored plot, exhibits interesting properties. Notably, it is concentrated around zero, and this aligns with our previous point. The distribution of averages from a population tends to be centered at the same location as the distribution of the original population itself.</p>
<p>Although calculations and simulations can help us grasp these concepts conceptually, we can observe this phenomenon without explicitly performing them. Imagine rolling a die thousands of times and plotting a histogram of the results. In this case, approximately <span class="math inline">\(\frac{1}{6}\)</span> of the rolls would occur for each number from one to six. As we increase the number of rolls, these bars would eventually balance out. The center of mass for this distribution, which would achieve balance, is 3.5 (not exactly, given the finite number of rolls, but in theory, it would converge to 3.5 with an infinite number of rolls).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-17"></span>
<img src="resources/images/week_01_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_49.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.15: Die roll simulation
</p>
</div>
<p>Consider the scenario where we roll the die twice and calculate the average of the numbers obtained. If we repeat this process multiple times and create a distribution of these averages, we see a different pattern in the second panel. It appears more Gaussian in shape (we’ll discuss this further later), and importantly, it is centered at the same location as before.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-18"></span>
<img src="resources/images/week_01_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_52.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.16: Coin toss average
</p>
</div>
<p>The population mean of averages of two die rolls is identical to the population mean of individual die rolls. This concept applies to other scenarios as well. For instance, if we were to flip a coin numerous times, we would expect approximately 50% of the outcomes to be zero (tails) and 50% to be one (heads). These proportions would converge to balance at around 0.5. When flipping the coin only a few times, the observed sample proportion may deviate from 0.5. However, as we increase the number of flips, the simulation variability becomes insignificant, and the proportion approaches 0.5.</p>
<p>If we flip the coin ten times, calculate the average, and repeat this process multiple times. This simulation provides insights into the distribution of averages of ten coin flips. We can extend this analysis to averages of 20 coin flips and averages of 30 coin flips. In each case, we observe that as the average incorporates more coin flips, the distribution becomes more concentrated around the mean. Nevertheless, regardless of the number of coin flips involved, the distribution of averages is consistently centered at 0.5.</p>
<p>To summarize the key points covered thus far:</p>
<ul>
<li>Expected values are inherent properties of distributions. The population mean represents the center of mass of that population, and any movement in the mean would correspondingly shift the distribution.</li>
<li>The sample mean represents the center of mass of the observed data. It serves as an estimate of the population mean and is considered unbiased.</li>
<li>The population mean of the distribution of sample means precisely matches the population mean it aims to estimate. This understanding is vital as it allows us to estimate the population distribution accurately when collecting substantial amounts of data.</li>
<li>We must recognize that while we obtain only one sample mean from our data, knowing the properties associated with sample means is immensely valuable.</li>
<li>As more data contributes to the sample mean, the density mass function becomes more concentrated around the population mean. We also observe that, even in cases such as coin flipping and dice rolling, the distribution tends to exhibit Gaussian-like characteristics. We’ll explore these concepts further in subsequent lectures.</li>
</ul>
</div>
<div id="practical-r-exercises-in-swirl" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> Practical R Exercises in swirl</h2>
<p>During this course we’ll be using the <a href="http://swirlstats.com/">swirl</a> software package for R in order to illustrate some key concepts. The swirl package turns the R console into an interactive learning environment. Using swirl will also give you the opportunity to construct and explore your own regression models. In this programming assignment, you’ll have the opportunity to practice some key concepts from this course.</p>
<p>Since swirl is an R package, you can easily install it by entering a single command from the R console:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="probability-expected-values.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;swirl&quot;</span>)</span></code></pre></div>
<p>If you’ve installed swirl in the past make sure you have version 2.2.21 or later. You can check this with:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="probability-expected-values.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">packageVersion</span>(<span class="st">&quot;swirl&quot;</span>)</span></code></pre></div>
<p>Every time you want to use swirl, you need to first load the package. From the R console:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="probability-expected-values.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(swirl)</span></code></pre></div>
<p>Install the Statistical Inference course
swirl offers a variety of interactive courses, but for our purposes, you want the one called Statistical Inference. Type the following from the R prompt to install this course:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="probability-expected-values.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install_from_swirl</span>(<span class="st">&quot;Statistical Inference&quot;</span>)</span></code></pre></div>
<p>Start swirl and complete the lessons
Type the following from the R console to start swirl:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="probability-expected-values.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">swirl</span>()</span></code></pre></div>
<p>Then, follow the menus and select the Statistical Inference course when given the option. For the first part of this course you should complete the following lessons:</p>
<ul>
<li>Introduction</li>
<li>Probability1</li>
<li>Probability2</li>
<li>ConditionalProbability</li>
<li>Expectations</li>
</ul>
<p>If you need help…</p>
<ul>
<li>Visit the <a href="https://github.com/swirldev/swirl/wiki/Coursera-FAQ">Frequently Asked Questions</a> (FAQ) page to see if you can answer your own question immediately.</li>
<li>Search the Discussion Forums this course.</li>
<li>If you still can’t find an answer to your question, then create a new thread under the swirl Programming Assignment sub-forum and provide the following information:
<ul>
<li>A descriptive title</li>
<li>Any input/output from the console (copy &amp; paste) or a screenshot</li>
<li>The output from sessionInfo()</li>
</ul></li>
</ul>
<p>Good luck and have fun!</p>
<p>For more information on swirl, visit Swirlstats(<a href="https://swirlstats.com" class="uri">https://swirlstats.com</a>).</p>

</div>
</div>
<hr>
<center> 
  <div class="footer">
      All illustrations <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY. </a>
      <br>
      All other materials <a href= "https://creativecommons.org/licenses/by/4.0/"> CC-BY </a> unless noted otherwise.
  </div>
</center>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="variability-distribution-asymptotics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
