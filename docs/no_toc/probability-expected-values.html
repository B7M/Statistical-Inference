<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Probability &amp; Expected Values | Course Name</title>
  <meta name="description" content="Description about Course/Book." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Probability &amp; Expected Values | Course Name" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Description about Course/Book." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Probability &amp; Expected Values | Course Name" />
  
  <meta name="twitter:description" content="Description about Course/Book." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="assets/dasl_favicon.ico" type="image/x-icon" />
<link rel="prev" href="index.html"/>
<link rel="next" href="variability-distribution-asymptotics.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>




<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<a href="http://jhudatascience.org/"><img src="https://jhudatascience.org/images/dasl.png" style=" width: 80%; padding-left: 40px; padding-top: 8px; vertical-align: top "</a>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Course</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#available-course-formats"><i class="fa fa-check"></i><b>0.1</b> Available course formats</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="probability-expected-values.html"><a href="probability-expected-values.html"><i class="fa fa-check"></i><b>1</b> Probability &amp; Expected Values</a>
<ul>
<li class="chapter" data-level="1.0.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#syllabus"><i class="fa fa-check"></i><b>1.0.1</b> Syllabus</a></li>
<li class="chapter" data-level="1.0.2" data-path="probability-expected-values.html"><a href="probability-expected-values.html#course-book-statistical-inference-for-data-science"><i class="fa fa-check"></i><b>1.0.2</b> Course Book: Statistical Inference for Data Science</a></li>
<li class="chapter" data-level="1.0.3" data-path="probability-expected-values.html"><a href="probability-expected-values.html#homework-problems"><i class="fa fa-check"></i><b>1.0.3</b> Homework Problems</a></li>
<li class="chapter" data-level="1.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="probability-expected-values.html"><a href="probability-expected-values.html#probability"><i class="fa fa-check"></i><b>1.2</b> Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#probability-mass-functions-and-probability-density-functions"><i class="fa fa-check"></i><b>1.2.1</b> Probability mass functions and probability density functions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="probability-expected-values.html"><a href="probability-expected-values.html#conditional-probability"><i class="fa fa-check"></i><b>1.3</b> Conditional probability</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#independence"><i class="fa fa-check"></i><b>1.3.1</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="probability-expected-values.html"><a href="probability-expected-values.html#expected-values"><i class="fa fa-check"></i><b>1.4</b> Expected values</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html"><i class="fa fa-check"></i><b>2</b> Variability, Distribution, &amp; Asymptotics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#variability"><i class="fa fa-check"></i><b>2.1</b> Variability</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#variance-simulation-examples"><i class="fa fa-check"></i><b>2.1.1</b> Variance simulation examples</a></li>
<li class="chapter" data-level="2.1.2" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#variance-data-example"><i class="fa fa-check"></i><b>2.1.2</b> Variance data example</a></li>
<li class="chapter" data-level="2.1.3" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#poisson-distribution"><i class="fa fa-check"></i><b>2.1.3</b> Poisson distribution</a></li>
<li class="chapter" data-level="2.1.4" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#asymptotics-and-the-clt"><i class="fa fa-check"></i><b>2.1.4</b> Asymptotics and the CLT</a></li>
<li class="chapter" data-level="2.1.5" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#asymptotics-and-confidence-intervals"><i class="fa fa-check"></i><b>2.1.5</b> Asymptotics and confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#practical-r-exercises-in-swirl"><i class="fa fa-check"></i><b>2.2</b> Practical R Exercises in swirl</a></li>
<li class="chapter" data-level="2.3" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#quiz"><i class="fa fa-check"></i><b>2.3</b> Quiz</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html"><i class="fa fa-check"></i><b>3</b> Intervals, Testing, &amp; Pvalues</a>
<ul>
<li class="chapter" data-level="3.0.1" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#independent-group-t-intervals"><i class="fa fa-check"></i><b>3.0.1</b> Independent group T intervals</a></li>
<li class="chapter" data-level="3.1" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.1</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#t-tests"><i class="fa fa-check"></i><b>3.1.1</b> T tests</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#p-values"><i class="fa fa-check"></i><b>3.2</b> P values</a></li>
<li class="chapter" data-level="3.3" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#knitr"><i class="fa fa-check"></i><b>3.3</b> Knitr</a></li>
<li class="chapter" data-level="3.4" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#practical-r-exercises-in-swirl-1"><i class="fa fa-check"></i><b>3.4</b> Practical R Exercises in swirl</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html"><i class="fa fa-check"></i><b>4</b> Power, Bootstrapping, &amp; Permutation Tests</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#calculating-power"><i class="fa fa-check"></i><b>4.0.1</b> Calculating Power</a></li>
<li class="chapter" data-level="4.1" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#multiple-comparisons"><i class="fa fa-check"></i><b>4.1</b> Multiple Comparisons</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="5" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>5</b> References</a></li>
<li class="divider"></li>
<p style="text-align:center;"> <a href="https://github.com/jhudsl/OTTR_Template" target="blank" > This content was published with</a> <a href="https://bookdown.org/" target="blank"> bookdown by:</a> </p>
<p style="text-align:center;"> <a href="http://jhudatascience.org/"> The Johns Hopkins Data Science Lab </a></p>
<p style="text-align:center; font-size: 12px;"> <a href="https://github.com/rstudio4edu/rstudio4edu-book/"> Style adapted from: rstudio4edu-book </a> <a href ="https://creativecommons.org/licenses/by/2.0/"> (CC-BY 2.0) </a></p>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Course Name</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<head>
  <meta name="viewport" content="width=device-width,minimum-scale=1.0,maximum-scale=1.0,initial-scale=1.0">
  <!--script src="https://kit.fontawesome.com/6a26f47516.js"></script-->
  <!--<script src="assets/hideOutput.js"></script>-->
  <link href="assets/style.css" rel="stylesheet">
</head>



<div class="hero-image-container">
  <img class= "hero-image" src= "https://github.com/jhudsl/OTTR_Template/raw/main/assets/dasl_thin_main_image.png">
</div>
<div id="probability-expected-values" class="section level1" number="1">
<h1><span class="header-section-number">Chapter 1</span> Probability &amp; Expected Values</h1>
<p>This week, we’ll focus on the fundamentals including probability, random variables, expectations and more.
## About the course</p>
<div id="syllabus" class="section level3" number="1.0.1">
<h3><span class="header-section-number">1.0.1</span> Syllabus</h3>
</div>
<div id="course-book-statistical-inference-for-data-science" class="section level3" number="1.0.2">
<h3><span class="header-section-number">1.0.2</span> Course Book: Statistical Inference for Data Science</h3>
</div>
<div id="homework-problems" class="section level3" number="1.0.3">
<h3><span class="header-section-number">1.0.3</span> Homework Problems</h3>
</div>
<div id="introduction" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Introduction</h2>
<p>Greetings and a warm welcome to the Probability class, which is a part of the Statistical Inference course within the Coursera Data Science series. I’m Brian Caffo, and I will be one of your instructors for this class. Alongside me, we have Jeff Leek and Roger Peng, who will also be co-teaching the course. We all belong to the Department of Biostatistics at the Bloomberg School of Public Health.</p>
</div>
<div id="probability" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Probability</h2>
<p>In today’s lecture, we will cover the fundamentals of probability at a beginner’s level, providing you with the necessary knowledge for your journey in the data science specialization. If you’re interested in delving deeper into this topic, I highly recommend checking out my comprehensive mathematical biostatistics boot camp series. In this module we discuss probability, the foundation of statistical analysis. Probability assigns a number between 0 and 1 to events to give a sense of the “chance” of the event. Probability has become our default model for apparently random phenomena. Our eventual goal is to use probability models, our formal mechanism for connecting our data to a population. However, before we get to probability models, we need to understand the basics of probability calculus. The next few lectures cover these basics.</p>
<p><strong>Probability</strong> = the study of quantifying the likelihood of particular events occurring
- given a random experiment, probability = population quantity that summarizes the randomness
This summary is not just about the data at hand, but a conceptual quantity that exist in the population that we want to estimate.</p>
<p>Let’s consider a random experiment, such as rolling a die, where probability serves as a measure of the overall characteristic that captures the randomness involved. It’s important to highlight the concept of population in this context. Specifically, when we talk about the probability of a die roll, we view it as an inherent property of the die itself, rather than being dependent on a specific set of predetermined rolls. When discussing probability, it’s crucial to understand that we are referring to a conceptual notion that exists within the population we aim to estimate, rather than being directly observed in the available data. Now, let’s delve into the specific principles that govern probability, known as probability calculus. To begin with, probability operates on the potential outcomes that can arise from an experiment. For instance, when rolling a die, the possible outcomes could be any specific number like 1, or a broader set such as 1 or 2, or even broader categories like even numbers (2, 4, 6) or odd numbers (1, 3, 5), and so on. Thus, probability can be seen as a function that assigns a number between 0 and 1 to each of these sets of possible outcomes.</p>
<p>One important rule is that the probability of an event occurring, such as rolling the die and obtaining a certain number, must be equal to 1. In other words, the probabilities of all possible outcomes collectively should sum up to 1. Furthermore, the probability of the union of two sets of outcomes that have no common elements must be equal to the sum of their individual probabilities. For instance, let’s consider the scenario of rolling a die. One possible outcome could be obtaining a one or a two, while another possible outcome could be getting a three or a four. These two sets, {1, 2} and {3, 4}, are mutually exclusive since they cannot occur simultaneously. The probability of the union, which involves getting a 1, 2, 3, or 4, is calculated by adding the probabilities of the individual sets: the probability of getting a 1 or a 2, plus the probability of getting a 3 or a 4. These fundamental rules I’ve described encompass the core principles that govern probability. Interestingly, these rules were discovered by the Russian mathematician Kolmogorov and serve as the basis for all other rules associated with probability. Let’s outline some of these rules, some of which I have already mentioned, while others follow as consequences of the previously stated rules.</p>
<ol style="list-style-type: decimal">
<li><p>The probability of an event that cannot occur is zero. In the case of rolling a die, it is impossible to obtain no outcome, so the probability of nothing occurring is zero.</p></li>
<li><p>Conversely, the probability of an event occurring, such as obtaining a number when rolling a die, is equal to one. This is because something must happen, and getting a number is a certain outcome.</p></li>
<li><p>It is intuitive to understand that the probability of an event happening is equal to one minus the probability of the opposite event occurring. For example, the probability of rolling an even number on a die is equal to one minus the probability of rolling an odd number. This is because the set of odd numbers is considered the opposite of obtaining an even number in the context of rolling a die.</p></li>
</ol>
<p>The probability of at least one of two or more mutually exclusive events, which cannot occur simultaneously, is the sum of their individual probabilities. This aligns with the definition we discussed earlier.</p>
<p>Another consequence of probability calculus is that if event A implies the occurrence of event B, then the probability of event A is less than or equal to the probability of event B. Although this may sound complex when explained verbally, it becomes clearer when visualized using a Venn diagram. In the diagram, event A is represented by a circle contained within event B. When we consider the probability of A, we assign a number to the area within circle A. Similarly, when discussing event B, we refer to the probability assigned to the entire circle, which includes the area of A. Therefore, it logically follows that the probability of B is larger than or equal to the probability of A. This concept is often intuitive and easily understood once visualized.</p>
<p>For instance, the probability of rolling a 1 (set A) is less than the probability of rolling a 1 or a 2 (set B).</p>
<p>Now, let’s discuss a useful rule: for any two events, the probability of at least one occurring is equal to the sum of their probabilities minus the probability of their intersection. Again, visualizing this with a Venn diagram helps in understanding it better. Consider set A and set B. When we add their individual probabilities, we are effectively adding the intersection region twice, once when considering A and once when considering B. Since we have counted the intersection twice, to obtain the probability of their union, we need to subtract the intersection once. This rule highlights that we cannot simply add probabilities if there exists a non-trivial intersection between the events.</p>
<p>Now, let’s illustrate an example to demonstrate why we cannot simply add probabilities when the events are not mutually exclusive. According to the National Sleep Foundation, approximately 3% of the American population has sleep apnea, while around 10% of the North American and European population has restless leg syndrome. Let’s assume, for the sake of argument, that these probabilities are derived from the same population.</p>
<p>The question is, can we add these probabilities together to conclude that about 13% of people in this population have at least one of these sleep problems? The answer is no. The reason is that these events, sleep apnea and restless leg syndrome, can occur simultaneously and are not mutually exclusive. There is a non-trivial portion of the population that experiences both conditions concurrently.</p>
<p>To elaborate further, let’s define event A as the occurrence of sleep apnea in a person drawn from this population, and event B as the occurrence of restless leg syndrome. In this case, we believe that the intersection of these two events (the occurrence of both conditions) is non-trivial. If we were to naively add the probabilities of A and B, we would essentially count the intersection twice, which would result in an overestimate. To determine the probability of the union (at least one of the conditions), we need to subtract the intersection once, recognizing that it was mistakenly included twice in the initial addition.</p>
<p><strong>General Probability Rules</strong>
- discovered by Russian mathematician Kolmogorov, also known as “Probability Calculus”
- probability = function of any set of outcomes and assigns it a number between 0 and 1
- 0 ≤ P(E) ≤ 1, where E = event
- probability that nothing occurs = 0 (impossible, have to roll dice to create outcome), that something occurs is 1 (certain)
- probability of outcome or event E, P(E) = ratio of ways that E could occur to number of all possible outcomes or events
- probability of something = 1 - probability of the opposite occurring
- probability of the union of any two sets of outcomes that have nothing in common (mutually exclusive) = sum of respective probabilities</p>
<p>xxx</p>
<ul>
<li>if A implies occurrence of B, then P(A) occurring &lt; P(B) occurring</li>
</ul>
<p>xxx</p>
<ul>
<li>for any two events, probability of at least one occurs = the sum of their probabilities - their intersection (in other words, probabilities can not be added simply if they have non-trivial intersection)
xxx</li>
<li>for independent events A and B, P(A ∪ B) = P(A) × P(B)</li>
<li>for outcomes that can occur with different combination of events and these combinations are mutually
exclusive, the P(E_total) = P(E_part)</li>
</ul>
<div id="probability-mass-functions-and-probability-density-functions" class="section level3" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Probability mass functions and probability density functions</h3>
<p>Probability calculus provides a valuable framework for understanding the fundamental rules that govern probability and serves as the basis for all probabilistic thinking. However, when it comes to numeric outcomes of experiments, we require a more practical approach. This is where densities and mass functions for random variables come into play, serving as a convenient starting point. These concepts will be sufficient for our purposes.</p>
<p>One of the most well-known examples of a density function is the bell curve, also known as the normal distribution. In this class, you will gain a deeper understanding of what it truly means for data to follow a bell curve. You will learn about the significance and interpretation of the bell curve. Importantly, you will also realize that when discussing probabilities associated with the bell curve or the normal distribution, we are referring to population quantities, not statements solely based on the observed data.</p>
<p>The approach we will take involves collecting data that will be utilized to estimate properties of the population. This is the direction we aim to progress towards throughout the course.</p>
<p>Before delving into data analysis, it is crucial to develop our intuition for understanding population quantities. A random variable represents the numerical outcome of an experiment. In our study, we will encounter two types of random variables: discrete and continuous.</p>
<p>Discrete random variables are those that can be counted, such as the number of web hits or the possible outcomes of rolling a die. They can even include non-numeric attributes like hair color, which can be assigned numeric values (e.g., 1 for blonde, 2 for brown, 3 for black, etc.). For discrete random variables, we assign probabilities to each possible value they can take.</p>
<p>On the other hand, continuous random variables can assume any value within a range or continuum. When working with continuous random variables, we assign probabilities to ranges of values they can take.</p>
<p>Let’s consider some simple examples that can be viewed as random variables, as these examples will aid in building our intuition throughout the course. One prominent example is the flip of a coin, where we can assign values of “heads” or “tails” (or 0 and 1) to represent the outcomes. This is a discrete random variable since it can only take two distinct levels.</p>
<p>Another example of a discrete random variable is the outcome of rolling a die. It can only take one of six possible values, making it a discrete random variable with simple probability mechanics.</p>
<p>Now let’s consider some more complex random variables. For instance, the amount of website traffic or the number of web hits on a given day can be treated as a count random variable. While we’ll likely treat it as discrete, it’s interesting because it doesn’t have an upper bound. In such cases, we might employ the Poisson distribution to model it.</p>
<p>Next, let’s take the example of measuring a subject’s body mass index (BMI) four years after a baseline measurement. In this case, BMI would be considered a continuous random variable, as it can assume any value within a range.</p>
<p>The hypertension status of a randomly selected subject from a population can also be a random variable. We may assign a value of 1 to indicate the presence of hypertension or a diagnosis, and 0 otherwise. This random variable would typically be modeled as discrete.</p>
<p>Consider another example: the number of people who click on an advertisement. This is also a discrete random variable, but it is unbounded. Nevertheless, we would assign probabilities to different values, such as zero clicks, one click, two clicks, and so on.</p>
<p>Lastly, intelligence quotients (IQ) are often modeled as continuous random variables.</p>
<p>When working with discrete random variables, we assign a probability to each possible value they can take. We represent this assignment using a function called the probability mass function (PMF). The PMF takes any value of the discrete random variable and assigns the probability of it taking that specific value.</p>
<p>For example, in the case of a die roll, the PMF would assign a probability of one-sixth to the value one, one-sixth to the value two, one-sixth to the value three, and so on.</p>
<p>To ensure that the PMF satisfies the basic rules of probability, we have two requirements. First, the PMF must always be greater than or equal to zero since probabilities range from zero to one, inclusive. Second, the sum of the probabilities assigned to all possible values of the random variable must add up to one. In the case of a die roll, if we add the probabilities of getting one, two, three, four, five, and six, the sum should equal one. This ensures that the probability of any possible outcome occurring is accounted for.</p>
<p>Therefore, the PMF of a discrete random variable must adhere to these two rules to accurately represent probabilities.</p>
<p>We will primarily focus on using probability mass functions (PMFs) that are particularly useful in our context. Two examples of such PMFs are the binomial distribution, commonly used for coin flips, and the Poisson distribution, commonly used for counting events. However, let’s discuss one of the most well-known PMFs, the Bernoulli distribution, which is often used to model the outcome of a coin flip.</p>
<p>Let’s denote the random variable representing the coin flip outcome as capital X, where X = 0 represents tails and X = 1 represents heads. In this notation, an uppercase letter represents a potential value of the random variable that may or may not occur. On the other hand, a lowercase x serves as a placeholder for a specific value that we will substitute.</p>
<p>The PMF for the Bernoulli distribution is represented as p(X) = (1/2)^x * (1/2)^(1-x). When we substitute x = 0 into this PMF, we obtain a probability of one-half. Similarly, when we substitute x = 1, we also get a probability of one-half. This means that the probability of the random variable X taking the value 0 is one-half, and the probability of it taking the value 1 is also one-half.</p>
<p>When we introduce an unfair coin, we can adjust our approach by considering a parameter, theta, representing the probability of getting a head. The probability of getting a tail would then be 1 minus theta, where theta is a number between 0 and 1. In this case, the probability mass function can be written as follows: P(X) = theta^x * (1 - theta)^(1 - x).</p>
<p>By substituting x = 1 into this PMF, we obtain the probability theta. Similarly, when we substitute x = 0, we get the probability 1 minus theta. This implies that for this population distribution, the probability of the random variable X taking the value 0 is 1 minus theta, and the probability of it taking the value 1 is theta.</p>
<p>This approach is particularly useful for modeling the prevalence of a certain condition or event. For instance, if we want to model the prevalence of hypertension, we can assume that the population or sample we are studying can be likened to the outcomes of biased coin flips with a success probability represented by theta. However, the challenge lies in not knowing the exact value of theta. Therefore, we will utilize our data to estimate this proportion within the population.</p>
<p>In contrast to the probability mass function, which assigns probabilities to specific values for discrete random variables, the probability density function (PDF) is associated with continuous random variables. Similar to the rules that the probability mass function follows, a valid probability density function must satisfy two specific rules: it must be greater than or equal to zero everywhere, and the total area under the function must be equal to one.</p>
<p>The key concept of a probability density function is that areas under the curve correspond to probabilities for the random variable. For instance, if we state that intelligence quotients (IQ) are normally distributed with a mean of 100 and a standard deviation of 15, we are implying that the population follows a bell-shaped curve. In this case, the probability that a randomly selected individual from that population has an IQ between 100 and 115 is represented by the area under the curve within that range.</p>
<p>It is important to note that the probability density function represents a statement about the population of IQs and not the data itself. The data will be used to assess and evaluate the assumptions made about the population’s probability distribution. It is worth emphasizing that whenever the term “probability” is used, it refers to a population quantity.</p>
<p>It is interesting to note that when we model continuous probabilities using probability density functions (PDFs) for continuous random variables, the probability of the variable taking any specific value is actually zero. This is due to the fact that the area under a line, which represents a single point, is zero. However, this does not pose a problem and is simply a quirk arising from modeling random variables with infinite precision. It does not affect the functioning of probability calculations.</p>
<p>The bell-shaped curve, which represents a normal distribution, can be quite challenging to work with until you learn the appropriate techniques, which will be covered in a separate lecture. For now, let’s consider a simpler density function that resembles a right triangle. We’ll use the function f(x) = 2x for x between 0 and 1, and 0 otherwise, as an example. Let’s provide some context for this function: imagine it represents the proportion of help calls that are addressed in a random day by a helpline.</p>
<p>What does this density function imply? It means that the probability of the number of calls being addressed falling between 20% and 60% of the total calls for that day is given by the area under the curve in that range. Now, let’s evaluate whether this function is a mathematically valid probability density function.</p>
<p>Looking at the plot of the PDF, which resembles a right triangle, we can see that it is always greater than or equal to zero. Next, let’s calculate the area under the curve. Since it is a right triangle, the area is equal to half the base (which is 1) multiplied by the height (which is 2). Thus, the area is 1. Therefore, this function satisfies the requirements of a valid probability density function, as it is always non-negative and the total area under the curve is equal to 1.</p>
<p>Let’s walk through an example of working with this density function. We want to find the probability that 75% or fewer calls get addressed in a randomly sampled day from this population. Fortunately, it is convenient that this scenario corresponds to another right triangle that we can calculate.</p>
<p>At the point (0.75, 1.5) on the density function, the height is 1.5 because the function is defined as 2 times x. The base value is 0.75. To calculate the probability, we divide the area, which is half the base times the height, by 2. So the probability turns out to be 56%, as shown in the example.</p>
<p>Interestingly, this density function is a special case of a well-known distribution called the beta distribution. I have provided the R code here for obtaining the probability directly from the beta distribution. Although in this simple case we don’t need it because we are working with triangles, in more complex scenarios, we will require these functions. It’s worth mentioning that the “p” prefix before a function denotes the calculation of probabilities. In this case, pbeta asks for the probability from a beta distribution of being less than 0.75. The parameters 2 and 1 define the specific triangle we are using in this example, and you can see that it yields the same result of 56%. Certain areas of the density are so commonly used that they are given specific names.</p>
<p>For instance, the cumulative distribution function (CDF) of a random variable X gives the probability that X is less than or equal to a given value x. This definition holds for both discrete and continuous random variables. In the case of the beta distribution we just examined, the pbeta function in R always returns the probability of being less than or equal to the first argument provided. Therefore, when using pdensity name in R, it is essentially calculating the cumulative distribution function.</p>
<p>Alternatively, the survival function is another useful concept. It is defined as 1 minus the cumulative distribution function and represents the probability of a random variable being greater than a given value. Suppose we wanted to determine the cumulative distribution function for the previously mentioned density. For instance, we might want to find the probability that 40% or fewer, 50% or fewer, or 60% or fewer of the calls get answered in a given day based on this specific right triangle population density function. In each case, the calculation will resemble what we did earlier for 0.75. Since the density function is a right triangle, the probability is half the area of the base times the height. This simplifies to one-half times x times 2x, which equals x squared. Therefore, the function x squared provides the probability of that percentage or fewer calls being answered on a randomly sampled day.</p>
<p>Alright. Let’s examine the results when we use the pbeta function, which corresponds to the cumulative distribution function in R, for the three values mentioned earlier. The parameters 2 and 1 are utilized to evaluate the specific beta density, yielding probabilities of 16%, 25%, and 36%. Therefore, the probability that 40% or fewer of the calls get answered on a given day is 16%, the probability that 50% or fewer get answered is 25%, and the probability that 60% or fewer get answered is 36%. In terms of the survival function, it is simply 1 minus the cumulative distribution function, which can be expressed as 1 minus x squared.</p>
<p>As we progress, we will encounter more complex density functions. However, the process will be simpler since we can rely on existing functions such as pnorm and pbeta instead of calculating them directly.</p>
<p>You’re already familiar with sample quantiles, such as the 95th percentile, which represents the 0.95 quantile of a dataset. If you score at the 95th percentile on an exam, it means that 95% of the students scored worse than you while 5% scored better. Now, let’s introduce the concept of population analogs for quantiles. In the case of the 95th percentile or the 0.95 quantile, you would order the observations from least to greatest and locate the point or exam score below which 95% of the observations lie. This point is denoted as x-sub-alpha, where alpha corresponds to the quantile. In other words, it satisfies the condition F(x-alpha) = alpha, where F is the distribution function. To better understand this concept, let’s try to visualize it.</p>
<p>Let’s consider the distribution function F(x), which represents the area below point x on a density plot. This area corresponds to the probability that a random variable from the population is less than or equal to x. To illustrate this concept, let’s imagine a population of test scores, an infinite population of students. The distribution function gives us the probability of obtaining a score equal to or lower than x for a randomly selected student from this population.</p>
<p>Now, let’s introduce the concept of the alphath quantile. We move a line along the distribution until we find the point x-sub-alpha, where exactly alpha proportion of the probability lies below it. This is similar to what we do with our data when finding an empirical quantile, where we locate the data point such that, for example, 95% of the test scores lie below it, which corresponds to the sample 95th percentile. In the population distribution, we move the x point until we find the point where the probability of being below it is 95%. Percentiles are essentially quantiles with alpha expressed as a percentage rather than a proportion. The median, often the most well-known quantile, represents the 50th percentile.</p>
<p>Quantiles are frequently used, particularly with the normal distribution. However, we rarely need to directly work with densities to calculate quantiles, as the distributions we commonly encounter have well-defined quantiles. In R, we can easily find quantiles using the “q” prefix before the density function name. For example, for the beta density we discussed earlier, the function “qbeta” gives us the relevant quantile. We can input 0.5 to find the median, considering that R expects the quantile argument as a proportion rather than a percentage. The parameters 2 and 1 are specific to the density we’re working with, which you’ll have to trust me on for now. When we calculate the quantile using “qbeta” with 0.5 as the argument, we obtain the same result as before, 0.7 or 0.71.</p>
<p>At this point, you might be wondering why the concept of the median seems simpler when ordering observations and selecting the middle value or averaging the two middle values for an even number of observations.</p>
<p>In the previous example, we discussed the concept of a sample quantile, which serves as an estimator. However, in this class, we aim to go beyond just estimators and focus on the targets of estimation, known as estimands. In the case of the sample median, it estimates the population median.</p>
<p>To understand this, let’s consider an example where we sample a few days and calculate the percentage of calls answered on those days. If we line up these percentages in ascending order, the middle value represents the sample median. We can think of this sample median as an estimator for the true median percentage of calls answered in the population. However, to establish a connection between the sample and the population, we need to make certain assumptions, which we will thoroughly explore and formalize in this class.</p>
<p>In essence, for every estimator, there exists an estimand in this class. The sample mean estimates the population mean, the sample median estimates the population median, and the sample standard deviation estimates the population standard deviation, and so on. This process is known as statistical inference, where we link our sample data to the underlying population through appropriate estimators and estimands.</p>
</div>
</div>
<div id="conditional-probability" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Conditional probability</h2>
<p>Conditional probability is a very intuitive idea, “What is the probability given partial information about what has occurred?”. The probability of getting hit by lightning is small. However, it’s much larger for people playing outside in open fields <a href="https://xkcd.com/795/">during a lightning storm</a>! In these lectures we go over the formal rules of conditional probability.</p>
<p>Welcome to the conditional probability lecture, part of the statistical inference class in the Coursera data design specialization. To illustrate the concept of conditioning, let’s take a look at an XKCD comic. The comic portrays two individuals standing in a field during a lightning storm near a tree. One person suggests going inside, but the other dismisses the idea, citing the low chance of getting struck by lightning, approximately one in seven million. However, the comic humorously points out that the death rate among people who know this statistic is one in six. The underlying message is that the second person has failed to consider the additional information available to them, leading to an incorrect assessment of risk.</p>
<p>Let’s explore another example to better understand conditional probabilities. Suppose we have a standard die, and the probability of rolling a one is assumed to be one-sixth. However, if we are given the extra information that the roll resulted in an odd number (one, three, or five), our perspective changes. Now, conditioned on this new information, we would no longer say that the probability of rolling a one is one-sixth. Instead, we would consider the one, three, and five to be equally likely outcomes, so the probability of rolling a one becomes one-third. This demonstrates how conditional probabilities adjust our understanding based on additional information.</p>
<p>Formally, let’s define conditional probability. Suppose we have an event B with a nonzero probability. Then, the conditional probability of event A given that B has occurred is denoted as P(A | B) and is defined as the probability of the intersection of A and B divided by the probability of B. In the case where A and B are statistically independent events (we will define this later), the conditional probability simplifies to the probability of A. This means that if the occurrence of event B provides no new information about event A, the probability of A remains unchanged.</p>
<p>Let’s verify that the concept of conditional probability aligns with our intuition in the example of rolling a die. In this case, event B represents the occurrence of an odd number (one, three, or five), and event A represents rolling a one. We want to find the probability of A given that B has occurred. In other words, we are interested in the probability of rolling a one when we know that the outcome is an odd number. Using the definition of conditional probability, P(A | B) = P(A ∩ B) / P(B). Since A is entirely contained within B, the probability of A ∩ B is simply the probability of A, which is one-sixth. The probability of B, in this case, is three-sixths (one-sixth for each of the three mutually exclusive possibilities). Thus, the conditional probability P(A | B) equals one-sixth divided by three-sixths, which simplifies to one-third, confirming our previous understanding.</p>
<p>Conditional probability allows us to update our probabilities based on new information, and it plays a crucial role in statistical inference.
### Bayes’ rule
One of the well-known applications of conditional probability is Bayes’ rule, named after Thomas Bayes, a Presbyterian minister whose work was published posthumously. Bayes’ rule allows us to reverse the conditioning set and the set we are interested in finding the probability of. Suppose we want to calculate the probability of event B given event A, and we already know or can easily calculate the probability of event A given event B. Bayes’ rule enables us to evaluate the probability of B given A in terms of the probability of A given B. However, to apply Bayes’ rule, we also need the marginal probability of event B, which is valuable in various contexts such as diagnostic tests.</p>
<p>Let’s discuss conditional probability in the context of a diagnostic test, which exemplifies one of the significant applications of conditional probability and Bayes’ rule. Consider a test for a disease, where we define plus and minus as events representing a positive or negative test result, respectively. D and D complement represent the events of having or not having the disease, respectively. The sensitivity of the test is the probability that the test is positive given that the subject actually has the disease. A high sensitivity indicates a good test. The specificity, on the other hand, is the probability that the test is negative given that the subject does not have the disease. A high specificity is desirable for a good test. While obtaining accurate estimates of sensitivity and specificity can be challenging, in certain cases, like an HIV blood test, it is possible to test individuals known to have or not have the disease to estimate these probabilities.</p>
<p>When a diagnostic test is positive, the probability of having the disease given the positive test result (positive predictive value) is of particular interest. Similarly, when the test is negative, the probability of not having the disease given the negative test result (negative predictive value) becomes relevant. In the absence of a test, the probability of having the disease is known as the prevalence of the disease.</p>
<p>Let’s work through an example to illustrate the calculation of positive predictive value using Bayes’ rule. Suppose a study comparing the efficacy of an HIV test reports sensitivity as 99.7% and specificity as 98.5%. These numbers are for illustrative purposes and do not reflect actual HIV test statistics. Now, consider a subject from a population with a 0.1% prevalence of HIV who receives a positive test result. We want to calculate the associated positive predictive value.</p>
<p>Applying Bayes’ rule, we have the probability of disease given a positive test result (P(D|+)) equal to the probability of a positive test result given disease (P(+|D)) multiplied by the probability of disease (P(D)), divided by the denominator. To simplify, we express the probability of a positive test result given no disease as 1 minus the specificity and the probability of no disease as 1 minus the prevalence. Substituting known values, we find the positive predictive value to be 6% for this test in the given population. The low positive predictive value is primarily due to the low prevalence of the disease.</p>
<p>However, in a counseling scenario, if the counselor discovers that the subject is an intravenous drug user who regularly has intercourse with an HIV-infected partner, the counselor would consider a much higher prevalence for this particular individual, leading to a higher positive predictive value.</p>
<p>Bayes’ rule provides a powerful framework for incorporating new information and adjusting probabilities based on conditional events, making it valuable in various fields, including diagnostics and decision-making.</p>
<p>Now, let’s distinguish between two components: the prevalence-dependent aspect and the objective evidence reflected in the positive test result. This is where diagnostic likelihood ratios come into play, and we’ll explore them further. First, let’s revisit the formula for positive predictive value in Bayes’ rule, which depends on sensitivity, specificity, and disease prevalence. We can apply a similar approach to calculate the probability of not having the disease given a positive test result. By dividing these two equations, we arrive at the odds of disease given a positive test result divided by the odds of not having the disease given a positive test result. Dividing a probability by 1 minus that probability gives us the odds. Therefore, on the left side, we have the odds of disease given a positive test result, while on the right side, we have the odds of disease without the test result. The factor in the middle represents the diagnostic likelihood ratio for a positive test result.</p>
<p>The equation can be expressed as follows: the pretest odds of disease multiplied by the diagnostic likelihood ratio equals the post-test odds of disease. In other words, the diagnostic likelihood ratio of a positive test result indicates how much the odds change when multiplied by it, transitioning from pretest to post-test odds.</p>
<p>Returning to our example, let’s assume a subject has a positive HIV test. Using the sensitivity and specificity values mentioned earlier, the diagnostic likelihood ratio is calculated as 0.997 divided by 1 minus 0.985, resulting in 66. Regardless of the pretest odds, multiplying them by 66 gives the post-test odds. Thus, the hypothesis of disease is 66 times more supported by the data compared to the hypothesis of no disease. Even if the pretest odds are initially small, multiplying them by 66 will still yield a larger but still small number.</p>
<p>Now, let’s briefly consider the scenario when a subject receives a negative test result using the DLR minus. In this case, the DLR minus, derived from the sensitivity and specificity values mentioned earlier, is 0.003. Consequently, the post-test odds of disease in light of a negative test result become 0.3% of the pretest odds of disease. Stated differently, the hypothesis of disease is supported 0.003 times the hypothesis of no disease given the negative test result.</p>
<p>By incorporating diagnostic likelihood ratios, we can assess the impact of a test result on the odds of disease and gain insights into the strength of evidence provided by the test.</p>
<div id="independence" class="section level3" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Independence</h3>
<p>Let’s briefly discuss the concept of independence. As mentioned earlier, event A is considered independent of event B if the probability of A given B is equal to the probability of A, given that event B has a positive probability. Another definition of independence states that events A and B are independent if the probability of their intersection (A intersect B) equals the product of their individual probabilities. This leads us to an important lesson: we cannot simply multiply probabilities without considering the independence of the events involved. Multiplication of probabilities is valid only for independent events.</p>
<p>To illustrate this, let’s consider a numerical example. What is the probability of getting two consecutive heads when flipping a fair coin? We define event A as the probability of getting a head on the first flip and event B as the probability of getting a head on the second flip. Both probabilities are 0.5 since we assume a fair coin. In this case, because the events are independent, the probability of A intersect B (getting heads on both flips) is the product of their probabilities, which is 0.25. This calculation is straightforward and correct.</p>
<p>However, problems arise when people multiply probabilities in situations where they shouldn’t. A notable example of incorrectly multiplying probabilities was reported in volume 309 of Science. It involved a physician who gave expert testimony in a criminal trial. The trial concerned a mother whose two children had died from sudden infant death syndrome (SIDS). The expert testimony multiplied the prevalence of SIDS (1 out of 8,500) by itself to calculate the probability of two children from the same mother having SIDS. Based on this evidence, among other factors, the mother was convicted of murder. The fundamental mistake in this case was multiplying probabilities for events that were not necessarily independent. It is reasonable to assume that events within families, such as the occurrence of SIDS, are dependent due to genetic or familial environmental factors.</p>
<p>In our class, we will primarily use the concept of independence by assuming that a collection of random variables are independent and identically distributed (IID). This means that the random variables are independent from each other and follow the same probability distribution. For example, several coin flips can be considered IID because each flip is independent of the others, and they all follow the same distribution with a 0.5 probability for heads and 0.5 for tails. IID sampling serves as our default model for a random sample. Even if we do not have an actual random sample, we often use the conceptual model of random sampling or IID to analyze our data. It will be the principal mode of analysis in this class.</p>
</div>
</div>
<div id="expected-values" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Expected values</h2>
<p>The empirical average is a very intuitive idea; it’s the middle of our data in a sense. But, what is it estimating? We can formally define the middle of a population distribution. This is the expected value. Expected values are very useful for characterizing populations and usually represent the first thing that we’re interested in estimating.</p>
<p>Now, let’s discuss the process of drawing conclusions about populations based on noisy data obtained from them. We will assume that the populations and the randomness governing our samples are described by probability density functions and probability mass functions. Instead of focusing on the entire function, we will examine characteristics of these distributions that are reflected in the random variables drawn from them. The most valuable such characteristics are expected values, particularly the mean. The mean represents the center of a distribution. As the mean shifts, the distribution moves either to the left or right.</p>
<p>Another important characteristic is variance, which measures the spread of a distribution. Similar to how sample quantiles estimate population quantiles, sample expected values estimate population expected values. Therefore, the sample mean serves as an estimate of the population mean, the sample variance estimates the population variance, and the sample standard deviation approximates the population standard deviation.</p>
<p>The expected value, or mean, of a random variable represents the center of its distribution. For a discrete random variable x with a probability mass function p(x), the expected value is calculated by summing the possible values that x can take multiplied by their respective probabilities. Conceptually, the expected value draws inspiration from the idea of the physical center of mass, where the probabilities act as weights and x represents the location along an axis.</p>
<p>To illustrate this notion of center of mass, let’s consider the sample mean. Even though we are focusing on the population mean in this discussion, it is interesting to note that the sample mean can be seen as the center of mass if we treat each data point as equally likely. In other words, each data point xi is assigned a probability of 1/N, where N is the sample size. Intuitively, we employ this center of mass idea when using the sample mean.</p>
<p>To demonstrate this concept, I have provided some code that calculates the sample mean of a dataset and depicts it as the center of mass by generating a histogram. The example employs a dataset called “Galton,” which consists of paired data representing the heights of parents and their children. The histogram displays the child’s height distribution, and a continuous density estimate is superimposed. To further explore this concept, we can use the “manipulate” function available in RStudio. By manipulating the mean value, we can observe how it balances out the histogram. The mean squared error is a measure of imbalance, indicating how stable or unsteady the histogram appears. As we move the mean closer to the center of the distribution, the mean value increases, while the mean squared error decreases, signifying a better balance. However, if we move the mean too far from the center, the mean squared error increases again, indicating increased imbalance. This demonstration illustrates that the empirical mean serves as the balancing point for the empirical distribution, and we will utilize this concept when discussing the population mean, which serves as the balancing point for the population distribution.</p>
<p>Now, let’s consider an example to understand how to obtain the expected value of a population. Suppose we flip a fair coin, and we assign the value 0 to tails and the value 1 to heads. What is the expected value of X? Again, the expected value represents a property of the population. By plugging the values into our formula, we calculate the expected value of X as follows: The probability of obtaining tails (0) is 0.5 multiplied by the value 0, plus the probability of obtaining heads (1) is also 0.5 multiplied by the value 1. When we compute this expression, we find that the expected value of X is 0.5. It’s interesting to note that the expected value is a value that the coin itself cannot actually take.</p>
<p>However, from a geometric perspective, the answer becomes quite obvious. If we visualize the coin’s values as two bars of equal height, one at 0 and the other at 1, we can easily determine the balancing point by placing our finger exactly at 0.5.</p>
<p>Now, let’s consider a scenario where a random variable X represents the outcome of a biased coin flip. The probability of obtaining heads is denoted as p, while the probability of obtaining tails is 1 minus p. What is the expected value of X in this case? By directly applying the formula, we multiply the value 0 by the probability 1 minus p and add it to the value 1 multiplied by the probability p. The result simplifies to p. Therefore, the expected value of a coin flip, even when the coin is biased, corresponds to the true long-run proportion of obtaining heads in an infinite number of coin flips.</p>
<p>Now, let’s move on to a die. Suppose we roll a fair six-sided die, and X represents the number that appears face up. What is the expected value of X? Here, we take the values 1, 2, 3, 4, 5, and 6 and multiply each by the corresponding probability of the random variable X taking those values (each value has a probability of one-sixth). When we perform this calculation, we find that the expected value of X is 3.5. Once again, this is a value that the die itself cannot actually show.</p>
<p>Similar to the coin example, the geometric argument makes it evident. We have six bars, each with a height of one-sixth, representing the possible outcomes of the die. If we were to balance them, it becomes clear that the balancing point would be at 3.5.
### Expected values for PDFs
When dealing with continuous random variables, it can be helpful to imagine cutting out the probability density from, let’s say, a piece of wood and determining where you would place your finger to balance it out. This concept aligns with the notion of the center of mass of a continuous body. In the case of probability mass functions, as the bars representing the probabilities become narrower and smaller, we can visualize their balancing point. To illustrate this, let’s consider an example.</p>
<p>Suppose we have a density that ranges from zero to one, and the question arises: Is this a valid density? The answer is yes; it corresponds to a well-known density called the Uniform density. Now, what is its expected value? If we were to cut this density out of a piece of wood and balance it, the position where we would place our finger to achieve balance is precisely at 0.5. This aligns perfectly with the expected value of the uniform density. Now, let’s delve into the topic of expected values and touch upon some important facts.</p>
<p>First, it’s crucial to understand that expected values represent properties of the distribution. They serve as the center of mass of a distribution. Additionally, it’s important to note that the average of random variables is, in itself, a random variable. For example, if we roll six dice and calculate their average, the resulting value is a random variable. By repeatedly sampling from this average through multiple dice rolls, we generate a distribution that also possesses an expected value. The center of mass of this distribution coincides with the center of mass of the original distribution.</p>
<p>This topic becomes highly relevant to the field of inference, so let’s explore some simulation examples to gain a better understanding. In the first example, the blue density represents the outcome of numerous simulations based on a standard normal distribution. Due to the large number of simulations, this density provides a reliable approximation of the true distribution. It shows that collecting ample data from a population allows us to approximate its originating distribution effectively. The center of mass of this distribution, which would achieve balance, is located at zero.</p>
<p>Now, let’s shift our focus to simulating the average of ten standard normals. By repeatedly performing this process and plotting the resulting histogram or density estimate, we obtain a different distribution. It no longer represents the distribution of standard normals; rather, it illustrates the distribution of averages of ten standard normals. This new distribution, represented by the salmon-colored plot, exhibits interesting properties. Notably, it is concentrated around zero, and this aligns with our previous point. The distribution of averages from a population tends to be centered at the same location as the distribution of the original population itself.</p>
<p>Although calculations and simulations can help us grasp these concepts conceptually, we can observe this phenomenon without explicitly performing them. Let’s explore additional examples to solidify our understanding. Imagine rolling a die thousands of times and plotting a histogram of the results. In this case, approximately one-sixth of the rolls would occur for each number from one to six. As we increase the number of rolls, these bars would eventually balance out. The center of mass for this distribution, which would achieve balance, is 3.5 (not exactly, given the finite number of rolls, but in theory, it would converge to 3.5 with an infinite number of rolls).</p>
<p>Now, let’s consider the scenario where we roll the die twice and calculate the average of the numbers obtained. If we repeat this process multiple times and create a distribution of these averages, we see a different pattern in the second panel. It appears more Gaussian in shape (we’ll discuss this further later), and importantly, it is centered at the same location as before.</p>
<p>The population mean of averages of two die rolls is identical to the population mean of individual die rolls. This concept applies to other scenarios as well. For instance, if we were to flip a coin numerous times, we would expect approximately 50% of the outcomes to be zero (tails) and 50% to be one (heads). These proportions would converge to balance at around 0.5. When flipping the coin only a few times, the observed sample proportion may deviate from 0.5. However, as we increase the number of flips, the simulation variability becomes insignificant, and the proportion approaches 0.5.</p>
<p>Now, let’s consider the scenario where we flip the coin ten times, calculate the average, and repeat this process multiple times. This simulation provides insights into the distribution of averages of ten coin flips. We can extend this analysis to averages of 20 coin flips and averages of 30 coin flips. In each case, we observe that as the average incorporates more coin flips, the distribution becomes more concentrated around the mean. Nevertheless, regardless of the number of coin flips involved, the distribution of averages is consistently centered at 0.5.</p>
<p>To summarize the key points covered thus far:</p>
<ul>
<li>Expected values are inherent properties of distributions. The population mean represents the center of mass of that population, and any movement in the mean would correspondingly shift the distribution.</li>
<li>The sample mean represents the center of mass of the observed data. It serves as an estimate of the population mean and is considered unbiased.</li>
<li>The population mean of the distribution of sample means precisely matches the population mean it aims to estimate. This understanding is vital as it allows us to estimate the population distribution accurately when collecting substantial amounts of data.</li>
<li>We must recognize that while we obtain only one sample mean from our data, knowing the properties associated with sample means is immensely valuable.</li>
<li>As more data contributes to the sample mean, the density mass function becomes more concentrated around the population mean. We also observe that, even in cases such as coin flipping and dice rolling, the distribution tends to exhibit Gaussian-like characteristics. We’ll explore these concepts further in subsequent lectures.
## Practical R Exercises in swirl</li>
</ul>

</div>
</div>
<hr>
<center> 
  <div class="footer">
      All illustrations <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY. </a>
      <br>
      All other materials <a href= "https://creativecommons.org/licenses/by/4.0/"> CC-BY </a> unless noted otherwise.
  </div>
</center>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="variability-distribution-asymptotics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

</body>

</html>
