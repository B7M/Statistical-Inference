<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Variability, Distribution, &amp; Asymptotics | Statistical Inference</title>
  <meta name="description" content="Description about Course/Book." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Variability, Distribution, &amp; Asymptotics | Statistical Inference" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Description about Course/Book." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Variability, Distribution, &amp; Asymptotics | Statistical Inference" />
  
  <meta name="twitter:description" content="Description about Course/Book." />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="assets/dasl_favicon.ico" type="image/x-icon" />
<link rel="prev" href="probability-expected-values.html"/>
<link rel="next" href="intervals-testing-pvalues.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<a href="http://jhudatascience.org/"><img src="https://jhudatascience.org/images/dasl.png" style=" width: 80%; padding-left: 40px; padding-top: 8px; vertical-align: top "</a>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this Course</a></li>
<li class="chapter" data-level="1" data-path="probability-expected-values.html"><a href="probability-expected-values.html"><i class="fa fa-check"></i><b>1</b> Probability &amp; Expected Values</a>
<ul>
<li class="chapter" data-level="1.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#course-book"><i class="fa fa-check"></i><b>1.1.1</b> Course Book:</a></li>
<li class="chapter" data-level="1.1.2" data-path="probability-expected-values.html"><a href="probability-expected-values.html#github-repository"><i class="fa fa-check"></i><b>1.1.2</b> Github repository</a></li>
<li class="chapter" data-level="1.1.3" data-path="probability-expected-values.html"><a href="probability-expected-values.html#homework-problems"><i class="fa fa-check"></i><b>1.1.3</b> Homework Problems</a></li>
<li class="chapter" data-level="1.1.4" data-path="probability-expected-values.html"><a href="probability-expected-values.html#differences-of-opinion"><i class="fa fa-check"></i><b>1.1.4</b> Differences of opinion</a></li>
<li class="chapter" data-level="1.1.5" data-path="probability-expected-values.html"><a href="probability-expected-values.html#data-science-specialization-community-site"><i class="fa fa-check"></i><b>1.1.5</b> Data Science Specialization Community Site</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="probability-expected-values.html"><a href="probability-expected-values.html#probability"><i class="fa fa-check"></i><b>1.2</b> Probability</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#probability-mass-functions-and-probability-density-functions"><i class="fa fa-check"></i><b>1.2.1</b> Probability mass functions and probability density functions</a></li>
<li class="chapter" data-level="1.2.2" data-path="probability-expected-values.html"><a href="probability-expected-values.html#quantiles"><i class="fa fa-check"></i><b>1.2.2</b> Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="probability-expected-values.html"><a href="probability-expected-values.html#conditional-probability"><i class="fa fa-check"></i><b>1.3</b> Conditional probability</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="probability-expected-values.html"><a href="probability-expected-values.html#independence"><i class="fa fa-check"></i><b>1.3.1</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="probability-expected-values.html"><a href="probability-expected-values.html#expected-values"><i class="fa fa-check"></i><b>1.4</b> Expected values</a></li>
<li class="chapter" data-level="1.5" data-path="probability-expected-values.html"><a href="probability-expected-values.html#practical-r-exercises-in-swirl"><i class="fa fa-check"></i><b>1.5</b> Practical R Exercises in swirl</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html"><i class="fa fa-check"></i><b>2</b> Variability, Distribution, &amp; Asymptotics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#variability"><i class="fa fa-check"></i><b>2.1</b> Variability</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#variance-simulation-examples"><i class="fa fa-check"></i><b>2.1.1</b> Variance simulation examples</a></li>
<li class="chapter" data-level="2.1.2" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#normal-distribution"><i class="fa fa-check"></i><b>2.1.2</b> Normal distribution</a></li>
<li class="chapter" data-level="2.1.3" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#poisson-distribution"><i class="fa fa-check"></i><b>2.1.3</b> Poisson distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#asymptotics"><i class="fa fa-check"></i><b>2.2</b> Asymptotics</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#asymptotics-and-the-clt"><i class="fa fa-check"></i><b>2.2.1</b> Asymptotics and the CLT</a></li>
<li class="chapter" data-level="2.2.2" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#asymptotics-and-confidence-intervals"><i class="fa fa-check"></i><b>2.2.2</b> Asymptotics and confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="variability-distribution-asymptotics.html"><a href="variability-distribution-asymptotics.html#practical-r-exercises-in-swirl-1"><i class="fa fa-check"></i><b>2.3</b> Practical R Exercises in swirl</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html"><i class="fa fa-check"></i><b>3</b> Intervals, Testing, &amp; Pvalues</a>
<ul>
<li class="chapter" data-level="3.1" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#confidence-intervals"><i class="fa fa-check"></i><b>3.1</b> Confidence intervals</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#independent-group-t-intervals"><i class="fa fa-check"></i><b>3.1.1</b> Independent group T intervals</a></li>
<li class="chapter" data-level="3.1.2" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#a-note-on-unequal-variance"><i class="fa fa-check"></i><b>3.1.2</b> A note on unequal variance</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.2</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#t-tests"><i class="fa fa-check"></i><b>3.2.1</b> T tests</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#p-values"><i class="fa fa-check"></i><b>3.3</b> P values</a></li>
<li class="chapter" data-level="3.4" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#knitr"><i class="fa fa-check"></i><b>3.4</b> Knitr</a></li>
<li class="chapter" data-level="3.5" data-path="intervals-testing-pvalues.html"><a href="intervals-testing-pvalues.html#practical-r-exercises-in-swirl-2"><i class="fa fa-check"></i><b>3.5</b> Practical R Exercises in swirl</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html"><i class="fa fa-check"></i><b>4</b> Power, Bootstrapping, &amp; Permutation Tests</a>
<ul>
<li class="chapter" data-level="4.1" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#power"><i class="fa fa-check"></i><b>4.1</b> Power</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#calculating-power"><i class="fa fa-check"></i><b>4.1.1</b> Calculating Power</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#multiple-comparisons"><i class="fa fa-check"></i><b>4.2</b> Multiple Comparisons</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#notes-on-the-bootstrap"><i class="fa fa-check"></i><b>4.2.1</b> Notes on the bootstrap</a></li>
<li class="chapter" data-level="4.2.2" data-path="power-bootstrapping-permutation-tests.html"><a href="power-bootstrapping-permutation-tests.html#permutation-tests"><i class="fa fa-check"></i><b>4.2.2</b> Permutation tests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="5" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>5</b> References</a></li>
<li class="divider"></li>
<p style="text-align:center;"> <a href="https://github.com/jhudsl/OTTR_Template" target="blank" > This content was published with</a> <a href="https://bookdown.org/" target="blank"> bookdown by:</a> </p>
<p style="text-align:center;"> <a href="http://jhudatascience.org/"> The Johns Hopkins Data Science Lab </a></p>
<p style="text-align:center; font-size: 12px;"> <a href="https://github.com/rstudio4edu/rstudio4edu-book/"> Style adapted from: rstudio4edu-book </a> <a href ="https://creativecommons.org/licenses/by/2.0/"> (CC-BY 2.0) </a></p>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<head>
  <meta name="viewport" content="width=device-width,minimum-scale=1.0,maximum-scale=1.0,initial-scale=1.0">
  <!--script src="https://kit.fontawesome.com/6a26f47516.js"></script-->
  <!--<script src="assets/hideOutput.js"></script>-->
  <link href="assets/style.css" rel="stylesheet">
</head>



<div class="hero-image-container">
  <img class= "hero-image" src= "https://github.com/jhudsl/OTTR_Template/raw/main/assets/dasl_thin_main_image.png">
</div>
<div id="variability-distribution-asymptotics" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Variability, Distribution, &amp; Asymptotics</h1>
<div id="variability" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Variability</h2>
<p>An important characterization of a population is how spread out it is. One of the key measures of spread is variability. We measure population variability with the sample variance, or more often we consider the square root of it, called the standard deviation. The reason for taking the standard deviation is because that measure has the same units as the population. So if our population is a length measurement in meters, the standard deviation is in meters (whereas the variance is in meters squared). Variability has many important uses in statistics. First, the population variance is itself an intrinsically interesting quantity that we want to estimate. Secondly, variability in our estimates is what makes them not imprecise. An important aspect of statistics is quantifying the variability in our estimates.</p>
In the previous lecture, we discussed the population mean as a measure of the center of a distribution. Now, let’s explore another important property called variance, which describes the spread or concentration of the density around the mean. If we imagine a bell curve, the probability density function will shift to the left or right as the mean changes. Variance quantifies how widely or narrowly the density is distributed around the mean.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="week_02_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_19.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.1: Variance of a distribution
</p>
</div>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="variability-distribution-asymptotics.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb8-2"><a href="variability-distribution-asymptotics.html#cb8-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="at">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb8-3"><a href="variability-distribution-asymptotics.html#cb8-3" aria-hidden="true" tabindex="-1"></a>mean <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb8-4"><a href="variability-distribution-asymptotics.html#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="variability-distribution-asymptotics.html#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard deviations</span></span>
<span id="cb8-6"><a href="variability-distribution-asymptotics.html#cb8-6" aria-hidden="true" tabindex="-1"></a>sds <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="fl">1.5</span>, <span class="dv">2</span>)</span>
<span id="cb8-7"><a href="variability-distribution-asymptotics.html#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="variability-distribution-asymptotics.html#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the plot</span></span>
<span id="cb8-9"><a href="variability-distribution-asymptotics.html#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">dnorm</span>(x, <span class="at">mean =</span> mean, <span class="at">sd =</span> sds[<span class="dv">1</span>]), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>, <span class="at">xlab =</span> <span class="st">&quot;x&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>,<span class="at">yaxt=</span><span class="st">&quot;n&quot;</span>)</span>
<span id="cb8-10"><a href="variability-distribution-asymptotics.html#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="variability-distribution-asymptotics.html#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Add lines for the other distributions with different colors</span></span>
<span id="cb8-12"><a href="variability-distribution-asymptotics.html#cb8-12" aria-hidden="true" tabindex="-1"></a>colors <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;orange&quot;</span>, <span class="st">&quot;purple&quot;</span>)</span>
<span id="cb8-13"><a href="variability-distribution-asymptotics.html#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(sds)) {</span>
<span id="cb8-14"><a href="variability-distribution-asymptotics.html#cb8-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lines</span>(x, <span class="fu">dnorm</span>(x, <span class="at">mean =</span> mean, <span class="at">sd =</span> sds[i]), <span class="at">col =</span> colors[i], <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb8-15"><a href="variability-distribution-asymptotics.html#cb8-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-16"><a href="variability-distribution-asymptotics.html#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="variability-distribution-asymptotics.html#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Add legend</span></span>
<span id="cb8-18"><a href="variability-distribution-asymptotics.html#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">legend =</span> <span class="fu">paste</span>(<span class="st">&quot;sd=&quot;</span>, sds), <span class="at">col =</span> colors, <span class="at">lwd =</span> <span class="dv">2</span>,<span class="at">box.lwd =</span> <span class="dv">0</span>, <span class="at">box.col =</span> <span class="st">&quot;white&quot;</span>)</span></code></pre></div>
<p>The blue-colored density represents a standard normal distribution with a standard deviation of 0.5. As the standard deviation increases, the density becomes flatter and spreads more into the tails. Consequently, if a variable is from a normal distribution with a standard deviation of 1.5, they are more likely to have a value beyond 2 compared to a variable from a normal distribution with a standard deviation of 1.</p>
<p>For a random variable X with a mean μ, the variance is precisely the expected squared distance between the random variable and the mean.
<span class="math display">\[ Var(X)=E[(X-\mu)^2]\]</span>
There’s also a useful shortcut:
<span class="math display">\[ Var(X)=E[X^2]-E[X]^2\]</span></p>
<p>Densities with higher variance are more spread out compared to those with lower variances. The square root of variance is known as the standard deviation, which is expressed in the same units as X.</p>
<p>Example: In the previous lecture, we found that the expected value of X, when rolling a die, is 3.5.
<span class="math display">\[ Var(X)=E[X^2]-E[X]^2\]</span>
To calculate the expected value of X squared, we square each number (1, 2, 3, 4, 5) and multiply them by their associated probabilities. Summing these values gives us 15.17.
<span class="math inline">\(Var(X)=15.17 - 3.5^2= 2.92\)</span></p>
<p>Example: Consider tossing a coin with a probability of heads, <span class="math inline">\(p\)</span>. From the previous lecture, we know that the expected value of a coin toss is <span class="math inline">\(p\)</span>. When calculating the expected value of X squared, 0 squared is 0, and 1 squared is 1. Thus, the expected value of X squared is <span class="math inline">\(p\)</span>. Plugging these values into our formula, we get <span class="math inline">\(Var(X)=p - p^2\)</span>, which simplifies to <span class="math inline">\(p(1 - p)\)</span>. This formula is widely recognized and we suggest you memorize it.</p>
<p>Similar to the relationship between population mean and sample mean, the population variance and sample variance are directly analogous. The population mean represents the center of mass of the population, while the sample mean represents the center of mass of the observed data. Similarly, the population variance quantifies the expected squared distance of a random variable from the population mean, while the sample variance measures the average squared distance of the observed data points from the sample mean.
<span class="math display">\[ S^2=\frac{\Sigma_{i=1}(X_i-\bar X)^2}{n-1}\]</span>
Note that in the denominator of the sample variance formula, we divide by <span class="math inline">\(n- 1\)</span> instead of <span class="math inline">\(n\)</span>.</p>
<p>Recall that the sample variance is a function of the data, making it a random variable with its own population distribution. The expected value of this distribution corresponds to the population variance being estimated by the sample variance. As we gather more data, the distribution of the sample variance becomes increasingly concentrated around the population variance it seeks to estimate.</p>
<div id="variance-simulation-examples" class="section level3" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Variance simulation examples</h3>
<p>Suppose we simulate ten standard normal random variables and calculate their sample variance. If we repeat this process many times, we will obtain a distribution of sample variances. This distribution, represented by the salmon-colored density, emerges from repeating the process thousands of times. If we sample enough data points, the center of mass of this distribution will precisely match the variance of the original population we were sampling from—the standard normal distribution with a variance of one.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-3"></span>
<img src="week_02_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_69.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.3: Variance of a distribution of die roll
</p>
</div>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="variability-distribution-asymptotics.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb9-2"><a href="variability-distribution-asymptotics.html#cb9-2" aria-hidden="true" tabindex="-1"></a>nosim <span class="ot">&lt;-</span> <span class="dv">10000</span>; </span>
<span id="cb9-3"><a href="variability-distribution-asymptotics.html#cb9-3" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb9-4"><a href="variability-distribution-asymptotics.html#cb9-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="fu">c</span>(<span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">rnorm</span>(nosim <span class="sc">*</span> <span class="dv">10</span>), nosim), <span class="dv">1</span>, var),</span>
<span id="cb9-5"><a href="variability-distribution-asymptotics.html#cb9-5" aria-hidden="true" tabindex="-1"></a>          <span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">rnorm</span>(nosim <span class="sc">*</span> <span class="dv">20</span>), nosim), <span class="dv">1</span>, var),</span>
<span id="cb9-6"><a href="variability-distribution-asymptotics.html#cb9-6" aria-hidden="true" tabindex="-1"></a>          <span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">rnorm</span>(nosim <span class="sc">*</span> <span class="dv">30</span>), nosim), <span class="dv">1</span>, var)),</span>
<span id="cb9-7"><a href="variability-distribution-asymptotics.html#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">n =</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;10&quot;</span>, <span class="st">&quot;20&quot;</span>, <span class="st">&quot;30&quot;</span>), <span class="fu">c</span>(nosim, nosim, nosim))) </span>
<span id="cb9-8"><a href="variability-distribution-asymptotics.html#cb9-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb9-9"><a href="variability-distribution-asymptotics.html#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(dat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">fill =</span> n)) <span class="sc">+</span> <span class="fu">geom_density</span>(<span class="at">size =</span> <span class="dv">2</span>, <span class="at">alpha =</span> .<span class="dv">2</span>) <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">1</span>, <span class="at">size =</span> <span class="dv">2</span>) </span></code></pre></div>
<p>The same holds true when we consider sample variances based on 20 observations from the standard normal distribution. we repeat the process of sampling 20 standard normals, calculating the sample variance, and obtaining a distribution of sample variances. This distribution, depicted in a more aqua color, is also centered at one. The pattern continues when we examine sample variances based on 30 observations. However, what’s interesting to note is that as the sample size increases, the variance of the population distribution of the sample variances becomes more concentrated. In simpler terms, collecting more data leads to a better and more tightly focused estimate of what the sample variance is trying to estimate. In this case, all the sample variances are estimating a population variance of one because they are sampled from a population with a variance of one.</p>
Before we found that the variance of a die roll was <span class="math inline">\(2.92\)</span>. Now, imagine if we were to roll ten dice and calculate the sample variance of the numbers on the sides facing up. By repeating this process numerous times, we can obtain a reliable understanding of the population distribution of the variance of ten die rolls. Although it requires a large number of repetitions, with the help of a computer, we can simulate this process thousands of times, as demonstrated here.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-5"></span>
<img src="week_02_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_72.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.5: Variance of a distribution of coin toss
</p>
</div>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="variability-distribution-asymptotics.html#cb10-1" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb10-2"><a href="variability-distribution-asymptotics.html#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">c</span>(<span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">sample</span>(<span class="dv">1</span> <span class="sc">:</span> <span class="dv">6</span>, nosim <span class="sc">*</span> <span class="dv">10</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>), </span>
<span id="cb10-3"><a href="variability-distribution-asymptotics.html#cb10-3" aria-hidden="true" tabindex="-1"></a>                     nosim), <span class="dv">1</span>, var),</span>
<span id="cb10-4"><a href="variability-distribution-asymptotics.html#cb10-4" aria-hidden="true" tabindex="-1"></a>        <span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">sample</span>(<span class="dv">1</span> <span class="sc">:</span> <span class="dv">6</span>, nosim <span class="sc">*</span> <span class="dv">20</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>), </span>
<span id="cb10-5"><a href="variability-distribution-asymptotics.html#cb10-5" aria-hidden="true" tabindex="-1"></a>                     nosim), <span class="dv">1</span>, var),</span>
<span id="cb10-6"><a href="variability-distribution-asymptotics.html#cb10-6" aria-hidden="true" tabindex="-1"></a>        <span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">sample</span>(<span class="dv">1</span> <span class="sc">:</span> <span class="dv">6</span>, nosim <span class="sc">*</span> <span class="dv">30</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>), </span>
<span id="cb10-7"><a href="variability-distribution-asymptotics.html#cb10-7" aria-hidden="true" tabindex="-1"></a>                     nosim), <span class="dv">1</span>, var)</span>
<span id="cb10-8"><a href="variability-distribution-asymptotics.html#cb10-8" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb10-9"><a href="variability-distribution-asymptotics.html#cb10-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">size =</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>), <span class="fu">rep</span>(nosim, <span class="dv">3</span>))))</span>
<span id="cb10-10"><a href="variability-distribution-asymptotics.html#cb10-10" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(dat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">fill =</span> size)) <span class="sc">+</span> <span class="fu">geom_histogram</span>(<span class="at">alpha =</span> .<span class="dv">20</span>, <span class="at">binwidth=</span>.<span class="dv">3</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>) </span>
<span id="cb10-11"><a href="variability-distribution-asymptotics.html#cb10-11" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> g <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fl">2.92</span>, <span class="at">size =</span> <span class="dv">2</span>)</span>
<span id="cb10-12"><a href="variability-distribution-asymptotics.html#cb10-12" aria-hidden="true" tabindex="-1"></a>g <span class="sc">+</span> <span class="fu">facet_grid</span>(. <span class="sc">~</span> size)</span></code></pre></div>
<p>Notice that the distribution of the variance of ten die rolls is precisely centered around 2.92, which is the variance of a single die roll. As I increase the number of dice to 20 and 30, the center of the distribution remains the same, but it becomes more concentrated around the true population variance. This indicates that the sample variance provides a good estimate of the population variance. As we collect more data, the distribution of the sample variance becomes increasingly concentrated around the true value it aims to estimate, demonstrating its unbiasedness.</p>
<p>The reason we divide by <span class="math inline">\(n - 1\)</span> instead of <span class="math inline">\(n\)</span> in the denominator of the sample variance formula is to ensure its unbiasedness.
### Standard error of the mean
Now that we have extensively discussed variances and briefly touched upon the distribution of sample variances, let’s revisit the distribution of sample means. It is important to remember that the average of numbers sampled from a population is a random variable with its own population mean and population variance. The population mean remains the same as the original population, while the variance of the sample mean can be related to the variance of the original population. Specifically, the variance of the sample mean decreases to zero as more data is accumulated.
<span class="math display">\[ Var(\bar X)=\frac{\sigma^2}{n}\]</span>
This means that the sample mean becomes more concentrated around the population mean it is trying to estimate, which is a valuable characteristic since we usually only have one sample mean in a given dataset.</p>
<p>Although we do not have multiple repeated sample means to investigate their variability like we do in simulation experiments, we can still estimate the population variance, denoted as <span class="math inline">\(\sigma^2\)</span>, using the available data. With knowledge of <span class="math inline">\(\sigma^2\)</span> and the sample size (denoted as n), we can gather valuable information about the distribution of the sample mean. The square root of the statistic, sigma over square root n, is referred to as the standard error of the mean, denoted as the standard deviation of the distribution of a statistic. The term “standard error” is used to represent the variability of means, while the standard error of a regression coefficient describes the variability in regression coefficients.</p>
<p>In summary, considering a population with a mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, when we draw a random sample from that population and calculate the variance <span class="math inline">\(S^2\)</span>, it serves as an estimate of <span class="math inline">\(\sigma^2\)</span>. Similarly, when we calculate the mean, it estimates <span class="math inline">\(\mu\)</span> (population mean). However, <span class="math inline">\(S^2\)</span> (sample variance) is also a random variable with its own distribution centered around <span class="math inline">\(\sigma^2\)</span>, becoming more concentrated around it as more observations contribute to the squared value. Additionally, the distribution of sample means from that population is centered at <span class="math inline">\(\mu\)</span> and becomes more concentrated around <span class="math inline">\(\mu\)</span> as more observations are included. Moreover, we precisely know the variance of the distribution of sample means, which is <span class="math inline">\(\sigma^2\)</span> divided by n.</p>
<p>Since we lack repeated sample means in a given dataset, we estimate the sample variance of the mean as <span class="math inline">\(S^2\)</span> divided by n and the logical estimate of the standard error as <span class="math inline">\(\frac{S}{\sqrt{n}}\)</span>. The standard error of the mean (or the sample standard error of the mean) is defined as <span class="math inline">\(\frac{S}{\sqrt{n}}\)</span>. The standard deviation (S) is an estimate of the variability of the population, while the standard error (<span class="math inline">\(\frac{S}{\sqrt{n}}\)</span>) represents the variability of averages of random samples of size n from the population.</p>
<p>Example: If we take standard normals (with a variance of one), the standard deviation of means of n standard normals is expected to be one over <span class="math inline">\(\frac{1}{\sqrt{n}}\)</span>. By simulating multiple draws of ten standard normals and calculating their mean, followed by taking the standard deviation of these averages, we should obtain an approximate value of <span class="math inline">\(\frac{1}{\sqrt{n}}\)</span>.</p>
<p>You can explore this using the following code snippet in R:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="variability-distribution-asymptotics.html#cb11-1" aria-hidden="true" tabindex="-1"></a>nosim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb11-2"><a href="variability-distribution-asymptotics.html#cb11-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb11-3"><a href="variability-distribution-asymptotics.html#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(<span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">rnorm</span>(nosim <span class="sc">*</span> n), nosim), <span class="dv">1</span>, mean))</span></code></pre></div>
<pre><code>## [1] 0.3173953</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="variability-distribution-asymptotics.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">/</span> <span class="fu">sqrt</span>(n)</span></code></pre></div>
<pre><code>## [1] 0.3162278</code></pre>
<p>Similar simulations can be performed for standard uniforms (variance of <span class="math inline">\(\frac{1}{12}\)</span>), Poisson(4) distributions (variance of 4), and coin flips (variance of <span class="math inline">\(p*(1-p)\)</span>, assuming p=0.5 then <span class="math inline">\(Var(X)=0.25\)</span>). The results of these simulations should align with the theoretical values predicted by our rule.</p>
<p>Understanding the standard error of the mean is crucial in determining the variability of sample means. Simulation experiments can help illustrate these concepts, especially when investigating the distribution of sample means and estimating their standard error.</p>
Example: Consider the father-son data from UsingR library. We will focus on the height of the sons, with “n” representing the number of observations as usual. If we plot a histogram of the son’s height and overlay it with a continuous density estimate, we observe a distribution that closely resembles a Gaussian curve.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-9"></span>
<img src="week_02_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_75.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 2.1: Histogram of son heights
</p>
</div>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="variability-distribution-asymptotics.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(UsingR); <span class="fu">data</span>(father.son); </span>
<span id="cb15-2"><a href="variability-distribution-asymptotics.html#cb15-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> father.son<span class="sc">$</span>sheight</span>
<span id="cb15-3"><a href="variability-distribution-asymptotics.html#cb15-3" aria-hidden="true" tabindex="-1"></a>n<span class="ot">&lt;-</span><span class="fu">length</span>(x)</span>
<span id="cb15-4"><a href="variability-distribution-asymptotics.html#cb15-4" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data =</span> father.son, <span class="fu">aes</span>(<span class="at">x =</span> sheight)) </span>
<span id="cb15-5"><a href="variability-distribution-asymptotics.html#cb15-5" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> g <span class="sc">+</span> <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..density..), <span class="at">fill =</span> <span class="st">&quot;lightblue&quot;</span>, <span class="at">binwidth=</span><span class="dv">1</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb15-6"><a href="variability-distribution-asymptotics.html#cb15-6" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> g <span class="sc">+</span> <span class="fu">geom_density</span>(<span class="at">size =</span> <span class="dv">2</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb15-7"><a href="variability-distribution-asymptotics.html#cb15-7" aria-hidden="true" tabindex="-1"></a>g</span></code></pre></div>
<p>This density estimate provides an approximation of the population density, given the finite amount of data we have collected. The histogram’s variability, which the sample variance calculates, serves as an estimate of the variability in son’s height from the population this data was drawn from, assuming it was a random sample.</p>
<p>By calculating the variance of x, variance of x divided by n, standard deviation of x, and standard deviation of <span class="math inline">\(\frac{x}{\sqrt{n}}\)</span>, and rounding them to two decimal places, we obtain 7.92 and 2.81 as the variance of x and the standard deviation of x, respectively.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="variability-distribution-asymptotics.html#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(UsingR); <span class="fu">data</span>(father.son); </span>
<span id="cb16-2"><a href="variability-distribution-asymptotics.html#cb16-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> father.son<span class="sc">$</span>sheight</span>
<span id="cb16-3"><a href="variability-distribution-asymptotics.html#cb16-3" aria-hidden="true" tabindex="-1"></a>n<span class="ot">&lt;-</span><span class="fu">length</span>(x)</span>
<span id="cb16-4"><a href="variability-distribution-asymptotics.html#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">c</span>(<span class="fu">var</span>(x), <span class="fu">var</span>(x) <span class="sc">/</span> n, <span class="fu">sd</span>(x), <span class="fu">sd</span>(x) <span class="sc">/</span> <span class="fu">sqrt</span>(n)),<span class="dv">2</span>)</span></code></pre></div>
<p>These numbers represent the variability in son’s heights from the dataset and act as estimates of the population variability of son’s heights if we assume these sons are a random sample from a meaningful population. In this case, we prefer the value 2.81 over 7.92 since 7.92 is expressed in inches squared, while 2.81 is expressed in inches. Working with the actual units is more intuitive.
Moving on to 0.01 and 0.09, these values no longer reflect the variability in children’s heights. Instead, they represent the variability in averages of ten children’s heights. The value 0.09 is particularly meaningful as it represents the standard error or the standard deviation in the distribution of averages of n children’s heights. While it’s an estimate based on the available data, it’s the best estimate we can derive from the dataset.</p>
<p>In this section we covered several complex topics, but at its core, understanding variability is the key to understanding statistics. In fact, grasping the concept of variability might be the most crucial aspect of statistics. Here’s a summary of our findings: the sample variance provides an estimate of the population variance, and the distribution of the sample variance is centered around the value it is estimating, indicating an unbiased estimation. Moreover, as more data is collected, the distribution becomes more concentrated around the estimated value, leading to a better estimate. We have also gained insights into the distribution of sample means. In addition to knowing its center, as discussed in the previous lecture, we now understand that the variance of the sample mean is the population variance divided by n, and its square root, <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span> is known as the standard error. These quantities capture the variability of averages drawn from the population, and surprisingly, even though we only have access to one sample mean in a given dataset, we can make substantial inferences about the distribution of averages from random samples. This knowledge provides us with a solid foundation for various statistical analyses and methodologies.
## Distributions
Some probability distributions are so important that we need to internalize their characteristics. Here we will cover the most important probability distributions.
### Binomial distribution
Perhaps the simplest distribution is known as the Bernoulli distribution, named after Jacob Bernoulli, a renowned mathematician from a distinguished family of mathematicians. If you’re interested, you can explore the Bernoulli family further through their Wikipedia pages. The Bernoulli distribution originates from a coin flip, where a “0” represents tails and a “1” represents heads. We can consider a potentially biased coin with a probability “p” for heads and “1 - p” for tails. The Bernoulli probability mass function is typically denoted as:
<span class="math display">\[P(X=x)=p^x * (1 - p)^(1 - x)\]</span>
As we have seen before, the mean of a Bernoulli random variable is <span class="math inline">\(p\)</span>, and the variance is <span class="math inline">\(p* (1 - p)\)</span>. In the context of a Bernoulli random variable, we often refer to “x = 1” as a success, irrespective of the specific definition of success in a given scenario, and “x = 0” as a failure.</p>
<p>A binomial random variable is obtained by summing up a series of independent and identically distributed (iid) Bernoulli random variables. Essentially, a binomial random variable represents the total number of heads obtained in a series of coin flips with a potentially biased coin. Mathematically, if we let <span class="math inline">\(X_1\)</span> to <span class="math inline">\(X_n\)</span> be iid Bernoulli variables with parameter <span class="math inline">\(p\)</span>, then the sum of these variables, denoted as <span class="math inline">\(X\)</span>, is a binomial random variable.
<span class="math display">\[X=\Sigma_{i=1}^n X_i\]</span>
<span class="math display">\[P(X=x)=\left(\begin{array}{c}  n \\ x \end{array}\right)p^x(1 - p)^{n-x}=\frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}\]</span>
The binomial probability mass function closely resembles the Bernoulli mass function, but with the inclusion of “n choose x” in front. The notation “n choose x” represents the binomial coefficient, calculated as <span class="math inline">\(\frac{n!}{x!(n-x)!}\)</span>. It is worth noting that “n choose 0” <span class="math inline">\(\left(\begin{array}{c} n \\ 0 \end{array}\right)\)</span> and “n choose n” <span class="math inline">\(\left(\begin{array}{c} n \\ n \end{array}\right)\)</span> both equal 1. This coefficient helps solve a common combinatorial problem, counting the number of ways to select “x” items out of “n” without replacement while disregarding the ordering of the items.</p>
<p>Example: Suppose your friend has eight children, with seven of them being girls (and no twins). Assuming each gender has an independent 50% probability for each birth, what is the probability of having seven or more girls out of eight births?
We can apply the binomial formula to calculate this probability:
<span class="math display">\[P(X\geq7) = \left(\begin{array}{c}  8 \\ 7 \end{array}\right) * 0.5^7 * (1 - 0.5)^1 + \left(\begin{array}{c}  8 \\ 8 \end{array}\right) * 0.5^8 * (1 - 0.5)^0≈0.04\]</span></p>
<p>In the provided R code, you can find the implementation of this calculation. Furthermore, for most common distributions, including the binomial distribution, there are built-in functions in R. For example, the <code>pbinom</code> function can be used to obtain these probabilities conveniently.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="variability-distribution-asymptotics.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">choose</span>(<span class="dv">8</span>, <span class="dv">7</span>) <span class="sc">*</span> .<span class="dv">5</span> <span class="sc">^</span> <span class="dv">8</span> <span class="sc">+</span> <span class="fu">choose</span>(<span class="dv">8</span>, <span class="dv">8</span>) <span class="sc">*</span> .<span class="dv">5</span> <span class="sc">^</span> <span class="dv">8</span> </span>
<span id="cb17-2"><a href="variability-distribution-asymptotics.html#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">pbinom</span>(<span class="dv">6</span>, <span class="at">size =</span> <span class="dv">8</span>, <span class="at">prob =</span> .<span class="dv">5</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
</div>
<div id="normal-distribution" class="section level3" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Normal distribution</h3>
<p>Probabilities play a crucial role in statistics, and among all the distributions, the normal distribution stands out as the most important one. In the upcoming lecture, we will explore why it holds such significance. In fact, if all distributions were to gather and elect a leader, the normal distribution would undoubtedly take the crown.</p>
<p>A random variable that follows a normal (Gaussian) distribution with a mean of <span class="math inline">\(\mu\)</span> and a variance of <span class="math inline">\(\sigma^2\)</span>. This distribution is characterized by a density function that resembles a bell curve. If we have a random variable X with this density, its expected value is μ, and its variance is <span class="math inline">\(\sigma^2\)</span>. We can express this concisely as <span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span>, denoting a normal distribution with mean μ and variance <span class="math inline">\(\sigma^2\)</span>. When μ=0 and σ=1, the resulting distribution is known as the <strong>standard normal distribution</strong>. Standard normal random variables are often denoted by the letter <span class="math inline">\(z\)</span>. Here, we depict the standard normal density function, which represents the famous bell curve you have likely encountered before.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="variability-distribution-asymptotics.html#cb18-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="at">length =</span> <span class="dv">1000</span>)</span>
<span id="cb18-2"><a href="variability-distribution-asymptotics.html#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb18-3"><a href="variability-distribution-asymptotics.html#cb18-3" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> <span class="fu">dnorm</span>(x)), </span>
<span id="cb18-4"><a href="variability-distribution-asymptotics.html#cb18-4" aria-hidden="true" tabindex="-1"></a>            <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">2</span>)</span>
<span id="cb18-5"><a href="variability-distribution-asymptotics.html#cb18-5" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> g <span class="sc">+</span> <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="sc">-</span><span class="dv">3</span> <span class="sc">:</span> <span class="dv">3</span>, <span class="at">size =</span> <span class="dv">2</span>)</span>
<span id="cb18-6"><a href="variability-distribution-asymptotics.html#cb18-6" aria-hidden="true" tabindex="-1"></a>g</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-14"></span>
<img src="week_02_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_78.png" alt="Alternative text" width="480" />
<p class="caption">
Figure 1.12: Standard normal distribution
</p>
</div>
<p>It is important to note that for the standard normal distribution, the mean is 0, and the standard deviation (and variance) is 1. In the diagram, we illustrate one standard deviation above and below the mean, two standard deviations above and below the mean, and three standard deviations above and below the mean. The units on the standard normal distribution can be interpreted as standard deviation units. Additionally, it is worth mentioning that statisticians often find it convenient to revert to the standard normal distribution when discussing normal probabilities, even when dealing with non-standard normal distributions. Therefore, if you want to calculate the probability that a non-standard normal lies between <span class="math inline">\(μ + 1σ\)</span> and <span class="math inline">\(μ - 1σ\)</span> (where μ and σ are specific to its distribution), the probability area is equivalent to that between -1 and +1 on the standard normal distribution. In essence, all normal distributions have the same underlying shape, with the only difference being the units along the axis. By reverting to standard deviations from the mean, all probabilities and calculations can be transformed back to those associated with the standard normal distribution.</p>
<p>Some fundamental reference probabilities related to the standard normal distribution can be easily explained using the graph above as visual aids. First, consider one standard deviation from the mean in the standard normal distribution (or any normal distribution). Approximately 34% of the distribution lies on each side, resulting in a total area of 68% within one standard deviation. Moving on to two standard deviations, denoted by the magenta area in the diagram, around 95% of the distribution falls within this range for any normal distribution. This leaves 2.5% in each tail, and we often utilize this information when calculating confidence intervals. Lastly, when considering three standard deviations from the mean, the area encompasses approximately 99% of the distribution’s mass, although it may be difficult to discern from the diagram. <em>These reference probabilities are essential to commit to memory.</em></p>
<p>Probabilities are a fundamental concept, and the normal distribution holds a special place in statistics. Understanding its properties and the relationship to the standard normal distribution allows us to solve problems effectively. All normal distributions share the same essential shape, differing only in their units along the axis. By leveraging the standard normal distribution and converting the non standard normals to standard normals, we can simplify calculations and derive consistent results. The primary difference between different normal distributions lies in the units along the axis. When discussing normal probabilities and converting to standard deviations from the mean, all probabilities and calculations revert back to those associated with the standard normal distribution.</p>
<p>Rules for converting between standard and non-standard normal distributions. If we have a random variable X that follows a normal distribution with a mean of μ and variance of σ squared, we can convert the units of X to standard deviations from the mean by subtracting the mean μ and dividing by the standard deviation σ. If <span class="math inline">\(X \sim N(\mu,\sigma^2)\)</span> then:
<span class="math display">\[Z = \frac{X -\mu}{\sigma} \sim N(0, 1)\]</span>
The resulting random variable Z will follow a standard normal distribution. Conversely, if we start with a standard normal random variable Z and want to convert back to the units of the original data, we multiply Z by σ and add μ.
If <span class="math inline">\(Z\)</span> is standard normal <span class="math display">\[X = \mu + \sigma Z \sim N(\mu, \sigma^2)\]</span>
The resulting random variable X will then follow a non-standard normal distribution with a mean of μ and variance of <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Standard normal quantiles that are important to remember -1.28 is a quantile such that 10% of the density lies below it, and 90% lies above it. By symmetry, 1.28 on the standard normal distribution represents the quantile at which 10% lies above it. For a potentially non-standard normal distribution, this point would be <span class="math inline">\(μ + 1.28σ\)</span>. Another crucial quantile is 1.96 (often approximated as 2), where -1.96 represents the point below which 2.5% of the mass of the normal distribution lies, and +1.96 represents the point above which 2.5% of the mass lies. This implies that 95% of the distribution lies between these two points. For a potentially non-standard normal distribution, these points would be <span class="math inline">\(μ - 1.96σ\)</span> and <span class="math inline">\(μ + 1.96σ\)</span>, respectively. It is worth noting that when μ equals 0 and σ equals 1 for the standard normal distribution, the calculation of 1.96 directly yields the correct value.</p>
<p>Example: Determine the <span class="math inline">\(95^{th}\)</span> percentile of a normal distribution with mean μ and variance σ squared.
In other words, we seek the value <span class="math inline">\(X_{0.95}\)</span> such that 95% of the distribution lies below it. This value represents the threshold if we were to draw samples from this population.</p>
<p>We can find the point <span class="math inline">\(X_{0.95}\)</span>, which represents the <span class="math inline">\(95^{th}\)</span> percentile of a normal distribution, by utilizing the q qualifier for the density in R. In this case, we can use the function <code>qnorm</code> with the desired quantile 0.95.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="variability-distribution-asymptotics.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.95</span>, <span class="at">mean =</span> mu, <span class="at">sd =</span> sd)</span></code></pre></div>
<p>It’s crucial to input the mean μ and the standard deviation σ (not the variance) into the function. By using <code>qnorm</code> with the specified parameters, we can directly obtain the desired value. Another approach to solving this is by leveraging our memorized standard normal quartiles. Since we know that 1.645 standard deviations from the mean corresponds to a quantile with 95% lying below it and 5% lying above it for the standard normal distribution (centered at 0 with standard deviation units from the mean), we can apply this concept to a non-standard normal distribution as well. To calculate the desired point, we can simply compute <span class="math inline">\(μ + σ * 1.64\)</span>.</p>
<p>Example: What is the probability that a non-standard normal distribution <span class="math inline">\(N(\mu,\sigma^2)\)</span> is larger than x? To answer this question in R, we can use the <code>pnorm</code> function with the specified values of <span class="math inline">\(x\)</span>, <span class="math inline">\(mean(\mu)\)</span>, and standard deviation (<span class="math inline">\(\sigma\)</span>). It’s important to remember to input the sigma value rather than the <span class="math inline">\(\sigma^2\)</span> value to avoid incorrect results. Additionally, we set the argument <code>lower.tail = FALSE</code> to indicate that we are interested in the upper tail of the distribution. Alternatively, we can omit this argument and calculate <span class="math inline">\(1 -pnorm(x,mean=\mu,sd=\sigma)\)</span> to achieve the same result.</p>
<p>A conceptually easy way to estimate this probability, which allows us to quickly assess probabilities mentally, is to convert the value x into the number of standard deviations it is from the mean. To achieve this, we compute <span class="math inline">\((μ -x)/ σ\)</span>. The resulting number represents x expressed in terms of how many standard deviations it is from the mean. For example, if the calculated value is approximately two standard deviations from the mean, we can estimate that the probability associated with it is around 2.5%.</p>
<p>Example: The number of daily ad clicks for companies follows an approximately normal distribution with a mean of 1020 clicks per day and a standard deviation of 50 clicks per day. We want to determine the probability of getting more than 1160 clicks on a given day.</p>
<p>Since <span class="math inline">\((1160-1020)/50=2.8\)</span> which means 2.8 standard deviation away from the mean, we can infer that this probability will be relatively low. This is because it is nearly 3 standard deviations away from the mean, and we know that such values are located in the tail of the normal distribution. To calculate this probability, we can use the <code>pnorm</code> function with the input values of 1,160 for the clicks, a mean of 1,020, and a standard deviation of 50.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="variability-distribution-asymptotics.html#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="dv">1160</span>, <span class="at">mean =</span> <span class="dv">1020</span>, <span class="at">sd =</span> <span class="dv">50</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb20-2"><a href="variability-distribution-asymptotics.html#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">2.8</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>By setting the argument <code>lower.tail = FALSE</code>, we ensure that we obtain the probability of the value being larger than 1,060. The result we obtain is approximately 0.003.</p>
<p>Alternatively, we can directly calculate this probability using the standard normal distribution. By expressing 1,160 as the number of standard deviations it is away from the mean, which is 2.8, we can plug this value into the <code>pnorm</code> function with <code>lower.tail = FALSE</code> and obtain the same result.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="variability-distribution-asymptotics.html#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="fl">2.8</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>Example: Assuming the number of daily ad clicks for the company follows an approximately normal distribution with a mean of 1020 and a standard deviation of 50, we want to find the number of daily ad clicks that represents the point where 75% of the days have fewer clicks.</p>
<p>Since 1020 is both the mean and the median of the specific normal distribution, we know that about 50% of the days lie below this point. Therefore, the desired number of clicks should be greater than 1020. Additionally, one standard deviation above the mean, which corresponds to 1,070. Within this range, we know that 68% of the days lie, leaving 32% outside of it, and 16% in each tail due to the symmetry of the normal distribution. Hence, the desired number of clicks should be around 84% of the distribution, lying between 1,020 and 1,070.</p>
<p>To calculate this quantile, we can use the <code>qnorm</code> function with the input value of 0.75, representing the 75th percentile. The mean is set to 1020, and the standard deviation is 50. When we execute this command, <code>qnorm(0.75, mean = 1020, sd = 50)</code>, we obtain a number between the previously mentioned range, approximately 1054.</p>
</div>
<div id="poisson-distribution" class="section level3" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Poisson distribution</h3>
<p>If there were a competition to determine the most useful distribution, the normal distribution would unquestionably win by a wide margin. However, selecting the second most useful distribution would spark a lively debate, with the Poisson distribution being a strong contender. The Poisson distribution is commonly employed to model counts, and its probability mass function is given by <span class="math display">\[P(X = x; \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}\]</span>
where x represents non-negative integers (0, 1, 2, and so on).
The mean of a Poisson random variable is equal to <span class="math inline">\(\lambda\)</span>, and the variance also equals <span class="math inline">\(\lambda\)</span>. When modeling with Poisson data, the mean and variance must be equal a condition that can be verified if one has repeated Poisson data.
fig xxx poisson distribution</p>
<p>The Poisson distribution finds utility in various instances. Whenever count data needs to be modeled, especially when the counts are unbounded, the Poisson distribution is a suitable choice. Another prevalent application arises in the field of biostatistics, where event time or survival data is common. For example, in cancer trials, the time until the recurrence of symptoms is modeled using statistical techniques that account for censoring, and these techniques have a strong association with the Poisson distribution. Additionally, when classifying a sample of people based on certain characteristics, creating a contingency table—such as tabulating hair color by race—the Poisson distribution is the default choice for modeling such data. The Poisson distribution is deeply connected to other models, including multinomials and binomials, which might be considered as alternatives.</p>
<p>Another prominent application of the Poisson distribution, though often overlooked due to its commonplace usage, is in cases where a binomial distribution is approximated by the Poisson distribution. This occurs when the sample size (n) is large, and the probability of success (p) is small. Epidemiology, for instance, frequently employs this approximation when dealing with situations where n is large (representing a population) and p is small (indicating the occurrence of rare events). By assuming a Poisson distribution, researchers can effectively model the occurrence rates of events, such as the number of new cases of respiratory diseases in a city as air pollution levels fluctuate. This practice is so prevalent that it is commonly understood within the field without explicit mention.</p>
<p>Example: The number of people showing up at a bus stop follows a Poisson distribution with a mean of 2.5 people per hour. If we observe the bus stop for four hours, we can calculate the probability of three or fewer people showing up during that entire duration. To do this, we apply the Poisson probability formula to the values of three, two, one, and zero, using a rate of 2.5 events per hour multiplied by four hours. The resulting probability is approximately 1%.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="variability-distribution-asymptotics.html#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ppois</span>(<span class="dv">3</span>, <span class="at">lambda =</span> <span class="fl">2.5</span> <span class="sc">*</span> <span class="dv">4</span>)</span></code></pre></div>
<p>Furthermore, we can discuss the Poisson approximation to the binomial distribution, specifically when the sample size (n) is large, and the probability of success (p) is small. In this scenario, the Poisson distribution can serve as a reasonably accurate approximation for the binomial distribution. To establish notation, let x represent a binomial distribution with parameters n and p, and define <span class="math inline">\(\lambda=n*p\)</span>. When n is large and p is small, it is proposed that the probability distribution governing x can be well approximated using Poisson probabilities, where the rate parameter λ is determined as n times p. </p>
<p>Example: In flipping a coin with a success probability of 0.01 for a total of 500 times, we want to calculate the probability of obtaining two or fewer successes. Using the binomial distribution with size 500 and probability 0.01, we obtain approximately 12%. By employing the Poisson approximation with a rate of λ = 500 * 0.01, the result is around 12.5%, which is reasonably close to the binomial calculation.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="variability-distribution-asymptotics.html#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pbinom</span>(<span class="dv">2</span>, <span class="at">size =</span> <span class="dv">500</span>, <span class="at">prob =</span> .<span class="dv">01</span>)</span>
<span id="cb23-2"><a href="variability-distribution-asymptotics.html#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ppois</span>(<span class="dv">2</span>, <span class="at">lambda=</span><span class="dv">500</span> <span class="sc">*</span> .<span class="dv">01</span>)</span></code></pre></div>
</div>
</div>
<div id="asymptotics" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Asymptotics</h2>
<p>Asymptotics are an important topics in statistics. Asymptotics refers to the behavior of estimators as the sample size goes to infinity. Our very notion of probability depends on the idea of asymptotics. For example, many people define probability as the proportion of times an event would occur in infinite repetitions. That is, the probability of a head on a coin is 50% because we believe that if we were to flip it infinitely many times, we would get exactly 50% heads.</p>
<p>We can use asymptotics to help is figure out things about distributions without knowing much about them to begin with. A profound idea along these lines is the <strong>Central Limit Theorem</strong>. It states that the distribution of averages is often normal, even if the distribution that the data is being sampled from is very non-normal. This helps us create robust strategies for creating statistical inferences when we’re not willing to assume much about the generating mechanism of our data.
### Asymptotics and LLN
Here we will explore the behavior of statistics as the sample size or some other relevant quantity approaches infinity, which is known as asymptotics. Specifically, we will discuss the case where the sample size tends to infinity.</p>
<p>In the land of asymptopia, everything works out well because there is an infinite amount of data available. Asymptotics play a crucial role in simple statistical inference and approximations. They serve as a versatile tool, akin to a Swiss army knife, allowing us to investigate the statistical properties of various statistics without requiring extensive computations. Asymptotics form the foundation for the frequency interpretation of probabilities. For instance, intuitively, we know that if we flip a coin and calculate the proportion of heads, it should approach 0.5 for a fair coin.</p>
<p>Fortunately, we don’t have to delve into the mathematical intricacies of the limits of random variables. Instead, we can rely on a set of powerful tools that enable us to discuss the behavior of sample means from a collection of independently and identically distributed (iid) observations in large samples. One of these tools is the law of large numbers, which states that the average of the observations converges to the population mean it is estimating. For example, if we repeatedly flip a fair coin, the sample proportion of heads will eventually converge to the true probability of a head.</p>
<p>Example: We’ll generate a large number of random normal variables and calculate their cumulative means. Initially, there is considerable variability in the means, but as the number of simulations increases, the cumulative means converge towards the true population mean of zero.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-20"></span>
<img src="week_02_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_95.png" alt="Cumulative means of random normal variables" width="480" />
<p class="caption">
Figure 2.2: Cumulative means of random normal variables
</p>
</div>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="variability-distribution-asymptotics.html#cb24-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10000</span>; means <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(<span class="fu">rnorm</span>(n)) <span class="sc">/</span> (<span class="dv">1</span>  <span class="sc">:</span> n); <span class="fu">library</span>(ggplot2)</span>
<span id="cb24-2"><a href="variability-distribution-asymptotics.html#cb24-2" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> <span class="dv">1</span> <span class="sc">:</span> n, <span class="at">y =</span> means), <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) </span>
<span id="cb24-3"><a href="variability-distribution-asymptotics.html#cb24-3" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> g <span class="sc">+</span> <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">2</span>) </span>
<span id="cb24-4"><a href="variability-distribution-asymptotics.html#cb24-4" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> g <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Number of obs&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Cumulative mean&quot;</span>)</span>
<span id="cb24-5"><a href="variability-distribution-asymptotics.html#cb24-5" aria-hidden="true" tabindex="-1"></a>g</span></code></pre></div>
Similarly, we can apply the law of large numbers to the case of coin flipping. By repeatedly flipping a coin and calculating the cumulative means, we observe that the sample proportion of heads converges to the true value of 0.5 as the number of coin flips increases.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-22"></span>
<img src="week_02_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_98.png" alt="Cumulative means of coin flips" width="480" />
<p class="caption">
Figure 2.3: Cumulative means of coin flips
</p>
</div>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="variability-distribution-asymptotics.html#cb25-1" aria-hidden="true" tabindex="-1"></a>means <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(<span class="fu">sample</span>(<span class="dv">0</span> <span class="sc">:</span> <span class="dv">1</span>, n , <span class="at">replace =</span> <span class="cn">TRUE</span>)) <span class="sc">/</span> (<span class="dv">1</span>  <span class="sc">:</span> n)</span>
<span id="cb25-2"><a href="variability-distribution-asymptotics.html#cb25-2" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> <span class="dv">1</span> <span class="sc">:</span> n, <span class="at">y =</span> means), <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) </span>
<span id="cb25-3"><a href="variability-distribution-asymptotics.html#cb25-3" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> g <span class="sc">+</span> <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="fl">0.5</span>) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">2</span>) </span>
<span id="cb25-4"><a href="variability-distribution-asymptotics.html#cb25-4" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> g <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Number of obs&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Cumulative mean&quot;</span>)</span>
<span id="cb25-5"><a href="variability-distribution-asymptotics.html#cb25-5" aria-hidden="true" tabindex="-1"></a>g</span></code></pre></div>
<p>Note: An estimator is considered <strong>consistent</strong> if it converges to the parameter it aims to estimate. For instance, the sample proportion from iid coin flips is consistent for estimating the true success probability of a coin. As we collect more and more coin flip data, the sample proportion of heads approaches the actual probability of obtaining a head. Moreover, not only are sample means consistent estimators, but the sample variance and sample standard deviation of iid random variables are also consistent estimators.</p>
<p>The law of large numbers guarantees the consistency of sample means, but it also applies to sample variances and standard deviations of iid random variables. In other words, these estimators also converge to their respective population counterparts as the sample size increases.</p>
<div id="asymptotics-and-the-clt" class="section level3" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Asymptotics and the CLT</h3>
<p>The Central Limit Theorem (CLT) is perhaps the most important theorem in statistics. It states that the distribution of averages of iid random variables becomes approximately standard normal as the sample size grows. The Central Limit Theorem is remarkably versatile, applying to a wide range of populations. Its loose requirements make it applicable in numerous settings.</p>
<p>To understand the Central Limit Theorem, let’s consider an estimate like the sample average <span class="math inline">\(\bar X\)</span>. If we subtract its population mean and divide by its standard error the resulting random variable approaches a standard normal distribution as the sample size increases.
<span class="math display">\[\frac{\bar{X_n}-\mu}{\sigma/\sqrt{n}}=\frac{\sqrt{n}(\bar{X_n}-\mu)}{\sigma}=\frac{{Estimate} - {Mean\,of\,estimate}}{Std.\,Err.\,of\,estimate}\]</span>
Importantly, replacing the unknown population standard deviation with the known sample standard deviation does not affect the Central Limit Theorem.</p>
<p>The most useful interpretation of the Central Limit Theorem is that the sample average is approximately normally distributed, with a mean equal to the population mean and a variance given by the standard error of the mean.</p>
Example: Using standard die with the mean of 3.5, and variance of 2.92. We simulate the die roll n times, calculate the sample mean, subtract the population mean, and dividing by the standard error.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-24"></span>
<img src="week_02_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_101.png" alt="Distribution of averages of iid random variables in die roll" width="480" />
<p class="caption">
Figure 2.4: Distribution of averages of iid random variables in die roll
</p>
</div>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="variability-distribution-asymptotics.html#cb26-1" aria-hidden="true" tabindex="-1"></a>nosim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb26-2"><a href="variability-distribution-asymptotics.html#cb26-2" aria-hidden="true" tabindex="-1"></a>cfunc <span class="ot">&lt;-</span> <span class="cf">function</span>(x, n) <span class="fu">sqrt</span>(n) <span class="sc">*</span> (<span class="fu">mean</span>(x) <span class="sc">-</span> <span class="fl">3.5</span>) <span class="sc">/</span> <span class="fl">1.71</span></span>
<span id="cb26-3"><a href="variability-distribution-asymptotics.html#cb26-3" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb26-4"><a href="variability-distribution-asymptotics.html#cb26-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">c</span>(<span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">sample</span>(<span class="dv">1</span> <span class="sc">:</span> <span class="dv">6</span>, nosim <span class="sc">*</span> <span class="dv">10</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>), </span>
<span id="cb26-5"><a href="variability-distribution-asymptotics.html#cb26-5" aria-hidden="true" tabindex="-1"></a>                     nosim), <span class="dv">1</span>, cfunc, <span class="dv">10</span>),</span>
<span id="cb26-6"><a href="variability-distribution-asymptotics.html#cb26-6" aria-hidden="true" tabindex="-1"></a>        <span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">sample</span>(<span class="dv">1</span> <span class="sc">:</span> <span class="dv">6</span>, nosim <span class="sc">*</span> <span class="dv">20</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>), </span>
<span id="cb26-7"><a href="variability-distribution-asymptotics.html#cb26-7" aria-hidden="true" tabindex="-1"></a>                     nosim), <span class="dv">1</span>, cfunc, <span class="dv">20</span>),</span>
<span id="cb26-8"><a href="variability-distribution-asymptotics.html#cb26-8" aria-hidden="true" tabindex="-1"></a>        <span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">sample</span>(<span class="dv">1</span> <span class="sc">:</span> <span class="dv">6</span>, nosim <span class="sc">*</span> <span class="dv">30</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>), </span>
<span id="cb26-9"><a href="variability-distribution-asymptotics.html#cb26-9" aria-hidden="true" tabindex="-1"></a>                     nosim), <span class="dv">1</span>, cfunc, <span class="dv">30</span>)</span>
<span id="cb26-10"><a href="variability-distribution-asymptotics.html#cb26-10" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb26-11"><a href="variability-distribution-asymptotics.html#cb26-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">size =</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>), <span class="fu">rep</span>(nosim, <span class="dv">3</span>))))</span>
<span id="cb26-12"><a href="variability-distribution-asymptotics.html#cb26-12" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(dat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">fill =</span> size)) <span class="sc">+</span> <span class="fu">geom_histogram</span>(<span class="at">alpha =</span> .<span class="dv">20</span>, <span class="at">binwidth=</span>.<span class="dv">3</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>, <span class="fu">aes</span>(<span class="at">y =</span> ..density..)) </span>
<span id="cb26-13"><a href="variability-distribution-asymptotics.html#cb26-13" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> g <span class="sc">+</span> <span class="fu">stat_function</span>(<span class="at">fun =</span> dnorm, <span class="at">size =</span> <span class="dv">2</span>)</span>
<span id="cb26-14"><a href="variability-distribution-asymptotics.html#cb26-14" aria-hidden="true" tabindex="-1"></a>g <span class="sc">+</span> <span class="fu">facet_grid</span>(. <span class="sc">~</span> size)</span></code></pre></div>
<p>The distribution approximates a bell curve. As we increase the number of rolls, the approximation improves.</p>
<p>Example: Let <span class="math inline">\(X_i\)</span> be the <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> result of the <span class="math inline">\(i^{th}\)</span> flip of a possibly unfair coin. The sample proportion, say <span class="math inline">\(\hat p\)</span>, is the average of the coin flips.
<span class="math inline">\(E[X_i] = p\)</span> and <span class="math inline">\(Var(X_i) = p(1-p)\)</span>
Standard error of the mean is <span class="math inline">\(\sqrt{p(1-p)/n}\)</span> Then
<span class="math display">\[\frac{\hat p - p}{\sqrt{p(1-p)/n}}\]</span>
will be approximately normally distributed</p>
<p>Flipping a fair coin <span class="math inline">\(n\)</span> times, taking the sample proportion of heads, subtracting off 0.5 and multiply the result by
<span class="math inline">\(2 \sqrt{n}\)</span> divide by <span class="math inline">\(1/(2 \sqrt{n})\)</span> is displayed below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-26"></span>
<img src="week_02_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_104.png" alt="Distribution of averages of iid random variables in coin flip" width="480" />
<p class="caption">
Figure 2.5: Distribution of averages of iid random variables in coin flip
</p>
</div>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="variability-distribution-asymptotics.html#cb27-1" aria-hidden="true" tabindex="-1"></a>nosim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb27-2"><a href="variability-distribution-asymptotics.html#cb27-2" aria-hidden="true" tabindex="-1"></a>cfunc <span class="ot">&lt;-</span> <span class="cf">function</span>(x, n) <span class="dv">2</span> <span class="sc">*</span> <span class="fu">sqrt</span>(n) <span class="sc">*</span> (<span class="fu">mean</span>(x) <span class="sc">-</span> <span class="fl">0.5</span>) </span>
<span id="cb27-3"><a href="variability-distribution-asymptotics.html#cb27-3" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb27-4"><a href="variability-distribution-asymptotics.html#cb27-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">c</span>(<span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">sample</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>, nosim <span class="sc">*</span> <span class="dv">10</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>), </span>
<span id="cb27-5"><a href="variability-distribution-asymptotics.html#cb27-5" aria-hidden="true" tabindex="-1"></a>                     nosim), <span class="dv">1</span>, cfunc, <span class="dv">10</span>),</span>
<span id="cb27-6"><a href="variability-distribution-asymptotics.html#cb27-6" aria-hidden="true" tabindex="-1"></a>        <span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">sample</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>, nosim <span class="sc">*</span> <span class="dv">20</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>), </span>
<span id="cb27-7"><a href="variability-distribution-asymptotics.html#cb27-7" aria-hidden="true" tabindex="-1"></a>                     nosim), <span class="dv">1</span>, cfunc, <span class="dv">20</span>),</span>
<span id="cb27-8"><a href="variability-distribution-asymptotics.html#cb27-8" aria-hidden="true" tabindex="-1"></a>        <span class="fu">apply</span>(<span class="fu">matrix</span>(<span class="fu">sample</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>, nosim <span class="sc">*</span> <span class="dv">30</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>), </span>
<span id="cb27-9"><a href="variability-distribution-asymptotics.html#cb27-9" aria-hidden="true" tabindex="-1"></a>                     nosim), <span class="dv">1</span>, cfunc, <span class="dv">30</span>)</span>
<span id="cb27-10"><a href="variability-distribution-asymptotics.html#cb27-10" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb27-11"><a href="variability-distribution-asymptotics.html#cb27-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">size =</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>), <span class="fu">rep</span>(nosim, <span class="dv">3</span>))))</span>
<span id="cb27-12"><a href="variability-distribution-asymptotics.html#cb27-12" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(dat, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">fill =</span> size)) <span class="sc">+</span> <span class="fu">geom_histogram</span>(<span class="at">binwidth=</span>.<span class="dv">3</span>, <span class="at">colour =</span> <span class="st">&quot;black&quot;</span>, <span class="fu">aes</span>(<span class="at">y =</span> ..density..)) </span>
<span id="cb27-13"><a href="variability-distribution-asymptotics.html#cb27-13" aria-hidden="true" tabindex="-1"></a>g <span class="ot">&lt;-</span> g <span class="sc">+</span> <span class="fu">stat_function</span>(<span class="at">fun =</span> dnorm, <span class="at">size =</span> <span class="dv">2</span>)</span>
<span id="cb27-14"><a href="variability-distribution-asymptotics.html#cb27-14" aria-hidden="true" tabindex="-1"></a>g <span class="sc">+</span> <span class="fu">facet_grid</span>(. <span class="sc">~</span> size)</span></code></pre></div>
<p>Taking the result of each flip (0 or 1) as an iid random variable, we calculate the sample proportion of heads <span class="math inline">\(\hat p\)</span>. We again obtain a distribution that approximates a bell curve. Similar to the previous example, the approximation improves as the number of coin flips increases.</p>
<p>It’s important to note that the speed at which the normalized coin flips converge to normality depends on the bias of the coin. If the coin is heavily biased, the approximation may not be perfect even with a large sample size. However, as the number of coin flips approaches infinity, the Central Limit Theorem guarantees an excellent approximation.</p>
<p>As a fun example, let’s discuss Galton’s quincunx. This machine, often found in science museums, visually demonstrates the Central Limit Theorem using a game resembling Pachinko. <a href="https://upload.wikimedia.org/wikipedia/commons/c/c1/Galton_box.jpg">An image from Wikipedia showing Galton’s quincunx</a>. In Galton’s quincunx, a ball falls through a series of pegs, bouncing left or right at each peg. Each bounce can be thought of as a coin flip or binomial experiment. The total number of successes (heads) follows an approximately normal distribution, as predicted by the Central Limit Theorem. At the museum, the balls collect in bins, forming a histogram that aligns with the expected normal distribution.</p>
<p>In summary, the Central Limit Theorem is a powerful tool that allows us to approximate the distribution of averages of iid random variables. It applies to various settings and provides valuable insights into statistical inference. The examples we explored, from dice rolls to coin flips to Galton’s quincunx, illustrate the practical applications of the Central Limit Theorem and the convergence to a standard normal distribution as the sample size increases.</p>
</div>
<div id="asymptotics-and-confidence-intervals" class="section level3" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Asymptotics and confidence intervals</h3>
<p>The central limit theorem tells us that the sample mean follows an approximately normal distribution with a population mean of μ and a standard deviation of <span class="math inline">\(\sigma/\sqrt{n}\)</span>. This distribution allows us to make inferences about the population mean based on sample data. When considering the distribution, we observe that <span class="math inline">\(μ+2\)</span> standard errors is quite far out in the tail, with only a 2.5% chance of a normal value being larger than two standard deviations in the tail. Similarly, <span class="math inline">\(μ-2\)</span> standard errors is far in the left tail, with only a 2.5% chance of a normal value being smaller than two standard deviations in the left tail. Therefore, the probability that the sample mean <span class="math inline">\(\bar X\)</span> is greater than <span class="math inline">\(μ+2\)</span> standard errors or smaller than <span class="math inline">\(μ-2\)</span> standard errors is 5%. Equivalently, the probability that μ is between these limits is 95%. By reversing the roles of <span class="math inline">\(\bar X\)</span> and μ, we can conclude that the interval <span class="math inline">\([\bar X - 2 \sigma /\sqrt{n}, \bar X + 2 \sigma /\sqrt{n}]\)</span> contains μ with a probability of 95%.</p>
<p>It’s important to note that in this interpretation, we treat the interval <span class="math inline">\([\bar X - 2 \sigma /\sqrt{n}, \bar X + 2 \sigma /\sqrt{n}]\)</span> as random, while μ is fixed. This allows us to discuss the probability that the interval contains μ. In practice, if we repeatedly obtain samples of size n from the population and construct a confidence interval in each case, about 95% of the intervals will contain μ, the parameter we are trying to estimate. If we want a 90% confidence interval, we need 5% in each tail, so we would use a different multiplier instead of 2 (e.g., 1.645).</p>
<p>Example: Using the father-son data from the “Using R” package we want to estimate the average height of sons <span class="math inline">\(\bar X\)</span>. We can calculate the mean of the sample plus or minus the 0.975th normal quantile times the standard error of the mean.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="variability-distribution-asymptotics.html#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(UsingR);<span class="fu">data</span>(father.son); x <span class="ot">&lt;-</span> father.son<span class="sc">$</span>sheight</span>
<span id="cb28-2"><a href="variability-distribution-asymptotics.html#cb28-2" aria-hidden="true" tabindex="-1"></a>(<span class="fu">mean</span>(x) <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="sc">*</span> <span class="fu">qnorm</span>(.<span class="dv">975</span>) <span class="sc">*</span> <span class="fu">sd</span>(x) <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="fu">length</span>(x))) <span class="sc">/</span> <span class="dv">12</span></span></code></pre></div>
<p>Dividing by 12 ensures that our confidence interval is in feet rather than inches. If we obtain a confidence interval of 5.710 to 5.738, we can say that if the sons’ height in this data are a random sample from the population of interest, the confidence interval for the average height of the sons would be 5.71 to 5.74.</p>
<p>Another application is when dealing with coin flips and estimating the success probability <span class="math inline">\(p\)</span> of the coin. Each observation <span class="math inline">\(X_i\)</span> in this case is either 0 or 1, with a common success probability <span class="math inline">\(p\)</span>. The variance of a coin flip is <span class="math inline">\(p * (1 - p)\)</span>, where p is the true success probability of the coin. The standard error of the mean is then:
<span class="math display">\[ \hat p \pm z_{1 - \alpha/2}  \sqrt{\frac{p(1 - p)}{n}}\]</span></p>
<p>Since we don’t know the true value of <span class="math inline">\(p\)</span>, we replace it with the estimated value <span class="math inline">\(\hat p\)</span>. This type of confidence interval is known as the <strong>Wald confidence interval</strong>, named after the statistician Wald. When p equals 0.5, the variance <span class="math inline">\(p(1 - p)\)</span> is maximized, resulting in a standard error of 0.5. Multiplying it by 2 in the 95% interval cancels out, leaving the following expression for a 95% confidence interval, which is a quick estimate for p:
<span class="math display">\[\hat p \pm 1/\sqrt{n}\]</span></p>
<p>Example: Imaging you are running for political office, and in a random sample of 100 likely voters, 56 intend to vote for you. To determine if you can relax or if you need to campaign more, you can use a quick calculation.</p>
<p>With the information from the sample you can with with probability of 0.56 taking <span class="math inline">\(\frac{1}{\sqrt{100}}=0.1\)</span> means the approximate 95% interval is 0.46 to 0.66. The confidence interval suggests that we cannot rule out possibilities below 0.5 with 95% confidence. Therefore, you shouldn’t relax and should continue campaigning.</p>
<p>As a general guideline, you typically need at least 100 observations for one decimal place in a binomial experiment, 10,000 for two decimal places, and a million for three decimal places. These numbers reflect the approximate sample sizes needed for accurate estimation.</p>
<p>In summary, the central limit theorem provides us with a practical tool for constructing confidence intervals and making inferences about population parameters. It allows us to estimate the population mean using the sample mean and provides a measure of uncertainty through confidence intervals. The Wald confidence interval is a useful approximation for estimating the success probability in binomial experiments. Additionally, considering the sample size helps determine the level of precision and confidence in our estimates.</p>
<p>Consider a simulation where we repeatedly flip a coin with a known success probability. The goal is to calculate the percentage of times that the confidence interval covers the true probability. In each simulation, we flip the coin 20 times and vary the true success probability between 0.1 and 0.9 in steps of 0.05. We conduct 1,000 simulations for each true success probability.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="variability-distribution-asymptotics.html#cb29-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">20</span>; pvals <span class="ot">&lt;-</span> <span class="fu">seq</span>(.<span class="dv">1</span>, .<span class="dv">9</span>, <span class="at">by =</span> .<span class="dv">05</span>); nosim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb29-2"><a href="variability-distribution-asymptotics.html#cb29-2" aria-hidden="true" tabindex="-1"></a>coverage <span class="ot">&lt;-</span> <span class="fu">sapply</span>(pvals, <span class="cf">function</span>(p){</span>
<span id="cb29-3"><a href="variability-distribution-asymptotics.html#cb29-3" aria-hidden="true" tabindex="-1"></a>  phats <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(nosim, <span class="at">prob =</span> p, <span class="at">size =</span> n) <span class="sc">/</span> n</span>
<span id="cb29-4"><a href="variability-distribution-asymptotics.html#cb29-4" aria-hidden="true" tabindex="-1"></a>  ll <span class="ot">&lt;-</span> phats <span class="sc">-</span> <span class="fu">qnorm</span>(.<span class="dv">975</span>) <span class="sc">*</span> <span class="fu">sqrt</span>(phats <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> phats) <span class="sc">/</span> n)</span>
<span id="cb29-5"><a href="variability-distribution-asymptotics.html#cb29-5" aria-hidden="true" tabindex="-1"></a>  ul <span class="ot">&lt;-</span> phats <span class="sc">+</span> <span class="fu">qnorm</span>(.<span class="dv">975</span>) <span class="sc">*</span> <span class="fu">sqrt</span>(phats <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> phats) <span class="sc">/</span> n)</span>
<span id="cb29-6"><a href="variability-distribution-asymptotics.html#cb29-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mean</span>(ll <span class="sc">&lt;</span> p <span class="sc">&amp;</span> ul <span class="sc">&gt;</span> p)</span>
<span id="cb29-7"><a href="variability-distribution-asymptotics.html#cb29-7" aria-hidden="true" tabindex="-1"></a>})</span></code></pre></div>
<p>For each true success probability, we generate 1,000 sets of 20 coin flips and calculate the sample proportion. Then, we compute the lower and upper limits of the confidence interval for each set of coin flips. Finally, we determine the proportion of times that the confidence interval covers the true value of the success probability. we store these proportions in a variable called “coverage.”</p>
To visualize the results, we can plot the coverage as a function of the true success probability used in the simulation. For example, if the true value of p is 0.5, we perform 1,000 simulations and calculate the coverage based on whether the confidence interval covers 0.5 or not. In this case, the coverage is over 95%, indicating that the confidence interval provides better than 95% coverage for a true success probability of 0.5.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-30"></span>
<img src="week_02_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_107.png" alt="Coverage of confidence intervals for coin flips, n=20" width="480" />
<p class="caption">
Figure 2.6: Coverage of confidence intervals for coin flips, n=20
</p>
</div>
<p>Although there is some Monte Carlo error due to the finite number of simulations, 1,000 simulations generally yield good accuracy. For a true success probability around 12%, the coverage falls well below the expected 95%. The reason behind this discrepancy is that the central limit theorem is not accurate enough for this specific value of n (the number of coin flips) and the true probability. To address this issue for smaller values of n, a quick fix is to add 2 to the number of successes and 2 to the number of failures. This adjustment modifies the sample proportion, making it <span class="math inline">\(\frac{X+2}{n+4}\)</span>. After applying this adjustment, the confidence interval procedure can be performed as usual. This modified interval is known as the <strong>Agresti/Coull interval</strong> and tends to perform better than the standard Wald interval. Before demonstrating the results for the adjusted intervals, it is important to note that larger values of n yield better performance. In a simulation where n is increased to 100, the coverage probability improves and remains close to the expected 95% across different values of p.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-31"></span>
<img src="week_02_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_110.png" alt="Coverage of confidence intervals for coin flips, n=100" width="480" />
<p class="caption">
Figure 2.7: Coverage of confidence intervals for coin flips, n=100
</p>
</div>
<p>Returning to the simulation with n=20, when using the add 2 successes and 2 failures interval, the coverage probability is higher than 95%, indicating an improvement compared to the poor coverage of the Wald interval for certain true probability values. However, it’s important to balance coverage and interval width, as being too conservative can lead to overly wide intervals. Based on these observations, we strongly recommend using the add 2 successes and 2 failures interval instead of the Wald interval in this specific scenario.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-32"></span>
<img src="week_02_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_81.png" alt="Coverage of confidence intervals for coin flips, n=20, add 2 successes and 2 failures" width="480" />
<p class="caption">
Figure 2.8: Coverage of confidence intervals for coin flips, n=20, add 2 successes and 2 failures
</p>
</div>
<p>Example: Create a Poisson interval using the formula that involves the estimate plus or minus the normal quantile standard error.
Although the application of the central limit theorem in this case may be less clear, we will discuss it shortly.Consider a nuclear pump that failed 5 times out of 94.32 days over a monitoring period. We want to calculate a 95% confidence interval for the failure rate per day. Assuming the number of failures follows a Poisson distribution with a failure rate of lambda and the monitoring period is denoted as t, the estimate of the failure rate is the number of failures divided by the total monitoring time. The variance of this estimate is <span class="math inline">\(\lambda/t\)</span>.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="variability-distribution-asymptotics.html#cb30-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">5</span>; t <span class="ot">&lt;-</span> <span class="fl">94.32</span>; lambda <span class="ot">&lt;-</span> x <span class="sc">/</span> t</span>
<span id="cb30-2"><a href="variability-distribution-asymptotics.html#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(lambda <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="sc">*</span> <span class="fu">qnorm</span>(.<span class="dv">975</span>) <span class="sc">*</span> <span class="fu">sqrt</span>(lambda <span class="sc">/</span> t), <span class="dv">3</span>)</span>
<span id="cb30-3"><a href="variability-distribution-asymptotics.html#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="fu">poisson.test</span>(x, <span class="at">T =</span> <span class="fl">94.32</span>)<span class="sc">$</span>conf</span></code></pre></div>
<p>In the calculations performed in R, the number of events <span class="math inline">\(x\)</span> is set to 5, and the monitoring time <span class="math inline">\(t\)</span> is 94.32. The rate estimate <span class="math inline">\(\hat \lambda\)</span> is computed as <span class="math inline">\(x/t\)</span>, and the confidence interval estimate is obtained by adding or subtracting the relevant standard normal quantile multiplied by the standard error. The resulting interval is rounded to three decimal places. In addition to the large sample interval, we can also calculate an exact Poisson interval using the <code>poisson.test</code> function in R. This exact interval guarantees the specified coverage (e.g., 95%), but it may be conservative and result in wider intervals than necessary.</p>
<p>To examine how confidence intervals perform in repeated samplings, let’s conduct a simulation similar to the one for the coin example, but for the Poisson coverage rate. We select a range of <span class="math inline">\(\lambda\)</span> values around those from our previous example and perform 1,000 simulations. The monitoring time is set to 100 for simplicity. We define coverage as the percentage of times the simulated interval contains the true <span class="math inline">\(\lambda\)</span> value used in the simulation. The simulation is repeated for various <span class="math inline">\(\lambda\)</span> values, and the resulting plot shows the <span class="math inline">\(\lambda\)</span> values on the x-axis and the estimated coverage on the y-axis.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-34"></span>
<img src="week_02_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_84.png" alt="Coverage of confidence intervals for Poisson data" width="480" />
<p class="caption">
Figure 2.9: Coverage of confidence intervals for Poisson data
</p>
</div>
<p>The plot reveals that as <span class="math inline">\(\lambda\)</span> values increase, the coverage approaches 95%. However, there is some Monte Carlo error due to the finite number of simulations. On the other hand, as the true <span class="math inline">\(\lambda\)</span> value becomes smaller, the coverage deteriorates significantly. For very small <span class="math inline">\(\lambda\)</span> values, the purported 95% interval may only provide 50% actual coverage. To address this issue, it is recommended not to rely on the asymptotic interval for small <span class="math inline">\(\lambda\)</span> values, especially when there are relatively few events during a large monitoring time. In such cases, the asymptotic interval does not align well with the Poisson distribution. Instead, an exact Poisson interval can be used as an alternative.</p>
<p>Although the central limit theorem’s application in the Poisson case may not be immediately clear, a simulation with a larger monitoring time (e.g., changing t from 100 to 1,000) demonstrates that as the monitoring time increases, the coverage improves and converges to 95% for most <span class="math inline">\(\lambda\)</span> values. However, some poor coverage may still occur for small <span class="math inline">\(\lambda\)</span> values, which we know the interval has trouble handling. In such cases, the exact Poisson interval remains a viable option.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-35"></span>
<img src="week_02_files/figure-html/1U1PiqeXG4XoKmg8hRFJqE1OFDOJfificBz1jLeDunHo_g257d7b8e795_0_122.png" alt="Coverage of confidence intervals for Poisson data, t=1000" width="480" />
<p class="caption">
Figure 2.10: Coverage of confidence intervals for Poisson data, t=1000
</p>
</div>
<p>To summarize briefly, we covered the Law of Large Numbers, which states that averages of independent and identically distributed (iid) random variables converge to the quantities they are estimating. This applies to Poisson rates as well, although the convergence process may be less clear. As the monitoring time tends to infinity, for example, Poisson rates converge to their estimated values. We also discussed the Central Limit Theorem, which states that averages are approximately normally distributed. These distributions are centered at the population mean, a concept we already knew without the theorem, with standard deviations equal to the standard error of the mean. However, the Central Limit Theorem does not guarantee that the sample size is large enough for this approximation to be accurate. We have observed instances where confidence intervals are very accurate and others where they are less accurate. Speaking of confidence intervals, our default approach for constructing them is to take the mean estimate and add or subtract the relevant normal quantile times the standard error. This method, known as “walled intervals,” is used not only in this context but also in regression analysis, general linear models, and other complex subjects. For a 95% confidence interval, the quantile value can be taken as 2 or, for more accuracy, 1.96. Confidence intervals become wider as the desired coverage increases within a specific technique. This is because wider intervals provide more certainty that the parameter lies within them. To illustrate, imagine an extreme scenario where your life depends on the confidence interval containing the true parameter. In this case, you would want to make the interval as wide as possible to ensure your safety. The mathematics behind confidence intervals follows the same principle.
In the cases of Poisson and binomial distributions, which are discrete, the Central Limit Theorem may not accurately approximate their distributions. However, exact procedures exist for these cases. We also learned a simple fix for constructing confidence intervals in the binomial case by adding two successes and two failures, which provides a better interval without requiring complex computations. This method can be easily done by hand or mentally, even without access to a computer.</p>
<ul>
<li>The LLN states that averages of iid samples
converge to the population means that they are estimating</li>
<li>The CLT states that averages are approximately normal, with
distributions
<ul>
<li>centered at the population mean</li>
<li>with standard deviation equal to the standard error of the mean</li>
<li>CLT gives no guarantee that <span class="math inline">\(n\)</span> is large enough</li>
</ul></li>
<li>Taking the mean and adding and subtracting the relevant
normal quantile times the SE yields a confidence interval for the mean
<ul>
<li>Adding and subtracting 2 SEs works for 95% intervals</li>
</ul></li>
<li>Confidence intervals get wider as the coverage increases
(why?)</li>
<li>Confidence intervals get narrower with less variability or
larger sample sizes</li>
<li>The Poisson and binomial case have exact intervals that
don’t require the CLT
<ul>
<li>But a quick fix for small sample size binomial calculations is to add 2 successes and failures</li>
</ul></li>
</ul>
</div>
</div>
<div id="practical-r-exercises-in-swirl-1" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Practical R Exercises in swirl</h2>

</div>
</div>
<hr>
<center> 
  <div class="footer">
      All illustrations <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY. </a>
      <br>
      All other materials <a href= "https://creativecommons.org/licenses/by/4.0/"> CC-BY </a> unless noted otherwise.
  </div>
</center>
            </section>

          </div>
        </div>
      </div>
<a href="probability-expected-values.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="intervals-testing-pvalues.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
